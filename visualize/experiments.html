<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Paper Experiments Viewer</title>
    <style>
        :root {
            --bg-primary: #0d1117;
            --bg-secondary: #161b22;
            --bg-tertiary: #21262d;
            --border-color: #30363d;
            --text-primary: #e6edf3;
            --text-secondary: #8b949e;
            --accent-blue: #58a6ff;
            --accent-green: #3fb950;
            --accent-purple: #a371f7;
            --accent-orange: #d29922;
            --accent-red: #f85149;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Noto Sans', Helvetica, Arial, sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.6;
            min-height: 100vh;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
        }

        header {
            text-align: center;
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        header h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            color: var(--text-primary);
        }

        header p {
            color: var(--text-secondary);
        }

        .stats-bar {
            display: flex;
            gap: 2rem;
            justify-content: center;
            margin-top: 1rem;
        }

        .stat {
            text-align: center;
        }

        .stat-value {
            font-size: 1.5rem;
            font-weight: 600;
            color: var(--accent-blue);
        }

        .stat-label {
            font-size: 0.8rem;
            color: var(--text-secondary);
        }

        .search-filter {
            display: flex;
            gap: 1rem;
            margin-bottom: 2rem;
            flex-wrap: wrap;
        }

        .search-filter input {
            flex: 1;
            min-width: 250px;
            padding: 0.75rem 1rem;
            background: var(--bg-secondary);
            border: 1px solid var(--border-color);
            border-radius: 6px;
            color: var(--text-primary);
            font-size: 1rem;
        }

        .search-filter input:focus {
            outline: none;
            border-color: var(--accent-blue);
        }

        .papers-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(380px, 1fr));
            gap: 1.5rem;
        }

        .paper-card {
            background: var(--bg-secondary);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 1.5rem;
            cursor: pointer;
            transition: all 0.2s ease;
        }

        .paper-card:hover {
            border-color: var(--accent-blue);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
        }

        .paper-card h3 {
            font-size: 1rem;
            margin-bottom: 0.75rem;
            color: var(--text-primary);
            display: -webkit-box;
            -webkit-line-clamp: 2;
            -webkit-box-orient: vertical;
            overflow: hidden;
        }

        .paper-focus {
            font-size: 0.85rem;
            color: var(--text-secondary);
            margin-bottom: 1rem;
            display: -webkit-box;
            -webkit-line-clamp: 3;
            -webkit-box-orient: vertical;
            overflow: hidden;
        }

        .paper-meta {
            display: flex;
            gap: 0.75rem;
            flex-wrap: wrap;
        }

        .badge {
            font-size: 0.75rem;
            padding: 0.25rem 0.6rem;
            border-radius: 12px;
            font-weight: 500;
        }

        .badge-main {
            background: rgba(88, 166, 255, 0.15);
            color: var(--accent-blue);
        }

        .badge-ablation {
            background: rgba(163, 113, 247, 0.15);
            color: var(--accent-purple);
        }

        .badge-other {
            background: rgba(210, 153, 34, 0.15);
            color: var(--accent-orange);
        }

        .badge-total {
            background: rgba(63, 185, 80, 0.15);
            color: var(--accent-green);
        }

        /* Modal Styles */
        .modal-overlay {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: rgba(0, 0, 0, 0.7);
            z-index: 1000;
            overflow-y: auto;
            padding: 2rem;
        }

        .modal-overlay.active {
            display: flex;
            justify-content: center;
            align-items: flex-start;
        }

        .modal {
            background: var(--bg-secondary);
            border: 1px solid var(--border-color);
            border-radius: 12px;
            max-width: 900px;
            width: 100%;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
            position: relative;
        }

        .modal-header {
            padding: 1.5rem;
            border-bottom: 1px solid var(--border-color);
            position: sticky;
            top: 0;
            background: var(--bg-secondary);
            z-index: 10;
        }

        .modal-close {
            position: absolute;
            top: 1rem;
            right: 1rem;
            background: none;
            border: none;
            color: var(--text-secondary);
            font-size: 1.5rem;
            cursor: pointer;
            padding: 0.25rem 0.5rem;
            border-radius: 4px;
        }

        .modal-close:hover {
            background: var(--bg-tertiary);
            color: var(--text-primary);
        }

        .modal-title {
            font-size: 1.25rem;
            margin-bottom: 0.5rem;
            padding-right: 2rem;
        }

        .modal-focus {
            color: var(--text-secondary);
            font-size: 0.9rem;
        }

        .modal-body {
            padding: 1.5rem;
        }

        .experiment-group {
            margin-bottom: 2rem;
        }

        .experiment-group:last-child {
            margin-bottom: 0;
        }

        .group-header {
            display: flex;
            align-items: center;
            gap: 0.75rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        .group-header h4 {
            font-size: 0.9rem;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            color: var(--text-secondary);
        }

        .group-count {
            font-size: 0.75rem;
            padding: 0.15rem 0.5rem;
            border-radius: 10px;
            background: var(--bg-tertiary);
            color: var(--text-secondary);
        }

        .experiment-card {
            background: var(--bg-tertiary);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            margin-bottom: 1rem;
            overflow: hidden;
        }

        .experiment-card:last-child {
            margin-bottom: 0;
        }

        .experiment-header {
            padding: 1rem;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            gap: 1rem;
        }

        .experiment-header:hover {
            background: rgba(255, 255, 255, 0.02);
        }

        .experiment-name {
            font-weight: 600;
            margin-bottom: 0.25rem;
        }

        .experiment-section {
            font-size: 0.8rem;
            color: var(--text-secondary);
        }

        .experiment-toggle {
            color: var(--text-secondary);
            font-size: 1.25rem;
            flex-shrink: 0;
            transition: transform 0.2s;
        }

        .experiment-card.expanded .experiment-toggle {
            transform: rotate(180deg);
        }

        .experiment-details {
            display: none;
            padding: 0 1rem 1rem;
            border-top: 1px solid var(--border-color);
        }

        .experiment-card.expanded .experiment-details {
            display: block;
        }

        .detail-section {
            margin-top: 1rem;
        }

        .detail-section h5 {
            font-size: 0.8rem;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            color: var(--accent-blue);
            margin-bottom: 0.5rem;
        }

        .detail-section p {
            font-size: 0.9rem;
            color: var(--text-secondary);
        }

        .benchmarks-list, .baselines-list {
            list-style: none;
        }

        .benchmarks-list li, .baselines-list li {
            padding: 0.5rem 0;
            border-bottom: 1px solid var(--border-color);
            font-size: 0.85rem;
        }

        .benchmarks-list li:last-child, .baselines-list li:last-child {
            border-bottom: none;
        }

        .benchmark-name, .baseline-name {
            font-weight: 500;
            color: var(--text-primary);
        }

        .benchmark-variant, .baseline-desc {
            color: var(--text-secondary);
            font-size: 0.8rem;
        }

        .conclusion-box {
            background: var(--bg-secondary);
            border-radius: 6px;
            padding: 1rem;
            margin-top: 0.5rem;
        }

        .main-finding {
            font-weight: 500;
            margin-bottom: 0.75rem;
            color: var(--text-primary);
        }

        .supporting-findings {
            list-style: disc;
            margin-left: 1.25rem;
            font-size: 0.85rem;
            color: var(--text-secondary);
        }

        .supporting-findings li {
            margin-bottom: 0.25rem;
        }

        .winner-badge {
            display: inline-block;
            margin-top: 0.75rem;
            padding: 0.35rem 0.75rem;
            background: rgba(63, 185, 80, 0.15);
            color: var(--accent-green);
            border-radius: 4px;
            font-size: 0.8rem;
            font-weight: 500;
        }

        .no-results {
            text-align: center;
            padding: 3rem;
            color: var(--text-secondary);
        }

        .loading {
            text-align: center;
            padding: 3rem;
            color: var(--text-secondary);
        }

        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            .papers-grid {
                grid-template-columns: 1fr;
            }

            .modal-overlay {
                padding: 1rem;
            }

            .stats-bar {
                gap: 1rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Paper Experiments Viewer</h1>
            <p>Explore experiments from research papers on neural SAT solving</p>
            <div class="stats-bar">
                <div class="stat">
                    <div class="stat-value" id="total-papers">0</div>
                    <div class="stat-label">Papers</div>
                </div>
                <div class="stat">
                    <div class="stat-value" id="total-experiments">0</div>
                    <div class="stat-label">Experiments</div>
                </div>
                <div class="stat">
                    <div class="stat-value" id="total-main">0</div>
                    <div class="stat-label">Main</div>
                </div>
                <div class="stat">
                    <div class="stat-value" id="total-ablation">0</div>
                    <div class="stat-label">Ablation</div>
                </div>
            </div>
        </header>

        <div class="search-filter">
            <input type="text" id="search-input" placeholder="Search papers by title, focus, or experiment names...">
        </div>

        <div class="papers-grid" id="papers-grid">
            <div class="loading">Loading experiment data...</div>
        </div>
    </div>

    <div class="modal-overlay" id="modal-overlay">
        <div class="modal" id="modal">
            <div class="modal-header">
                <button class="modal-close" onclick="closeModal()">&times;</button>
                <h2 class="modal-title" id="modal-title"></h2>
                <p class="modal-focus" id="modal-focus"></p>
            </div>
            <div class="modal-body" id="modal-body"></div>
        </div>
    </div>

    <script>
        // Paper data will be embedded here
        const papersData = [];

        // DOM elements
        const papersGrid = document.getElementById('papers-grid');
        const searchInput = document.getElementById('search-input');
        const modalOverlay = document.getElementById('modal-overlay');
        const modal = document.getElementById('modal');

        // Parse YAML-like content from markdown files
        function parseYamlContent(content) {
            // Remove the ```yaml wrapper if present
            content = content.replace(/^```yaml\s*/, '').replace(/```\s*$/, '');

            const data = {
                paper_title: '',
                paper_focus: '',
                experiment_inventory: {
                    total_experiments: 0,
                    main_experiments: 0,
                    ablation_studies: 0,
                    other_experiments: 0
                },
                experiments: {},
                summary_table: []
            };

            // Extract paper_title
            const titleMatch = content.match(/paper_title:\s*"([^"]+)"/);
            if (titleMatch) data.paper_title = titleMatch[1];

            // Extract paper_focus
            const focusMatch = content.match(/paper_focus:\s*"([^"]+)"/);
            if (focusMatch) data.paper_focus = focusMatch[1];

            // Extract experiment counts
            const totalMatch = content.match(/total_experiments:\s*["']?(\d+)["']?/);
            const mainMatch = content.match(/main_experiments:\s*["']?(\d+)["']?/);
            const ablationMatch = content.match(/ablation_studies:\s*["']?(\d+)["']?/);
            const otherMatch = content.match(/other_experiments:\s*["']?(\d+)["']?/);

            if (totalMatch) data.experiment_inventory.total_experiments = parseInt(totalMatch[1]);
            if (mainMatch) data.experiment_inventory.main_experiments = parseInt(mainMatch[1]);
            if (ablationMatch) data.experiment_inventory.ablation_studies = parseInt(ablationMatch[1]);
            if (otherMatch) data.experiment_inventory.other_experiments = parseInt(otherMatch[1]);

            // Extract experiments using regex
            const expRegex = /exp_\d+:\s*\n([\s\S]*?)(?=\n\s*exp_\d+:|# === ABLATION|# === CROSS|$)/g;
            let expMatch;
            let expIndex = 1;

            while ((expMatch = expRegex.exec(content)) !== null) {
                const expContent = expMatch[1];
                const exp = parseExperiment(expContent);
                if (exp.name) {
                    data.experiments[`exp_${expIndex}`] = exp;
                    expIndex++;
                }
            }

            return data;
        }

        function parseExperiment(content) {
            const exp = {
                name: '',
                paper_section: '',
                experiment_type: '',
                purpose: '',
                benchmarks: [],
                baselines: [],
                conclusion: {
                    main_finding: '',
                    supporting_findings: [],
                    winner: ''
                },
                detailed_description: ''
            };

            // Extract name
            const nameMatch = content.match(/name:\s*"([^"]+)"/);
            if (nameMatch) exp.name = nameMatch[1];

            // Extract paper_section
            const sectionMatch = content.match(/paper_section:\s*"([^"]+)"/);
            if (sectionMatch) exp.paper_section = sectionMatch[1];

            // Extract experiment_type
            const typeMatch = content.match(/experiment_type:\s*"?([^"\n]+)"?/);
            if (typeMatch) exp.experiment_type = typeMatch[1].trim();

            // Extract purpose
            const purposeMatch = content.match(/purpose:\s*"([^"]+)"/);
            if (purposeMatch) exp.purpose = purposeMatch[1];

            // Extract benchmarks
            const benchmarksSection = content.match(/benchmarks:\s*\n([\s\S]*?)(?=\n\s*baselines:|$)/);
            if (benchmarksSection) {
                const benchmarkRegex = /-\s*name:\s*"([^"]+)"[\s\S]*?(?:variant:\s*"([^"]+)")?[\s\S]*?(?:problem_sizes:\s*"([^"]+)")?/g;
                let bmMatch;
                while ((bmMatch = benchmarkRegex.exec(benchmarksSection[1])) !== null) {
                    exp.benchmarks.push({
                        name: bmMatch[1] || '',
                        variant: bmMatch[2] || '',
                        problem_sizes: bmMatch[3] || ''
                    });
                }
            }

            // Extract baselines
            const baselinesSection = content.match(/baselines:\s*\n([\s\S]*?)(?=\n\s*conclusion:|$)/);
            if (baselinesSection) {
                const baselineRegex = /-\s*name:\s*"([^"]+)"[\s\S]*?(?:description:\s*"([^"]*)")?/g;
                let blMatch;
                while ((blMatch = baselineRegex.exec(baselinesSection[1])) !== null) {
                    exp.baselines.push({
                        name: blMatch[1] || '',
                        description: blMatch[2] || ''
                    });
                }
            }

            // Extract conclusion
            const mainFindingMatch = content.match(/main_finding:\s*"([^"]+)"/);
            if (mainFindingMatch) exp.conclusion.main_finding = mainFindingMatch[1];

            const winnerMatch = content.match(/winner:\s*"([^"]+)"/);
            if (winnerMatch) exp.conclusion.winner = winnerMatch[1];

            // Extract supporting findings
            const supportingSection = content.match(/supporting_findings:\s*\n([\s\S]*?)(?=\n\s*winner:|$)/);
            if (supportingSection) {
                const findingRegex = /-\s*"([^"]+)"/g;
                let sfMatch;
                while ((sfMatch = findingRegex.exec(supportingSection[1])) !== null) {
                    exp.conclusion.supporting_findings.push(sfMatch[1]);
                }
            }

            // Extract detailed_description
            const descMatch = content.match(/detailed_description:\s*\|\s*\n([\s\S]*?)(?=\n\s*exp_\d+:|# ===|$)/);
            if (descMatch) {
                exp.detailed_description = descMatch[1].trim().replace(/^\s{8}/gm, '');
            }

            return exp;
        }

        function renderPaperCard(paper, index) {
            const inventory = paper.experiment_inventory || {};
            return `
                <div class="paper-card" onclick="openModal(${index})">
                    <h3>${paper.paper_title || 'Untitled Paper'}</h3>
                    <p class="paper-focus">${paper.paper_focus || 'No description available'}</p>
                    <div class="paper-meta">
                        <span class="badge badge-total">${inventory.total_experiments || 0} total</span>
                        ${inventory.main_experiments ? `<span class="badge badge-main">${inventory.main_experiments} main</span>` : ''}
                        ${inventory.ablation_studies ? `<span class="badge badge-ablation">${inventory.ablation_studies} ablation</span>` : ''}
                        ${inventory.other_experiments ? `<span class="badge badge-other">${inventory.other_experiments} other</span>` : ''}
                    </div>
                </div>
            `;
        }

        function groupExperimentsByType(experiments) {
            const groups = {
                main_comparison: [],
                ablation: [],
                other: []
            };

            for (const [key, exp] of Object.entries(experiments)) {
                const type = exp.experiment_type || 'other';
                if (type.includes('main') || type.includes('comparison')) {
                    groups.main_comparison.push(exp);
                } else if (type.includes('ablation')) {
                    groups.ablation.push(exp);
                } else {
                    groups.other.push(exp);
                }
            }

            return groups;
        }

        function renderExperimentCard(exp, index) {
            return `
                <div class="experiment-card" id="exp-card-${index}">
                    <div class="experiment-header" onclick="toggleExperiment(${index})">
                        <div>
                            <div class="experiment-name">${exp.name || 'Unnamed Experiment'}</div>
                            <div class="experiment-section">${exp.paper_section || ''}</div>
                        </div>
                        <span class="experiment-toggle">&#9660;</span>
                    </div>
                    <div class="experiment-details">
                        ${exp.purpose ? `
                            <div class="detail-section">
                                <h5>Purpose</h5>
                                <p>${exp.purpose}</p>
                            </div>
                        ` : ''}

                        ${exp.benchmarks && exp.benchmarks.length > 0 ? `
                            <div class="detail-section">
                                <h5>Benchmarks</h5>
                                <ul class="benchmarks-list">
                                    ${exp.benchmarks.map(b => `
                                        <li>
                                            <div class="benchmark-name">${b.name}</div>
                                            ${b.variant ? `<div class="benchmark-variant">Variant: ${b.variant}</div>` : ''}
                                            ${b.problem_sizes ? `<div class="benchmark-variant">Size: ${b.problem_sizes}</div>` : ''}
                                        </li>
                                    `).join('')}
                                </ul>
                            </div>
                        ` : ''}

                        ${exp.baselines && exp.baselines.length > 0 ? `
                            <div class="detail-section">
                                <h5>Baselines</h5>
                                <ul class="baselines-list">
                                    ${exp.baselines.map(b => `
                                        <li>
                                            <div class="baseline-name">${b.name}</div>
                                            ${b.description ? `<div class="baseline-desc">${b.description}</div>` : ''}
                                        </li>
                                    `).join('')}
                                </ul>
                            </div>
                        ` : ''}

                        ${exp.conclusion && exp.conclusion.main_finding ? `
                            <div class="detail-section">
                                <h5>Conclusion</h5>
                                <div class="conclusion-box">
                                    <p class="main-finding">${exp.conclusion.main_finding}</p>
                                    ${exp.conclusion.supporting_findings && exp.conclusion.supporting_findings.length > 0 ? `
                                        <ul class="supporting-findings">
                                            ${exp.conclusion.supporting_findings.map(f => `<li>${f}</li>`).join('')}
                                        </ul>
                                    ` : ''}
                                    ${exp.conclusion.winner ? `
                                        <span class="winner-badge">Winner: ${exp.conclusion.winner}</span>
                                    ` : ''}
                                </div>
                            </div>
                        ` : ''}
                    </div>
                </div>
            `;
        }

        function renderExperimentGroups(experiments) {
            const groups = groupExperimentsByType(experiments);
            let html = '';
            let globalIndex = 0;

            const groupLabels = {
                main_comparison: { label: 'Main Experiments', icon: '&#9679;' },
                ablation: { label: 'Ablation Studies', icon: '&#9675;' },
                other: { label: 'Other Experiments', icon: '&#9674;' }
            };

            for (const [groupKey, exps] of Object.entries(groups)) {
                if (exps.length > 0) {
                    const groupInfo = groupLabels[groupKey];
                    html += `
                        <div class="experiment-group">
                            <div class="group-header">
                                <h4>${groupInfo.label}</h4>
                                <span class="group-count">${exps.length}</span>
                            </div>
                            ${exps.map(exp => renderExperimentCard(exp, globalIndex++)).join('')}
                        </div>
                    `;
                }
            }

            return html || '<p class="no-results">No experiments found</p>';
        }

        function openModal(index) {
            const paper = papersData[index];
            document.getElementById('modal-title').textContent = paper.paper_title || 'Untitled Paper';
            document.getElementById('modal-focus').textContent = paper.paper_focus || '';
            document.getElementById('modal-body').innerHTML = renderExperimentGroups(paper.experiments || {});
            modalOverlay.classList.add('active');
            document.body.style.overflow = 'hidden';
        }

        function closeModal() {
            modalOverlay.classList.remove('active');
            document.body.style.overflow = '';
        }

        function toggleExperiment(index) {
            const card = document.getElementById(`exp-card-${index}`);
            card.classList.toggle('expanded');
        }

        function filterPapers(query) {
            query = query.toLowerCase();
            return papersData.filter(paper => {
                const title = (paper.paper_title || '').toLowerCase();
                const focus = (paper.paper_focus || '').toLowerCase();
                const experiments = Object.values(paper.experiments || {})
                    .map(e => (e.name || '').toLowerCase())
                    .join(' ');
                return title.includes(query) || focus.includes(query) || experiments.includes(query);
            });
        }

        function renderPapers(papers) {
            if (papers.length === 0) {
                papersGrid.innerHTML = '<div class="no-results">No papers found matching your search.</div>';
                return;
            }
            papersGrid.innerHTML = papers.map((paper, index) => {
                const realIndex = papersData.indexOf(paper);
                return renderPaperCard(paper, realIndex);
            }).join('');
        }

        function updateStats() {
            let totalPapers = papersData.length;
            let totalExperiments = 0;
            let totalMain = 0;
            let totalAblation = 0;

            papersData.forEach(paper => {
                const inv = paper.experiment_inventory || {};
                totalExperiments += inv.total_experiments || 0;
                totalMain += inv.main_experiments || 0;
                totalAblation += inv.ablation_studies || 0;
            });

            document.getElementById('total-papers').textContent = totalPapers;
            document.getElementById('total-experiments').textContent = totalExperiments;
            document.getElementById('total-main').textContent = totalMain;
            document.getElementById('total-ablation').textContent = totalAblation;
        }

        // Event listeners
        searchInput.addEventListener('input', (e) => {
            const query = e.target.value;
            const filtered = query ? filterPapers(query) : papersData;
            renderPapers(filtered);
        });

        modalOverlay.addEventListener('click', (e) => {
            if (e.target === modalOverlay) {
                closeModal();
            }
        });

        document.addEventListener('keydown', (e) => {
            if (e.key === 'Escape') {
                closeModal();
            }
        });

        // Embedded data - will be filled by the generator script
        const embeddedData = [
  {
    "filename": "2005.13406_Neural_heuristics_for_SAT_solving.md",
    "paper_title": "Neural Heuristics for SAT Solving",
    "paper_focus": "The paper proposes using neural graph networks with message-passing architecture and attention mechanism to enhance the branching heuristic (choose-literal function) in SAT-solving algorithms (DPLL and CDCL).",
    "experiment_inventory": {
      "total_experiments": 3,
      "main_experiments": 2,
      "ablation_studies": 1,
      "other_experiments": 0
    },
    "experiments": {
      "exp_1": {
        "name": "Comparison of all models with DLIS and JW-OS heuristics",
        "paper_section": "Section 4, Experiment 1",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate whether neural network-guided DPLL algorithms can compete with or outperform standard human-designed heuristics (DLIS and JW-OS) across different problem sizes.",
        "benchmarks": [
          {
            "name": "SR(n) - random satisfiable SAT problems",
            "variant": "SR(50), SR(70), SR(90), SR(110)",
            "problem_sizes": "n=50, 70, 90, 110 variables; 100 satisfiable instances per size"
          }
        ],
        "baselines": [
          {
            "name": "DLIS",
            "description": "Dynamic Largest Individual Sum heuristic - a standard human-designed branching heuristic",
            "source": "Marques-Silva [MS99] and Moskewicz et al. [MMZ+01]"
          },
          {
            "name": "JW-OS (Jeroslow-Wang One-Sided)",
            "description": "A standard human-designed branching heuristic considered one of the best strategies in most circumstances",
            "source": "Marques-Silva [MS99] and Moskewicz et al. [MMZ+01]"
          },
          {
            "name": "Neural model trained on SR(30)",
            "description": "Message-passing neural network with attention trained on SR(30) problems",
            "source": "Proposed method"
          },
          {
            "name": "Neural model trained on SR(50)",
            "description": "Message-passing neural network with attention trained on SR(50) problems",
            "source": "Proposed method"
          },
          {
            "name": "Neural model trained on SR(70)",
            "description": "Message-passing neural network with attention trained on SR(70) problems",
            "source": "Proposed method"
          },
          {
            "name": "Neural model trained on SR(100)",
            "description": "Message-passing neural network with attention trained on SR(100) problems",
            "source": "Proposed method"
          }
        ],
        "conclusion": {
          "main_finding": "Neural guidance-based algorithms proved to be the best on large problems (SR(90) and SR(110)), whereas JW-OS proved to be the best on average-sized problems (SR(50) and SR(70)).",
          "supporting_findings": [
            "Performance was measured as the percentage of instances (out of 100) solved by DPLL within 1000 steps",
            "Different models trained on different problem sizes showed varying performance across evaluation sets",
            "Neural models demonstrate competitive or superior performance on larger problem instances"
          ],
          "winner": "Context-dependent: JW-OS for medium-sized problems (SR(50), SR(70)); Neural models for larger problems (SR(90), SR(110))"
        },
        "detailed_description": "This experiment provides a comprehensive comparison of neural network-guided DPLL algorithms against two standard human-designed heuristics: DLIS and JW-OS. The authors trained four separate neural models on different problem sizes: SR(30), SR(50), SR(70), and SR(100). Each model uses a message-passing architecture with attention mechanism.\n\nThe evaluation was conducted on fresh, randomly generated satisfiable SR(n) formulas for n âˆˆ {50, 70, 90, 110}. For each problem size, 100 instances were tested. To make the comparison computationally feasible, the DPLL algorithm was stopped after 1000 branching steps, and success was measured as the percentage of instances solved within this limit.\n\nThe results reveal an interesting pattern: traditional heuristics (especially JW-OS) perform better on medium-sized problems (SR(50) and SR(70)), while neural guidance shows its strength on larger problems (SR(90) and SR(110)). This suggests that neural networks can learn patterns that help solve complex instances that traditional heuristics struggle with, though they may not yet capture all the optimization that human-designed heuristics provide for smaller instances.\n\nThe experiment demonstrates that neural heuristics are a promising direction for SAT solving, particularly for scaling to larger problem instances. The results are presented in Figure 3, showing clear performance trends across different problem sizes. This experiment validates the paper's main claim that neural networks can enhance SAT-solving algorithms and suggests that the benefit increases with problem complexity.\n"
      },
      "exp_2": {
        "name": "Detailed comparison with JW-OS heuristic using hybrid approach",
        "paper_section": "Section 4, Experiment 2",
        "experiment_type": "main_comparison",
        "purpose": "To perform a detailed head-to-head comparison between a hybrid neural-traditional approach and pure JW-OS heuristic without step restrictions, measuring the number of steps required to solve instances.",
        "benchmarks": [
          {
            "name": "SR(50) - random satisfiable SAT problems",
            "variant": "SR(50)",
            "problem_sizes": "n=50 variables; tested on multiple instances"
          }
        ],
        "baselines": [
          {
            "name": "JW-OS heuristic",
            "description": "Pure Jeroslow-Wang One-Sided heuristic used throughout the entire solving process",
            "source": "Standard heuristic from [MS99, MMZ+01]"
          },
          {
            "name": "Hybrid guidance (Proposed)",
            "description": "Uses neural model trained on SR(50) but switches to JW-OS when the network predicts sat probability below 0.3",
            "source": "Proposed method combining neural guidance with JW-OS fallback"
          }
        ],
        "conclusion": {
          "main_finding": "The hybrid approach is faster in terms of number of steps in a significant majority of cases for both DPLL and CDCL algorithms.",
          "supporting_findings": [
            "The hybrid method won (required fewer steps) in the majority of test cases for DPLL",
            "The hybrid method also won in the majority of test cases for CDCL",
            "The experiment was conducted without the 1000-step restriction used in Experiment 1",
            "Results demonstrate that neural guidance can enhance both simpler (DPLL) and more advanced (CDCL) SAT-solving algorithms"
          ],
          "winner": "Hybrid approach (neural model with JW-OS fallback)"
        },
        "detailed_description": "This experiment provides a more detailed and fair comparison between neural and traditional approaches by removing the artificial 1000-step limit imposed in Experiment 1. The authors selected the SR(50) model based on the results from Experiment 1 and designed a hybrid approach that intelligently combines neural guidance with traditional heuristics.\n\nThe key innovation in this experiment is the hybrid strategy: the neural network guides the search, but when it predicts a low satisfiability probability (below 0.3), the algorithm switches to the reliable JW-OS heuristic. This represents a practical approach to combining the strengths of both methods - using neural guidance when confident and falling back to proven traditional methods when uncertain.\n\nThe comparison was conducted on two different SAT-solving algorithms: DPLL (the simpler backtracking algorithm) and CDCL (the more advanced Conflict-Driven Clause Learning algorithm). For each instance, the number of branching steps required to find a solution was recorded. The results are presented in Figure 4 as bar charts showing the percentage of instances where each method won (required fewer steps), lost, or drew.\n\nThe results strongly favor the hybrid approach. In both DPLL and CDCL settings, the hybrid method solved the majority of instances in fewer steps than pure JW-OS. This demonstrates that: (1) neural guidance provides genuine algorithmic improvements, not just competitive performance, (2) the hybrid approach successfully leverages the strengths of both neural and traditional methods, and (3) the benefits transfer across different SAT-solving architectures (DPLL and CDCL).\n\nThis experiment is particularly important because it shows practical improvements in the actual solving process without artificial constraints, making a stronger case for the real-world applicability of neural heuristics in SAT solving.\n"
      },
      "exp_3": {
        "name": "Ablation study for attention mechanism",
        "paper_section": "Section 4, Experiment 3",
        "experiment_type": "ablation",
        "purpose": "To evaluate the contribution of the attention mechanism to the neural model's performance by comparing architectures with and without attention.",
        "benchmarks": [
          {
            "name": "SR(30)",
            "variant": "Level 20 and Level 40 (number of message-passing iterations)",
            "problem_sizes": "n=30 variables"
          },
          {
            "name": "SR(50)",
            "variant": "Level 20 and Level 40 (number of message-passing iterations)",
            "problem_sizes": "n=50 variables"
          }
        ],
        "baselines": [],
        "conclusion": {
          "main_finding": "The attention mechanism improves evaluation metrics (policy error) by a significant margin in most cases.",
          "supporting_findings": [
            "For SR(30) level 20, attention actually degraded model performance",
            "For SR(50) level 40, metrics with and without attention stayed within standard deviation of each other (inconclusive)",
            "For SR(30) level 40 and SR(50) level 20, attention provided clear improvements",
            "The selective acceptance of incoming messages made possible by attention mechanism is attributed as the reason for improved performance"
          ],
          "importance_of_component": "The attention mechanism is important and generally beneficial, though not universally across all problem sizes and iteration counts. It provides the most consistent benefits and should be included in the default architecture."
        },
        "detailed_description": "This ablation study investigates whether the attention mechanism genuinely contributes to the model's performance or if simpler aggregation methods would suffice. The attention mechanism is a key architectural component that allows receiving nodes to selectively accept or reject incoming messages based on the relationship between the message's K vector and the receiver's Q vector.\n\nThe authors trained multiple versions of the model with and without attention on two problem sizes (SR(30) and SR(50)) with two different numbers of message-passing iterations (20 and 40, referred to as \"levels\"). For each configuration, 3-5 models were trained to account for training variance. The evaluation metric is policy error, which measures how accurately the model predicts whether a formula with a specific literal assignment is satisfiable.\n\nThe results, presented in Figure 5, show mean policy error and standard deviation for each configuration. The pattern is mostly favorable for attention: in three out of four tested configurations (SR(30) level 40, SR(50) level 20, and implicitly others), the attention mechanism reduced policy error. However, there are two exceptions: SR(30) level 20 showed degraded performance with attention, and SR(50) level 40 showed no statistically significant difference.\n\nThe authors attribute the benefits of attention to its ability to selectively process messages. Unlike standard attention mechanisms that choose one message to focus on, their modified attention mechanism evaluates each message independently and can accept or reject multiple messages. This allows the model to filter out irrelevant or contradictory information during the message-passing process.\n\nThe mixed results at SR(30) level 20 suggest that attention may be less beneficial or even harmful on smaller, simpler problems with fewer iterations, possibly because the added complexity isn't necessary or because the training process needs adjustment for this configuration. However, the overall trend strongly supports including attention in the architecture, especially for the larger problems that are the main target of this research.\n\nThis ablation study provides important validation that the attention mechanism is not just architectural complexity for its own sake, but genuinely contributes to the model's ability to learn effective SAT-solving heuristics.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "Experiment 1: Comparison of all models with standard heuristics",
        "benchmarks": "SR(50), SR(70), SR(90), SR(110)",
        "baselines": "DLIS, JW-OS (Jeroslow-Wang One-Sided)",
        "key_conclusion": "Neural guidance-based algorithms perform best on large problems (SR(90), SR(110)), while JW-OS is best on average-sized problems (SR(50), SR(70))"
      },
      {
        "experiment": "Experiment 2: Detailed comparison with JW-OS using hybrid approach",
        "benchmarks": "SR(50) problems",
        "baselines": "JW-OS heuristic",
        "key_conclusion": "Hybrid approach (neural network switching to JW-OS when sat probability < 0.3) outperforms pure JW-OS in the majority of cases for both DPLL and CDCL algorithms"
      },
      {
        "experiment": "Experiment 3: Ablation study for attention mechanism",
        "benchmarks": "SR(30), SR(50) at levels 20 and 40",
        "baselines": "Same architecture without attention mechanism",
        "key_conclusion": "Attention mechanism improves performance in most cases; only SR(30) level 20 showed degradation with attention"
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "SR(30)",
          "used_in_experiments": [
            "exp_3 (ablation)"
          ]
        },
        {
          "benchmark": "SR(50)",
          "used_in_experiments": [
            "exp_1 (main comparison)",
            "exp_2 (hybrid comparison)",
            "exp_3 (ablation)"
          ]
        },
        {
          "benchmark": "SR(70)",
          "used_in_experiments": [
            "exp_1 (main comparison)"
          ]
        },
        {
          "benchmark": "SR(90)",
          "used_in_experiments": [
            "exp_1 (main comparison)"
          ]
        },
        {
          "benchmark": "SR(100)",
          "used_in_experiments": [
            "Training only, not evaluation"
          ]
        },
        {
          "benchmark": "SR(110)",
          "used_in_experiments": [
            "exp_1 (main comparison)"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "DLIS",
          "compared_in_experiments": [
            "exp_1"
          ]
        },
        {
          "baseline": "JW-OS",
          "compared_in_experiments": [
            "exp_1",
            "exp_2"
          ]
        },
        {
          "baseline": "Neural models (various sizes)",
          "compared_in_experiments": [
            "exp_1",
            "exp_2"
          ]
        },
        {
          "baseline": "Architecture without attention",
          "compared_in_experiments": [
            "exp_3"
          ]
        }
      ],
      "overall_narrative": "The three experiments together build a comprehensive case for neural heuristics in SAT solving. Experiment 1 establishes that neural models can compete with and exceed traditional heuristics, particularly on larger problems, validating the core concept. Experiment 2 demonstrates practical improvements through a hybrid approach that combines neural and traditional methods, showing that neural guidance provides real algorithmic benefits in terms of steps required. Experiment 3 validates a key architectural choice (attention mechanism) through ablation, confirming that the performance gains are due to thoughtful design rather than just model capacity.\n\nTogether, these experiments show: (1) neural heuristics are viable for SAT solving, (2) they scale better to larger problems than traditional heuristics, (3) they can be practically deployed in a hybrid system, (4) they work across different SAT-solving algorithms (DPLL and CDCL), and (5) the attention mechanism is a key component of their success. The progression from broad comparison to detailed analysis to architectural validation provides strong evidence for the paper's claims.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "Neural networks can enhance branching heuristics in SAT-solving algorithms",
          "supported_by": [
            "exp_1",
            "exp_2"
          ],
          "strength": "strong"
        },
        {
          "claim": "Neural heuristics perform particularly well on larger SAT problems",
          "supported_by": [
            "exp_1"
          ],
          "strength": "strong"
        },
        {
          "claim": "The attention mechanism improves the message-passing architecture",
          "supported_by": [
            "exp_3"
          ],
          "strength": "moderate"
        },
        {
          "claim": "Hybrid approaches combining neural and traditional heuristics can outperform pure traditional methods",
          "supported_by": [
            "exp_2"
          ],
          "strength": "strong"
        },
        {
          "claim": "Neural guidance transfers across different SAT-solving algorithms (DPLL and CDCL)",
          "supported_by": [
            "exp_2"
          ],
          "strength": "strong"
        }
      ],
      "limitations_acknowledged": [
        "Models show mixed performance on smaller problems (SR(30), SR(50)) compared to JW-OS",
        "Training stability issues: up to 2 out of 5 models per configuration were excluded due to high variance",
        "Evaluation focuses on number of steps rather than wall-clock execution time",
        "The 1000-step limit in Experiment 1 may not reflect real-world usage patterns",
        "Only tested on SR(n) benchmark class, generalization to other SAT problem types unknown"
      ],
      "key_takeaways": [
        "Neural heuristics are most effective on larger, more complex SAT problems (SR(90), SR(110)), where they outperform traditional heuristics like JW-OS",
        "A hybrid approach that combines neural guidance with traditional fallback heuristics achieves better performance than either method alone",
        "The attention mechanism is a valuable architectural component that enables selective message processing and improves performance in most configurations",
        "Neural guidance can enhance both simpler (DPLL) and more advanced (CDCL) SAT-solving algorithms, demonstrating broad applicability",
        "Message-passing graph neural networks can learn effective branching strategies from supervised training on satisfiability and policy labels"
      ]
    }
  },
  {
    "filename": "2110.14053_NeuroBack_Improving_CDCL_SAT_Solving_using_Graph_N.md",
    "paper_title": "NEUROBACK: IMPROVING CDCL SAT SOLVING USING GRAPH NEURAL NETWORKS",
    "paper_focus": "Proposes NeuroBack, a practical GNN-based method that uses a single offline inference to predict variable phases (focusing on backbone variables) to enhance the phase selection heuristic in CDCL SAT solvers, eliminating the need for GPU resources during solving.",
    "experiment_inventory": {
      "total_experiments": 3,
      "main_experiments": 2,
      "ablation_studies": 0,
      "other_experiments": 1
    },
    "experiments": {
      "exp_1": {
        "name": "NeuroBack Model Performance Evaluation",
        "paper_section": "Section 6 (RQ1)",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate the accuracy of the NeuroBack GNN model in classifying the phases of backbone variables.",
        "benchmarks": [
          {
            "name": "DataBack-FT validation set",
            "variant": "10% random split of the fine-tuning dataset",
            "problem_sizes": "Average #Var: 206,470, Average #Cla: 1,218,519"
          }
        ],
        "baselines": [
          {
            "name": "Pre-trained NeuroBack model",
            "description": "Model trained only on the diverse DataBack-PT dataset.",
            "source": "Proposed method (pre-training stage)"
          }
        ],
        "conclusion": {
          "main_finding": "Fine-tuning the pre-trained model on domain-specific data (SAT competition main track formulas) substantially improves its performance on the backbone phase classification task.",
          "supporting_findings": [
            "The pre-trained model achieved a baseline accuracy of 75.1%, with high precision (90.3%) but lower recall (76.6%).",
            "After fine-tuning, all metrics improved: precision to 94.1%, recall to 91.4%, F1 score to 92.8%, and accuracy to 88.7%."
          ],
          "winner": "Fine-tuned NeuroBack model"
        },
        "detailed_description": "This experiment assesses the core predictive capability of the NeuroBack GNN model. The model undergoes a two-stage training process: first, it is pre-trained on the large and diverse DataBack-PT dataset. Second, it is fine-tuned on 90% of the DataBack-FT dataset, which contains larger, more challenging formulas from historical SAT competition main tracks. The remaining 10% of DataBack-FT is held out as a validation set for evaluation. The performance is measured using standard classification metrics: precision, recall, F1 score, and accuracy, for the binary task of predicting the phase (true/false) of backbone variables. The results show that while the pre-trained model already learns generalized knowledge (achieving >90% precision), fine-tuning provides a significant boost across all metrics, especially recall and accuracy. This validates the model's ability to learn the target task and confirms the utility of the two-stage training approach for adapting to the specific problem domain (competition-level SAT instances).\n"
      },
      "exp_2": {
        "name": "NeuroBack Solver Performance on SAT Competitions",
        "paper_section": "Section 6 (RQ2)",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate the solving effectiveness of the NeuroBack-enhanced SAT solver (NeuroBack-Kissat) on recent, challenging SAT competition benchmarks.",
        "benchmarks": [
          {
            "name": "SATCOMP-2022 Main Track",
            "variant": "All 800 CNF formulas",
            "problem_sizes": "Not specified per problem, but the set contains large-scale competition instances."
          },
          {
            "name": "SATCOMP-2023 Main Track",
            "variant": "All 800 CNF formulas",
            "problem_sizes": "Not specified per problem, but the set contains large-scale competition instances."
          }
        ],
        "baselines": [
          {
            "name": "Default-Kissat",
            "description": "The baseline Kissat solver with its default phase initialization heuristic (initial phase of each variable set to true).",
            "source": "State-of-the-art solver used as base for enhancement"
          },
          {
            "name": "Random-Kissat",
            "description": "A variant of Kissat where the initial phase of each variable is assigned randomly (true or false).",
            "source": "Implemented as an additional baseline for comparison"
          }
        ],
        "conclusion": {
          "main_finding": "NeuroBack-Kissat solves more problems within the 5000-second time limit than both baseline solvers on both SATCOMP-2022 and SATCOMP-2023 datasets.",
          "supporting_findings": [
            "On SATCOMP-2022 (308 inferred problems): NeuroBack-Kissat solved 203 problems vs. 193 (Default-Kissat) and 197 (Random-Kissat), representing improvements of 5.2% and 3.0% respectively.",
            "On SATCOMP-2023 (353 inferred problems): NeuroBack-Kissat solved 204 problems vs. 198 (Default-Kissat) and 190 (Random-Kissat), representing improvements of 3.0% and 7.4% respectively.",
            "NeuroBack-Kissat consistently outperforms both baselines in cactus plots, solving more problems earlier in the runtime.",
            "The CPU inference time for NeuroBack is low (avg 1.7 sec, max 16.5 sec), confirming its practical, GPU-free nature."
          ],
          "winner": "NeuroBack-Kissat"
        },
        "detailed_description": "This is the primary end-to-end evaluation of the NeuroBack approach integrated into the Kissat solver. The experiment tests NeuroBack-Kissat against two baseline configurations of Kissat (Default and Random phase initialization) on the complete main track problems from the two most recent SAT competitions at the time of the paper (2022, 2023). A strict time limit of 5000 seconds per problem is enforced. NeuroBack's model inference runs offline on the CPU before solving begins; problems where inference failed (due to a memory threshold) were excluded from the comparative analysis, focusing the evaluation on where NeuroBack's guidance was active. The key metric is the number of problems solved within the time limit. Results are presented in two ways: 1) aggregate counts showing NeuroBack-Kissat solves the most problems, and 2) cactus plots (Figure 4) showing its superior performance curve over time. The scatter plots (Figure 5) further detail per-problem solving time comparisons, showing more points below the diagonal (faster for NeuroBack-Kissat). The experiment conclusively demonstrates that the neural phase predictions provided by NeuroBack lead to a more effective solver, validating the paper's main claim of a practical and effective ML enhancement for CDCL solvers.\n"
      },
      "exp_3": {
        "name": "Performance Analysis on SAT vs. UNSAT Problems",
        "paper_section": "Appendix 10.2",
        "experiment_type": "other",
        "purpose": "To analyze the differential impact of NeuroBack's phase predictions on satisfiable (SAT) versus unsatisfiable (UNSAT) problem instances.",
        "benchmarks": [
          {
            "name": "Subset of SATCOMP-2022 & 2023 problems",
            "variant": "410 problems (194 UNSAT, 216 SAT) solved by either Default-Kissat or NeuroBack-Kissat",
            "problem_sizes": "Not specified"
          }
        ],
        "baselines": [
          {
            "name": "Default-Kissat",
            "description": "The baseline Kissat solver.",
            "source": "State-of-the-art solver"
          },
          {
            "name": "Random-Kissat",
            "description": "Kissat with random phase initialization.",
            "source": "Implemented baseline"
          }
        ],
        "conclusion": {
          "main_finding": "NeuroBack-Kissat improves performance on both SAT and UNSAT problems, but the nature of the improvement differs: it has a higher rate of improvement on UNSAT problems, but a larger average performance gain on SAT problems.",
          "supporting_findings": [
            "On 194 solved UNSAT problems: NeuroBack-Kissat outperformed Default-Kissat in 121 cases (62.4%), while being outperformed in only 61 cases (31.4%). The average improvement was 14.6%.",
            "On 216 solved SAT problems: NeuroBack-Kissat outperformed Default-Kissat in 110 cases (50.9%), while being outperformed in 87 cases (40.3%). The average improvement was 53.2%.",
            "Similar trends were observed versus Random-Kissat."
          ],
          "winner": "NeuroBack-Kissat shows advantages for both categories"
        },
        "detailed_description": "This experiment provides a nuanced analysis of the results from Experiment 2 (exp_2) by separating performance on satisfiable (SAT) and unsatisfiable (UNSAT) problems. From the 661 problems from both competitions that were solved by at least one of the solvers, 410 were analyzed (194 UNSAT, 216 SAT). The analysis compares how often NeuroBack-Kissat was faster than Default-Kissat (improvement rate) and the magnitude of that improvement (average performance gain). The results reveal an interesting dichotomy: NeuroBack-Kissat has a higher *rate* of improvement on UNSAT problems (62.4% vs. 50.9%), meaning it is more consistently helpful. However, the average *magnitude* of improvement is much larger on SAT problems (53.2% vs. 14.6%). The paper offers explanations: for UNSAT problems, the predicted phases may help focus the search on the unsatisfiable part of the space by satisfying some components, indirectly aiding proof search. For SAT problems, the predicted phases can directly contribute to finding a full satisfying assignment, leading to potentially larger time savings when successful. This analysis strengthens the claim of NeuroBack's effectiveness by showing it provides benefits across both major outcome classes of SAT problems.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "NeuroBack Model Performance Evaluation",
        "benchmarks": "DataBack-FT validation set (10% split)",
        "baselines": "Pre-trained model (trained on DataBack-PT)",
        "key_conclusion": "Fine-tuning significantly improves the model's precision, recall, F1 score, and accuracy for classifying backbone variable phases."
      },
      {
        "experiment": "NeuroBack Solver Performance on SAT Competitions",
        "benchmarks": "SATCOMP-2022 Main Track (800 problems), SATCOMP-2023 Main Track (800 problems)",
        "baselines": "Default-Kissat (default phase=true), Random-Kissat (random phase initialization)",
        "key_conclusion": "NeuroBack-Kissat solves more problems than both baselines on both competition sets, demonstrating the effectiveness of the neural phase predictions."
      },
      {
        "experiment": "Performance Analysis on SAT vs. UNSAT Problems",
        "benchmarks": "Subset of solved problems from SATCOMP-2022 & 2023 (194 UNSAT, 216 SAT)",
        "baselines": "Default-Kissat, Random-Kissat",
        "key_conclusion": "NeuroBack-Kissat shows higher improvement rate on UNSAT problems but greater average performance gain on SAT problems."
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "DataBack-FT (validation split)",
          "used_in_experiments": [
            "exp_1"
          ]
        },
        {
          "benchmark": "SATCOMP-2022 Main Track",
          "used_in_experiments": [
            "exp_2",
            "exp_3"
          ]
        },
        {
          "benchmark": "SATCOMP-2023 Main Track",
          "used_in_experiments": [
            "exp_2",
            "exp_3"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "Default-Kissat",
          "compared_in_experiments": [
            "exp_2",
            "exp_3"
          ]
        },
        {
          "baseline": "Random-Kissat",
          "compared_in_experiments": [
            "exp_2",
            "exp_3"
          ]
        },
        {
          "baseline": "Pre-trained NeuroBack model",
          "compared_in_experiments": [
            "exp_1"
          ]
        }
      ],
      "overall_narrative": "The experiments collectively build a compelling case for NeuroBack. First, exp_1 validates the core machine learning component, showing the GNN model can accurately predict backbone variable phases, especially after fine-tuning on competition-like data. Second, exp_2 demonstrates that integrating these predictions into a state-of-the-art solver (Kissat) leads to tangible, measurable improvements in solving effectiveness on the most relevant and challenging benchmarks (recent SAT competitions), outperforming standard and random phase initialization strategies. Finally, exp_3 provides a deeper understanding, showing that the improvement holds for both SAT and UNSAT problems, albeit with different characteristics. Together, they answer the two research questions: the model is accurate (RQ1), and the overall approach is effective (RQ2), leading to a practical, GPU-free enhancement for CDCL solvers.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "NeuroBack can accurately predict the phases of backbone variables using a GNN.",
          "supported_by": [
            "exp_1"
          ],
          "strength": "strong"
        },
        {
          "claim": "Using NeuroBack's offline phase predictions to initialize a CDCL SAT solver (Kissat) makes it solve more problems faster.",
          "supported_by": [
            "exp_2",
            "exp_3"
          ],
          "strength": "strong"
        },
        {
          "claim": "NeuroBack is a practical approach as it requires only a single CPU-based inference before solving, eliminating reliance on GPUs.",
          "supported_by": [
            "exp_2"
          ],
          "strength": "strong"
        }
      ],
      "limitations_acknowledged": [
        "Pre-training requires a large dataset (DataBack-PT).",
        "The current implementation uses phase predictions only for static initialization; dynamic application during solving (e.g., in rephasing) is noted as future work.",
        "Model inference fails on very large formulas exceeding a memory threshold (135 MB in experiments), causing the solver to fall back to its baseline."
      ],
      "key_takeaways": [
        "A GNN model trained to predict backbone variable phases can be effectively transferred to predict phases for all variables, enhancing CDCL phase selection.",
        "Offline, single-query neural inference is a viable and practical strategy for improving SAT solvers, avoiding the computational overhead of online inference.",
        "NeuroBack-Kissat demonstrates statistically significant improvements over strong baselines on recent SAT competition benchmarks, solving 3.0-7.4% more problems."
      ]
    }
  },
  {
    "filename": "2111.07568_Can_Graph_Neural_Networks_Learn_to_Solve_MaxSAT_Pr.md",
    "paper_title": "Can Graph Neural Networks Learn to Solve MaxSAT Problem?",
    "paper_focus": "This paper investigates the capability of Graph Neural Networks (GNNs) to solve the Maximum Satisfiability (MaxSAT) problem, proposing two GNN models (MS-NSFG and MS-ESFG) and providing both theoretical approximation guarantees and extensive practical evaluation.",
    "experiment_inventory": {
      "total_experiments": 4,
      "experiments": [
        {
          "id": "exp_1",
          "name": "Accuracy Evaluation of GNN Models",
          "paper_section": "Experimental Evaluation / Accuracy of Models",
          "type": "main_comparison"
        },
        {
          "id": "exp_2",
          "name": "Influence of GNN Layers (Ablation on Depth)",
          "paper_section": "Experimental Evaluation / Influence of GNN Layers",
          "type": "ablation"
        },
        {
          "id": "exp_3",
          "name": "Generalization to Other Distributions",
          "paper_section": "Experimental Evaluation / Generalizing to Other Distributions",
          "type": "main_comparison"
        },
        {
          "id": "exp_4",
          "name": "Theoretical Approximation Analysis (Algorithm Alignment)",
          "paper_section": "Theoretical Analysis",
          "type": "other"
        }
      ]
    },
    "experiments": {
      "exp_1": {
        "name": "Accuracy Evaluation of GNN Models",
        "paper_section": "Experimental Evaluation / Accuracy of Models",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate the baseline performance and convergence of the two proposed GNN models (MS-NSFG and MS-ESFG) on standard MaxSAT benchmark datasets.",
        "benchmarks": [
          {
            "name": "Random MaxSAT",
            "variant": "R2(60,600)",
            "problem_sizes": "k=2, n=60 variables, m=600 clauses"
          },
          {
            "name": "Random MaxSAT",
            "variant": "R2(60,800)",
            "problem_sizes": "k=2, n=60 variables, m=800 clauses"
          },
          {
            "name": "Random MaxSAT",
            "variant": "R3(30,300)",
            "problem_sizes": "k=3, n=30 variables, m=300 clauses"
          }
        ],
        "baselines": [
          {
            "name": "Optimal Solution (MaxHS)",
            "description": "State-of-the-art exact MaxSAT solver used to generate ground truth labels.",
            "source": "Bacchus 2020"
          },
          {
            "name": "MS-NSFG",
            "description": "Proposed GNN model using Node-Splitting Factor Graph representation.",
            "source": "proposed method"
          },
          {
            "name": "MS-ESFG",
            "description": "Proposed GNN model using Edge-Splitting Factor Graph representation.",
            "source": "proposed method"
          }
        ],
        "conclusion": {
          "main_finding": "Both GNN models converge within 150 epochs and achieve very high accuracy, with approximation ratios >99.5% and assignment accuracies around 92% (Max2SAT) and 83% (Max3SAT).",
          "supporting_findings": [
            "MS-ESFG consistently performs slightly better than MS-NSFG.",
            "The gap to the optimal objective is less than 2 clauses on average for all models and datasets."
          ],
          "winner": "MS-ESFG (marginally)"
        },
        "detailed_description": "This experiment forms the core performance evaluation of the proposed GNN models. Each model (MS-NSFG and MS-ESFG) was trained separately on three distinct datasets: R2(60,600), R2(60,800), and R3(30,300). The training used a standard setup: embedding dimension d=128, T=20 GNN layers, Adam optimizer with learning rate 2e-5. Performance was measured using two primary metrics: 1) the \"gap to optimal objective\" (the difference in the number of satisfied clauses between the predicted and optimal solution), and 2) the \"accuracy of assignments\" (the percentage of variables assigned the correct Boolean value compared to the optimal solution). The results, exemplified by training curves on R2(60,600) shown in Figure 3, demonstrate rapid convergence and high final performance. The average gap was less than 2 clauses, translating to an approximation ratio exceeding 99.5%. Assignment accuracy was approximately 92% for Max2SAT problems and 83% for Max3SAT. The experiment confirms that both GNN architectures are highly effective at learning to solve MaxSAT from data, with MS-ESFG holding a slight edge.\n"
      },
      "exp_2": {
        "name": "Influence of GNN Layers (Ablation on Depth)",
        "paper_section": "Experimental Evaluation / Influence of GNN Layers",
        "experiment_type": "ablation",
        "purpose": "To analyze how the depth (number of message-passing layers T) of the GNN models affects their predictive capability and accuracy.",
        "benchmarks": [
          {
            "name": "Random MaxSAT",
            "variant": "R2(60,600)",
            "problem_sizes": "k=2, n=60, m=600"
          },
          {
            "name": "Random MaxSAT",
            "variant": "R2(60,800)",
            "problem_sizes": "k=2, n=60, m=800"
          },
          {
            "name": "Random MaxSAT",
            "variant": "R3(30,300)",
            "problem_sizes": "k=3, n=30, m=300"
          }
        ],
        "baselines": [
          {
            "name": "MS-NSFG with varying T",
            "description": "The MS-NSFG model trained with T ranging from 1 to 30 layers.",
            "source": "proposed method (ablated)"
          },
          {
            "name": "MS-ESFG with varying T",
            "description": "The MS-ESFG model trained with T ranging from 1 to 30 layers.",
            "source": "proposed method (ablated)"
          },
          {
            "name": "Optimal Solution (MaxHS)",
            "description": "Ground truth for comparison.",
            "source": "Bacchus 2020"
          }
        ],
        "conclusion": {
          "main_finding": "Model performance is highly sensitive to the number of layers; it improves dramatically from T=1 to T=20, then plateaus, with T=1 performing very poorly.",
          "supporting_findings": [
            "At T=1, predicted solutions are far from optimal.",
            "Significant improvement is observed by T=5.",
            "Near-optimal performance (average gap ~1 clause) is achieved at T=20.",
            "No obvious improvement occurs beyond T=20 up to T=60."
          ],
          "winner": "Models with T >= 20 layers"
        },
        "detailed_description": "This is a key ablation study investigating a fundamental hyperparameter: the number of GNN layers (T), which controls the receptive field and computational complexity of the message-passing process. Both MS-NSFG and MS-ESFG were trained on the three primary datasets (R2(60,600), R2(60,800), R3(30,300)) with T varied from 1 to 30. The results, summarized in Figure 4, show a clear trend. A single-layer model (T=1) performs poorly, producing solutions significantly far from the optimal value. This aligns with the theoretical result that a single layer has limited capability (1/2 approximation). Performance improves substantially by T=5. The models reach their peak tested performance at around T=20 layers, where the average gap to the optimal objective is roughly one clause. Increasing T further to 60 showed no measurable improvement, indicating diminishing returns and a potential saturation point. This experiment provides empirical evidence for the importance of multi-step reasoning (multiple message-passing rounds) in solving MaxSAT and validates the choice of T=20 as a standard configuration in other experiments.\n"
      },
      "exp_3": {
        "name": "Generalization to Other Distributions",
        "paper_section": "Experimental Evaluation / Generalizing to Other Distributions",
        "experiment_type": "main_comparison",
        "purpose": "To test the generalization capability of the trained GNN models to MaxSAT instances with different properties: different clause-to-variable ratios, larger problem sizes, and different clause lengths (k).",
        "benchmarks": [
          {
            "name": "Random MaxSAT",
            "variant": "R2(60,600)",
            "problem_sizes": "k=2, n=60, m=600"
          },
          {
            "name": "Random MaxSAT",
            "variant": "R2(60,800)",
            "problem_sizes": "k=2, n=60, m=800"
          },
          {
            "name": "Random MaxSAT",
            "variant": "R3(30,300)",
            "problem_sizes": "k=3, n=30, m=300"
          },
          {
            "name": "Random MaxSAT",
            "variant": "R2(80,800)",
            "problem_sizes": "k=2, n=80, m=800"
          },
          {
            "name": "Random MaxSAT",
            "variant": "R3(50,500)",
            "problem_sizes": "k=3, n=50, m=500"
          }
        ],
        "baselines": [
          {
            "name": "Optimal Solution (MaxHS)",
            "description": "Ground truth for the test sets.",
            "source": "Bacchus 2020"
          },
          {
            "name": "MS-NSFG",
            "description": "Trained on one dataset, tested on others.",
            "source": "proposed method"
          },
          {
            "name": "MS-ESFG",
            "description": "Trained on one dataset, tested on others.",
            "source": "proposed method"
          }
        ],
        "conclusion": {
          "main_finding": "GNN models generalize well to larger problem sizes within the same clause length (k) but poorly across different k (e.g., from 2SAT to 3SAT).",
          "supporting_findings": [
            "Generalization across different clause/variable ratios (e.g., R2(60,600) to R2(60,800)) is excellent, with accuracy nearly matching in-distribution performance.",
            "Generalization to larger, more difficult problems (e.g., R2(60,600) to R2(80,800) or R3(30,300) to R3(50,500)) is satisfactory, showing promise for scaling.",
            "Cross-k generalization (e.g., training on Max2SAT and testing on Max3SAT, or vice versa) is weak, with assignment accuracy dropping below 80% and objective gaps becoming large."
          ],
          "winner": "Models tested on larger instances of the same k (e.g., 2SAT->2SAT)"
        },
        "detailed_description": "This comprehensive experiment evaluates the models' robustness and practical applicability by testing them on out-of-distribution data. Three types of generalization were examined, with results detailed in Tables 2 and 3 for MS-NSFG and MS-ESFG respectively. 1) **Generalizing to different clause-to-variable ratios**: Models trained on R2(60,600) performed almost identically when tested on R2(60,800), and vice versa. This indicates insensitivity to this specific distributional shift. 2) **Generalizing to larger and harder problems**: Models trained on R2(60,600) or R2(60,800) maintained high accuracy when tested on the larger R2(80,800) instance. Similarly, models trained on R3(30,300) generalized well to R3(50,500). This is a strong result, suggesting GNNs could be used to bootstrap solutions for problems too large for exact solvers. 3) **Generalizing across different k**: This was the major limitation. Models trained on Max2SAT datasets performed poorly on Max3SAT test sets, and models trained on Max3SAT performed poorly on Max2SAT. The assignment accuracy remained below 80%, and the gap to optimal objective was significantly larger. This suggests the learned heuristics are specific to the clause structure defined by k. The experiment overall shows promising generalization within a problem class but highlights a key limitation in transferring across fundamentally different constraint types.\n"
      },
      "exp_4": {
        "name": "Theoretical Approximation Analysis (Algorithm Alignment)",
        "paper_section": "Theoretical Analysis",
        "experiment_type": "other",
        "purpose": "To provide a theoretical foundation for why GNNs can solve MaxSAT, by designing a distributed local algorithm (DLA) that aligns with a single-layer GNN and proving it guarantees a 1/2 approximation ratio.",
        "benchmarks": [
          {
            "name": "Theoretical MaxSAT Problem Class",
            "variant": "General k-SAT",
            "problem_sizes": "Arbitrary n, m"
          }
        ],
        "baselines": [
          {
            "name": "Proposed Distributed Local Algorithm (Algorithm 1)",
            "description": "A one-round communication algorithm where clauses pick a literal and literals decide assignment based on weighted votes.",
            "source": "proposed theoretical construct"
          },
          {
            "name": "Optimal Solution",
            "description": "The theoretically optimal MaxSAT solution.",
            "source": "NP-hard problem definition"
          }
        ],
        "conclusion": {
          "main_finding": "A single-layer GNN can theoretically achieve a 1/2 approximation ratio for MaxSAT by aligning with the proposed distributed local algorithm.",
          "supporting_findings": [
            "The designed DLA (Algorithm 1) can be exactly simulated by a single-layer GNN (Lemma 2).",
            "The DLA itself is a 1/2-approximation algorithm for MaxSAT (Lemma 3)."
          ],
          "winner": "The single-layer GNN aligned with Algorithm 1"
        },
        "detailed_description": "This is not a traditional empirical experiment but a theoretical analysis constituting a major contribution of the paper. It addresses the \"why\" behind GNN efficacy for MaxSAT. The authors first design a novel distributed local algorithm (DLA) for MaxSAT (Algorithm 1). This algorithm operates in one round: first, clauses send their composition to literals; then, each clause picks one literal and gives it a \"vote\"; finally, each literal is assigned True if its votes are >= the votes for its negation. The core theoretical result (Theorem 1) has two parts, proved via Lemmas 2 and 3 in the Appendix. Lemma 2 constructs a single-layer GNN whose components (aggregation and update functions) can exactly simulate each step of the DLA, proving alignment. Lemma 3 proves the DLA itself is a 1/2-approximation algorithm. The intuition is that the algorithm reduces the problem to a Max1SAT instance and then makes a greedy choice satisfying at least half the clauses. Therefore, by alignment, a single-layer GNN can also guarantee a 1/2 approximation. This theory explains the poor performance of the T=1 model in Experiment 2 and sets a baseline for what a minimal GNN can achieve, providing a foundation for understanding the improvements gained with more layers.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "Accuracy Evaluation of GNN Models",
        "benchmarks": "R2(60,600), R2(60,800), R3(30,300)",
        "baselines": "Optimal solution from MaxHS solver",
        "key_conclusion": "Both MS-NSFG and MS-ESFG achieve high accuracy (>99.5% approximation ratio) on their training distributions, with MS-ESFG performing slightly better."
      },
      {
        "experiment": "Influence of GNN Layers (Ablation on Depth)",
        "benchmarks": "R2(60,600), R2(60,800), R3(30,300)",
        "baselines": "Models with varying layers T=1 to T=30, optimal solution",
        "key_conclusion": "Model performance improves significantly with more layers up to T=20 (near-optimal), but plateaus thereafter; a single layer (T=1) performs poorly."
      },
      {
        "experiment": "Generalization to Other Distributions",
        "benchmarks": "R2(60,600), R2(60,800), R3(30,300), R2(80,800), R3(50,500)",
        "baselines": "Optimal solution from MaxHS solver, cross-dataset comparisons",
        "key_conclusion": "Models generalize well to larger problems within same k (e.g., 2SAT->2SAT) but poorly across different k (e.g., 2SAT->3SAT)."
      },
      {
        "experiment": "Theoretical Approximation Analysis (Algorithm Alignment)",
        "benchmarks": "Theoretical MaxSAT problem class",
        "baselines": "Proposed Distributed Local Algorithm (Algorithm 1), optimal solution",
        "key_conclusion": "A single-layer GNN can theoretically achieve a 1/2 approximation ratio for MaxSAT by aligning with the proposed distributed local algorithm."
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "Random MaxSAT R2(60,600)",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3"
          ]
        },
        {
          "benchmark": "Random MaxSAT R2(60,800)",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3"
          ]
        },
        {
          "benchmark": "Random MaxSAT R3(30,300)",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3"
          ]
        },
        {
          "benchmark": "Random MaxSAT R2(80,800)",
          "used_in_experiments": [
            "exp_3"
          ]
        },
        {
          "benchmark": "Random MaxSAT R3(50,500)",
          "used_in_experiments": [
            "exp_3"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "Optimal Solution (MaxHS)",
          "compared_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3"
          ]
        },
        {
          "baseline": "MS-NSFG",
          "compared_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3"
          ]
        },
        {
          "baseline": "MS-ESFG",
          "compared_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3"
          ]
        },
        {
          "baseline": "Distributed Local Algorithm (Theory)",
          "compared_in_experiments": [
            "exp_4"
          ]
        }
      ],
      "overall_narrative": "The experiments together tell a cohesive story of capability and limitation. The theoretical experiment (exp_4) establishes a foundational lower bound: a single-layer GNN can guarantee a 1/2 approximation. The ablation study (exp_2) empirically confirms the weakness of a single layer and shows that increasing depth is the key to surpassing this theoretical bound, achieving near-optimal performance. The main performance experiment (exp_1) demonstrates that with sufficient depth (T=20), both GNN models can learn to solve MaxSAT with remarkable accuracy on standard benchmarks. Finally, the generalization experiment (exp_3) explores the boundaries of this learned capability, revealing strong scaling potential to larger problems within the same class but a fundamental limitation in transferring knowledge across problems with different clause lengths (k). The narrative is that GNNs are powerful, data-driven approximators for MaxSAT that exceed basic theoretical guarantees through depth, but their learned heuristics are domain-specific to the training distribution's structure.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "GNNs can learn to solve MaxSAT problem with high accuracy in an end-to-end fashion.",
          "supported_by": [
            "exp_1"
          ],
          "strength": "strong"
        },
        {
          "claim": "Increasing the number of GNN layers (depth) is crucial for achieving high performance.",
          "supported_by": [
            "exp_2"
          ],
          "strength": "strong"
        },
        {
          "claim": "GNN models generalize well to larger problem sizes but poorly across different clause lengths (k).",
          "supported_by": [
            "exp_3"
          ],
          "strength": "strong"
        },
        {
          "claim": "There is a theoretical basis (algorithmic alignment) explaining why GNNs can solve MaxSAT, guaranteeing at least a 1/2 approximation for a single layer.",
          "supported_by": [
            "exp_4"
          ],
          "strength": "strong"
        }
      ],
      "limitations_acknowledged": [
        "Poor generalization from Max2SAT to Max3SAT problems and vice versa (cross-k generalization).",
        "Performance of single-layer GNNs is weak, aligning with the theoretical 1/2-approximation bound.",
        "Models are evaluated on random benchmarks; performance on structured, real-world MaxSAT instances is untested."
      ],
      "key_takeaways": [
        "GNNs, particularly the MS-ESFG model, are highly effective at solving random MaxSAT instances, achieving >99.5% approximation ratios.",
        "Model depth (number of message-passing layers) is a critical factor for performance, with ~20 layers being optimal for the tested scales.",
        "The learned models show promising generalization to larger instances but are specialized to the clause length (k) seen during training.",
        "The paper provides the first theoretical explanation for GNN efficacy on MaxSAT, linking it to distributed local algorithms and providing a provable approximation guarantee."
      ]
    }
  },
  {
    "filename": "2205.04423_Graph_Neural_Networks_for_Propositional_Model_Coun.md",
    "paper_title": "Graph Neural Networks for Propositional Model Counting",
    "paper_focus": "Proposes BPGAT, a Graph Attention Network architecture extending Belief Propagation Neural Networks (BPNN) to approximately solve the #SAT problem, demonstrating scalability, generalization via fine-tuning, and competitive performance against approximate solvers.",
    "experiment_inventory": {
      "total_experiments": 7,
      "experiments": [
        {
          "id": "exp_1",
          "name": "Scalability Test on Random Boolean Formulae",
          "paper_section": "Section 4.2 (Scalability)",
          "type": "main_comparison"
        },
        {
          "id": "exp_2",
          "name": "Generalization Test on SAT-encoded Combinatorial Problems (Baseline Comparison)",
          "paper_section": "Section 4.2 (Out-of-distribution generalization)",
          "type": "main_comparison"
        },
        {
          "id": "exp_3",
          "name": "Fine-tuning vs. Training from Scratch on OOD Problems",
          "paper_section": "Section 4.2 (Out-of-distribution generalization)",
          "type": "ablated_training_regime"
        },
        {
          "id": "exp_4",
          "name": "Comparison of BPGAT vs. BPNN on Scalability",
          "paper_section": "Appendix B",
          "type": "model_comparison"
        },
        {
          "id": "exp_5",
          "name": "Comparison of BPGAT vs. BPNN on Generalization under Different Training Regimes",
          "paper_section": "Appendix B",
          "type": "model_comparison"
        },
        {
          "id": "exp_6",
          "name": "BPGAT Parameter Tuning Ablation (Damping and Iterations)",
          "paper_section": "Appendix C.1",
          "type": "ablation"
        },
        {
          "id": "exp_7",
          "name": "Ablation Studies on GAT Layers (Message Transformation)",
          "paper_section": "Appendix C.2",
          "type": "ablation"
        }
      ]
    },
    "experiments": {
      "exp_1": {
        "name": "Scalability Test on Random Boolean Formulae",
        "paper_section": "Section 4.2 (Scalability)",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate if BPGAT, trained on small random formulae (10-30 vars, 20-50 clauses), can scale effectively to much larger problem sizes compared to a state-of-the-art approximate solver.",
        "benchmarks": [
          {
            "name": "Random Boolean Formulae",
            "variant": "Test 1",
            "problem_sizes": "Avg 61.8 vars, 76.89 clauses"
          },
          {
            "name": "Random Boolean Formulae",
            "variant": "Test 2",
            "problem_sizes": "Avg 60.43 vars, 143.61 clauses"
          },
          {
            "name": "Random Boolean Formulae",
            "variant": "Test 3",
            "problem_sizes": "Avg 124.07 vars, 75.26 clauses"
          },
          {
            "name": "Random Boolean Formulae",
            "variant": "Test 4",
            "problem_sizes": "Avg 377.59 vars, 275.11 clauses"
          }
        ],
        "baselines": [
          {
            "name": "ApproxMC",
            "description": "State-of-the-art randomized hashing approximate #SAT solver with PAC guarantees.",
            "source": "[9,23]"
          }
        ],
        "conclusion": {
          "main_finding": "BPGAT consistently outperforms ApproxMC in terms of Mean Relative Error (MRE) across all four significantly larger test sets, demonstrating strong scalability.",
          "supporting_findings": [
            "BPGAT achieves lower MRE: 0.001366 vs 0.001576 (Test 1), 0.003201 vs 0.02003 (Test 2), 0.001471 vs 0.01134 (Test 3), 0.007433 vs 0.04038 (Test 4).",
            "ApproxMC achieves lower Root Mean Squared Error (RMSE), attributed to BPGAT having a few outliers with large prediction errors, while most predictions are very close to ground truth."
          ],
          "winner": "BPGAT (based on MRE, the primary metric for approximation quality)"
        },
        "detailed_description": "This experiment tests the scalability of the proposed BPGAT model. The model was trained on a dataset of 1000 small random Boolean formulae (10-30 variables, 20-50 clauses). It was then evaluated without any modification on four new test sets of random formulae, each containing 300 instances, with sizes far exceeding the training distribution (up to ~378 variables and ~275 clauses). The primary baseline is ApproxMC, a leading approximate #SAT solver. Performance was measured using Root Mean Squared Error (RMSE) and Mean Relative Error (MRE) between the predicted log-count (ln áº) and the true log-count (ln Z) obtained via the exact solver sharpSAT. While ApproxMC had lower RMSE, BPGAT achieved superior MRE across all test sets. This indicates BPGAT's predictions are, on average, proportionally closer to the true count, even on vastly larger instances, validating its scalability. The higher RMSE for BPGAT is attributed to a small number of outliers, as visualized in Figure 1 of the paper.\n"
      },
      "exp_2": {
        "name": "Generalization Test on SAT-encoded Combinatorial Problems (Baseline Comparison)",
        "paper_section": "Section 4.2 (Out-of-distribution generalization)",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate the generalization capability of fine-tuned BPGAT (FT_BPGAT) on out-of-distribution (OOD) problem classes and compare it directly against ApproxMC.",
        "benchmarks": [
          {
            "name": "Network (QMR)",
            "variant": "Quick Medical Reference problems",
            "problem_sizes": "Avg 113.3 vars, 294.7 clauses"
          },
          {
            "name": "k-Dominating Set",
            "variant": "Encoded via CNFgen, Erdos-Renyi G(N=15, p=0.6), k=3",
            "problem_sizes": "Avg 38.43 vars, 510.15 clauses"
          },
          {
            "name": "Graph k-Coloring",
            "variant": "Encoded via CNFgen, Erdos-Renyi G(N=10, p=0.6), k=3",
            "problem_sizes": "Avg 65.63 vars, 294.54 clauses"
          },
          {
            "name": "k-Clique Detection",
            "variant": "Encoded via CNFgen, Erdos-Renyi G(N=15, p=0.5), k=3",
            "problem_sizes": "Avg 46.37 vars, 1145.73 clauses"
          }
        ],
        "baselines": [
          {
            "name": "ApproxMC",
            "description": "State-of-the-art approximate #SAT solver.",
            "source": "[9,23]"
          }
        ],
        "conclusion": {
          "main_finding": "The fine-tuned BPGAT model (FT_BPGAT) achieves performance comparable to and sometimes better than ApproxMC on diverse OOD combinatorial problems, particularly excelling in Mean Relative Error (MRE).",
          "supporting_findings": [
            "For Clique detection, FT_BPGAT significantly outperforms ApproxMC in MRE (0.002475 vs 0.04795).",
            "For Network, Domset, and Color problems, FT_BPGAT has lower MRE than ApproxMC, though ApproxMC often has lower RMSE.",
            "This demonstrates BPGAT can generalize effectively to structurally different formula distributions with limited fine-tuning data."
          ],
          "winner": "FT_BPGAT (based on superior or competitive MRE across all OOD benchmarks)"
        },
        "detailed_description": "This experiment evaluates the out-of-distribution generalization of BPGAT after a targeted fine-tuning phase. The model, pre-trained on random Boolean formulae, was fine-tuned on just 250 examples from each of four distinct problem distributions: Network (QMR diagnostic problems), k-Dominating Set, Graph k-Coloring, and k-Clique Detection (all SAT-encoded via CNFgen). The fine-tuned model (FT_BPGAT) was then compared against the approximate solver ApproxMC on held-out test instances from these distributions. The results show that FT_BPGAT is highly competitive. It matches or surpasses ApproxMC's performance, especially when considering the Mean Relative Error (MRE), which measures proportional accuracy. For instance, on the Clique benchmark, FT_BPGAT's MRE is nearly 20 times lower than ApproxMC's. This experiment successfully demonstrates that BPGAT can adapt to new, complex problem domains with minimal additional labeled data, fulfilling the generalization desideratum.\n"
      },
      "exp_3": {
        "name": "Fine-tuning vs. Training from Scratch on OOD Problems",
        "paper_section": "Section 4.2 (Out-of-distribution generalization)",
        "experiment_type": "ablated_training_regime",
        "purpose": "To determine the most data-efficient strategy for adapting BPGAT to new formula distributions: fine-tuning a pre-trained model vs. training a new model from scratch on the target distribution.",
        "benchmarks": [
          {
            "name": "Network (QMR)",
            "variant": "Quick Medical Reference problems",
            "problem_sizes": "Avg 113.3 vars, 294.7 clauses"
          },
          {
            "name": "k-Dominating Set",
            "variant": "Encoded via CNFgen, Erdos-Renyi G(N=15, p=0.6), k=3",
            "problem_sizes": "Avg 38.43 vars, 510.15 clauses"
          },
          {
            "name": "Graph k-Coloring",
            "variant": "Encoded via CNFgen, Erdos-Renyi G(N=10, p=0.6), k=3",
            "problem_sizes": "Avg 65.63 vars, 294.54 clauses"
          },
          {
            "name": "k-Clique Detection",
            "variant": "Encoded via CNFgen, Erdos-Renyi G(N=15, p=0.5), k=3",
            "problem_sizes": "Avg 46.37 vars, 1145.73 clauses"
          }
        ],
        "baselines": [
          {
            "name": "FT_BPGAT",
            "description": "BPGAT pre-trained on random formulae, then fine-tuned on 250 OOD examples.",
            "source": "proposed method"
          },
          {
            "name": "TS_BPGAT",
            "description": "BPGAT trained from scratch for 500 epochs on 250 OOD examples.",
            "source": "proposed method (ablated variant)"
          },
          {
            "name": "BPGAT",
            "description": "BPGAT trained only on random formulae, with no exposure to OOD distributions.",
            "source": "proposed method (baseline)"
          }
        ],
        "conclusion": {
          "main_finding": "Fine-tuning a model pre-trained on random formulae (FT_BPGAT) yields significantly better performance on OOD tasks than training an identical model from scratch (TS_BPGAT) on the same small OOD dataset.",
          "supporting_findings": [
            "FT_BPGAT consistently achieves lower RMSE and MRE than TS_BPGAT across all four OOD benchmarks.",
            "The vanilla BPGAT (trained only on random data) performs the worst, as expected, highlighting the distribution shift.",
            "This demonstrates that knowledge transfer from pre-training on easily generated random data is highly effective and data-efficient."
          ],
          "winner": "FT_BPGAT (Fine-tuned model)"
        },
        "detailed_description": "This experiment is an ablation on the training strategy to assess data efficiency. Three versions of BPGAT were compared on the out-of-distribution benchmarks: 1) FT_BPGAT: the model pre-trained on random formulae and then fine-tuned on 250 distribution-specific examples, 2) TS_BPGAT: a model with the same architecture but trained from scratch for 500 epochs on the same 250 OOD examples, and 3) the original BPGAT model trained solely on random data. The results clearly show the superiority of the fine-tuning approach. FT_BPGAT outperforms TS_BPGAT by a large margin in all cases (e.g., RMSE of 0.2580 vs 1.9334 on Network). This indicates that the features learned during pre-training on random formulae are broadly useful and provide a much stronger starting point for adaptation than random initialization. This finding is crucial for practical applications where labeled data for specific complex problem distributions is scarce but synthetic random data is cheap to generate.\n"
      },
      "exp_4": {
        "name": "Comparison of BPGAT vs. BPNN on Scalability",
        "paper_section": "Appendix B",
        "experiment_type": "model_comparison",
        "purpose": "To directly compare the performance of the proposed BPGAT architecture against its predecessor, BPNN (which lacks the attention mechanism), on the task of scaling to larger random Boolean formulae.",
        "benchmarks": [
          {
            "name": "Random Boolean Formulae",
            "variant": "Test 1",
            "problem_sizes": "Avg 61.8 vars, 76.89 clauses"
          },
          {
            "name": "Random Boolean Formulae",
            "variant": "Test 2",
            "problem_sizes": "Avg 60.43 vars, 143.61 clauses"
          },
          {
            "name": "Random Boolean Formulae",
            "variant": "Test 3",
            "problem_sizes": "Avg 124.07 vars, 75.26 clauses"
          },
          {
            "name": "Random Boolean Formulae",
            "variant": "Test 4",
            "problem_sizes": "Avg 377.59 vars, 275.11 clauses"
          }
        ],
        "baselines": [
          {
            "name": "BPNN",
            "description": "Belief Propagation Neural Network, the baseline architecture from [15] which BPGAT extends.",
            "source": "[15]"
          }
        ],
        "conclusion": {
          "main_finding": "BPGAT consistently and significantly outperforms the BPNN architecture across all scalability test sets, validating the benefit of incorporating the graph attention mechanism.",
          "supporting_findings": [
            "BPGAT achieves lower RMSE and MRE than BPNN on every dataset (Test 1-4).",
            "The performance gap is substantial, e.g., on Test 2, BPGAT MRE is 0.003201 vs BPNN's 0.01786."
          ],
          "winner": "BPGAT"
        },
        "detailed_description": "This experiment provides a direct architectural comparison to isolate the impact of the added attention mechanism. Both BPGAT and BPNN were trained on the same dataset of 1000 small random formulae (using their respective training protocols) and then evaluated on the four larger-scale test sets (Test 1-4). BPNN serves as an ablation baselineâ€”it is the same underlying BP-inspired GNN framework but without the GAT-style attention layers in the message passing steps. The results are unequivocal: BPGAT achieves superior accuracy (lower RMSE and MRE) on every test set. For example, on the challenging Test 4 set, BPGAT's MRE is less than half that of BPNN (0.007433 vs 0.01681). This experiment conclusively demonstrates that the attention mechanism is not merely an addition but a critical improvement that enhances the model's ability to approximate the model count accurately, especially when scaling to larger, more complex factor graphs.\n"
      },
      "exp_5": {
        "name": "Comparison of BPGAT vs. BPNN on Generalization under Different Training Regimes",
        "paper_section": "Appendix B",
        "experiment_type": "model_comparison",
        "purpose": "To compare the generalization capabilities of BPGAT and BPNN under three training scenarios: fine-tuning (FT), training from scratch on OOD data (TS), and using the model pre-trained only on random data.",
        "benchmarks": [
          {
            "name": "Network (QMR)",
            "variant": "Quick Medical Reference problems",
            "problem_sizes": "Avg 113.3 vars, 294.7 clauses"
          },
          {
            "name": "k-Dominating Set",
            "variant": "Encoded via CNFgen, Erdos-Renyi G(N=15, p=0.6), k=3",
            "problem_sizes": "Avg 38.43 vars, 510.15 clauses"
          },
          {
            "name": "Graph k-Coloring",
            "variant": "Encoded via CNFgen, Erdos-Renyi G(N=10, p=0.6), k=3",
            "problem_sizes": "Avg 65.63 vars, 294.54 clauses"
          },
          {
            "name": "k-Clique Detection",
            "variant": "Encoded via CNFgen, Erdos-Renyi G(N=15, p=0.5), k=3",
            "problem_sizes": "Avg 46.37 vars, 1145.73 clauses"
          }
        ],
        "baselines": [
          {
            "name": "FT_BPNN",
            "description": "BPNN pre-trained on random formulae, then fine-tuned on OOD examples.",
            "source": "[15] (implemented for comparison)"
          },
          {
            "name": "TS_BPNN",
            "description": "BPNN trained from scratch on OOD examples.",
            "source": "[15] (implemented for comparison)"
          },
          {
            "name": "BPNN",
            "description": "BPNN trained only on random formulae.",
            "source": "[15]"
          }
        ],
        "conclusion": {
          "main_finding": "BPGAT outperforms BPNN across all out-of-distribution benchmarks and under every training regime (fine-tuned, trained from scratch, or pre-trained only), confirming the general superiority of the attentive architecture.",
          "supporting_findings": [
            "In the fine-tuned regime (FT), BPGAT has lower RMSE/MRE than BPNN for all four OOD datasets.",
            "The same trend holds for the training-from-scratch (TS) and pre-trained-only regimes.",
            "The performance gap is often large, e.g., on the Color benchmark with fine-tuning, BPGAT MRE is 0.1774 vs BPNN's 0.8046."
          ],
          "winner": "BPGAT"
        },
        "detailed_description": "This extensive comparison experiment evaluates whether the advantage of BPGAT over BPNN holds specifically in the context of generalization to unseen problem distributions. The authors tested both architectures under three distinct training conditions relevant to generalization: 1) Fine-tuning a pre-trained model on a small OOD dataset (FT), 2) Training a new model from scratch on a small OOD dataset (TS), and 3) Using the model trained only on random data without any OOD adaptation. For each condition and each of the four OOD benchmarks (Network, Domset, Color, Clique), BPGAT achieved lower RMSE and MRE than the corresponding BPNN variant. This demonstrates that the attention mechanism in BPGAT provides a fundamental boost in learning capacity and transferability, making it more effective at capturing essential reasoning patterns that generalize beyond its initial training distribution. The results strongly justify the architectural innovation proposed in the paper.\n"
      },
      "exp_6": {
        "name": "BPGAT Parameter Tuning Ablation (Damping and Iterations)",
        "paper_section": "Appendix C.1",
        "experiment_type": "ablation",
        "purpose": "To determine the optimal configuration for the damping parameter (learned vs. fixed) and the number of message passing iterations (T) in the BPGAT architecture.",
        "benchmarks": [
          {
            "name": "Random Boolean Formulae",
            "variant": "Test 1",
            "problem_sizes": "Avg 61.8 vars, 76.89 clauses"
          },
          {
            "name": "Random Boolean Formulae",
            "variant": "Test 2",
            "problem_sizes": "Avg 60.43 vars, 143.61 clauses"
          },
          {
            "name": "Random Boolean Formulae",
            "variant": "Test 3",
            "problem_sizes": "Avg 124.07 vars, 75.26 clauses"
          }
        ],
        "baselines": [
          {
            "name": "BPGAT_VF",
            "description": "Uses learned MLP Î” for variable-to-factor messages, fixed Î±=0.5 for factor-to-variable.",
            "source": "proposed method (ablated variant)"
          },
          {
            "name": "BPGAT_ALL",
            "description": "Uses learned MLP Î” for both variable-to-factor and factor-to-variable messages.",
            "source": "proposed method (ablated variant)"
          },
          {
            "name": "BPGAT_NONE",
            "description": "Uses fixed scalar damping Î±=0.5 for both types of messages.",
            "source": "proposed method (ablated variant)"
          },
          {
            "name": "BPGAT_10",
            "description": "Uses T=10 message passing iterations.",
            "source": "proposed method (ablated variant)"
          },
          {
            "name": "BPGAT_15",
            "description": "Uses T=15 message passing iterations.",
            "source": "proposed method (ablated variant)"
          }
        ],
        "conclusion": {
          "main_finding": "The optimal BPGAT configuration uses a learned MLP damping operator (Î”) only for factor-to-variable messages (with variable-to-factor damping fixed at Î±=0.5) and performs T=5 message passing iterations.",
          "supporting_findings": [
            "BPGAT (the chosen config) outperformed BPGAT_VF, BPGAT_ALL, and BPGAT_NONE on Test 1-3.",
            "Increasing T to 10 or 15 iterations (BPGAT_10, BPGAT_15) degraded performance compared to T=5."
          ],
          "winner": "BPGAT (T=5, Î” on factor-to-variable only)"
        },
        "detailed_description": "This is a series of ablation studies focused on hyperparameter tuning. The first set explores the configuration of the damping mechanism, which is used to stabilize message updates. The authors tested four variants: the final chosen one (BPGAT), one where the learned operator is applied to variable-to-factor messages instead (BPGAT_VF), one where it's applied to both (BPGAT_ALL), and one with simple fixed damping for both (BPGAT_NONE). Results on Test 1-3 showed the chosen configuration performed best, justifying the specific design choice. The second set of ablations varied the number of message passing iterations T. Comparing T=5 (BPGAT), T=10 (BPGAT_10), and T=15 (BPGAT_15) revealed that performance peaked at T=5. More iterations did not improve accuracy and in fact worsened it, suggesting potential over-smoothing or that the essential information for the #SAT estimation is captured within a few propagation steps. These studies were crucial for establishing the specific architectural details reported in Section 4.1.\n"
      },
      "exp_7": {
        "name": "Ablation Studies on GAT Layers (Message Transformation)",
        "paper_section": "Appendix C.2",
        "experiment_type": "ablation",
        "purpose": "To isolate the contribution of the GAT attention mechanism by testing hybrid models that apply attention to only one message type (factor-to-variable or variable-to-factor) or replace one attention block with a simpler transformation (BP or BPNN-style MLP).",
        "benchmarks": [
          {
            "name": "Random Boolean Formulae",
            "variant": "Test 1",
            "problem_sizes": "Avg 61.8 vars, 76.89 clauses"
          },
          {
            "name": "Random Boolean Formulae",
            "variant": "Test 2",
            "problem_sizes": "Avg 60.43 vars, 143.61 clauses"
          },
          {
            "name": "Random Boolean Formulae",
            "variant": "Test 3",
            "problem_sizes": "Avg 124.07 vars, 75.26 clauses"
          }
        ],
        "baselines": [
          {
            "name": "FVGAT-VFNONE",
            "description": "Uses GAT for factor-to-variable messages, standard BP (no neural transformation) for variable-to-factor messages.",
            "source": "proposed method (ablated variant)"
          },
          {
            "name": "FVNONE-VFGAT",
            "description": "Uses standard BP for factor-to-variable messages, GAT for variable-to-factor messages.",
            "source": "proposed method (ablated variant)"
          },
          {
            "name": "FVGAT-VFMLP",
            "description": "Uses GAT for factor-to-variable messages, BPNN-style MLP for variable-to-factor messages.",
            "source": "proposed method (ablated variant)"
          },
          {
            "name": "FVMLP-VFGAT",
            "description": "Uses BPNN-style MLP for factor-to-variable messages, GAT for variable-to-factor messages.",
            "source": "proposed method (ablated variant)"
          }
        ],
        "conclusion": {
          "main_finding": "The full BPGAT architecture, which uses GAT attention mechanisms to transform both factor-to-variable AND variable-to-factor messages, delivers the best performance, confirming that attention is beneficial in both directions of message passing.",
          "supporting_findings": [
            "All hybrid models (FVGAT-VFNONE, FVNONE-VFGAT, FVGAT-VFMLP, FVMLP-VFGAT) performed worse than the full BPGAT.",
            "This indicates the dual attention mechanism creates a synergistic effect for accurate #SAT approximation."
          ],
          "winner": "BPGAT (Full dual-attention model)"
        },
        "detailed_description": "This experiment is a core ablation study designed to deconstruct the BPGAT architecture and understand the role of the attention mechanism in each message-passing direction. The authors created several hybrid models. One set combined GAT with the standard, non-learned Belief Propagation update for one message type (FVGAT-VFNONE and FVNONE-VFGAT). Another set combined GAT with the learned but non-attentive MLP transformation from the BPNN architecture for one message type (FVGAT-VFMLP and FVMLP-VFGAT). All these partial-attention variants were evaluated on the scalability test sets (Test 1-3). The results consistently showed that the complete BPGAT model, employing attention for both factor-to-variable and variable-to-factor message transformations, achieved the lowest error. This provides strong evidence that the self-attention mechanism is not merely a minor enhancement but a integral component that needs to be applied symmetrically in the bipartite message-passing scheme to achieve the full performance gain. It suggests that learning to focus on relevant neighboring clauses and variables dynamically is crucial in both directions for accurate model counting.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "Scalability Test on Random Boolean Formulae",
        "benchmarks": "Test 1, Test 2, Test 3, Test 4 (random Boolean formulae)",
        "baselines": "ApproxMC",
        "key_conclusion": "BPGAT outperforms ApproxMC in Mean Relative Error (MRE) across all larger-scale test sets, demonstrating effective scalability, though has higher RMSE due to few outliers."
      },
      {
        "experiment": "Generalization Test on SAT-encoded Combinatorial Problems (Baseline Comparison)",
        "benchmarks": "Network (QMR), Domset (k-dominating set), Color (k-coloring), Clique (k-clique detection)",
        "baselines": "ApproxMC",
        "key_conclusion": "Fine-tuned BPGAT (FT_BPGAT) achieves comparable or better performance (especially in MRE) than ApproxMC on diverse OOD problem classes."
      },
      {
        "experiment": "Fine-tuning vs. Training from Scratch on OOD Problems",
        "benchmarks": "Network, Domset, Color, Clique",
        "baselines": "BPGAT (pre-trained), TS_BPGAT (trained from scratch)",
        "key_conclusion": "Fine-tuning a model pre-trained on random formulae (FT_BPGAT) yields better performance than training from scratch (TS_BPGAT) on the same small OOD dataset, highlighting data-efficiency."
      },
      {
        "experiment": "Comparison of BPGAT vs. BPNN on Scalability",
        "benchmarks": "Test 1, Test 2, Test 3, Test 4 (random Boolean formulae)",
        "baselines": "BPNN",
        "key_conclusion": "BPGAT consistently outperforms BPNN (its non-attentional predecessor) in both RMSE and MRE across all scalability test sets, validating the benefit of the attention mechanism."
      },
      {
        "experiment": "Comparison of BPGAT vs. BPNN on Generalization under Different Training Regimes",
        "benchmarks": "Network, Domset, Color, Clique",
        "baselines": "FT_BPNN, TS_BPNN, BPNN (pre-trained)",
        "key_conclusion": "BPGAT outperforms BPNN under all training regimes (fine-tuned, trained from scratch, pre-trained) for OOD generalization, confirming the architectural advantage."
      },
      {
        "experiment": "BPGAT Parameter Tuning Ablation (Damping and Iterations)",
        "benchmarks": "Test 1, Test 2, Test 3",
        "baselines": "BPGAT_VF, BPGAT_ALL, BPGAT_NONE, BPGAT_10, BPGAT_15",
        "key_conclusion": "Optimal configuration uses a learned MLP damping operator (Î”) only for factor-to-variable messages (fixed Î±=0.5 for variable-to-factor) and T=5 message passing iterations."
      },
      {
        "experiment": "Ablation Studies on GAT Layers (Message Transformation)",
        "benchmarks": "Test 1, Test 2, Test 3",
        "baselines": "FVGAT-VFNONE, FVNONE-VFGAT, FVGAT-VFMLP, FVMLP-VFGAT",
        "key_conclusion": "Using GAT attention for transforming both factor-to-variable AND variable-to-factor messages (full BPGAT) yields the best performance, confirming the importance of the dual attention mechanism."
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "Random Boolean Formulae (Test 1-4)",
          "used_in_experiments": [
            "exp_1",
            "exp_4",
            "exp_6",
            "exp_7"
          ]
        },
        {
          "benchmark": "Network (QMR) problems",
          "used_in_experiments": [
            "exp_2",
            "exp_3",
            "exp_5"
          ]
        },
        {
          "benchmark": "k-Dominating Set (Domset)",
          "used_in_experiments": [
            "exp_2",
            "exp_3",
            "exp_5"
          ]
        },
        {
          "benchmark": "Graph k-Coloring (Color)",
          "used_in_experiments": [
            "exp_2",
            "exp_3",
            "exp_5"
          ]
        },
        {
          "benchmark": "k-Clique Detection (Clique)",
          "used_in_experiments": [
            "exp_2",
            "exp_3",
            "exp_5"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "ApproxMC",
          "compared_in_experiments": [
            "exp_1",
            "exp_2"
          ]
        },
        {
          "baseline": "BPNN (and its variants FT/TS)",
          "compared_in_experiments": [
            "exp_4",
            "exp_5"
          ]
        }
      ],
      "overall_narrative": "The experiments collectively tell a cohesive story validating the BPGAT architecture. First, against the external state-of-the-art (ApproxMC), BPGAT demonstrates competitive scalability (Exp 1) and strong generalization when fine-tuned (Exp 2). Second, against its direct predecessor (BPNN), BPGAT shows clear and consistent superiority in both scalability (Exp 4) and generalization under all training regimes (Exp 5), proving the value of the added attention mechanism. Third, internal ablation studies (Exp 3, 6, 7) fine-tune the model design and training strategy: they show fine-tuning is the optimal adaptation method (Exp 3), establish the best hyperparameters for damping and iterations (Exp 6), and confirm that the dual-attention mechanism is necessary for peak performance (Exp 7). Together, they robustly support the paper's claims that BPGAT is a scalable, generalizable, data-efficient, and time-efficient approximate #SAT solver.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "BPGAT can scale effectively to problem sizes much larger than those seen during training.",
          "supported_by": [
            "exp_1",
            "exp_4"
          ],
          "strength": "strong"
        },
        {
          "claim": "BPGAT can generalize to out-of-distribution formula types via an efficient fine-tuning protocol.",
          "supported_by": [
            "exp_2",
            "exp_3",
            "exp_5"
          ],
          "strength": "strong"
        },
        {
          "claim": "The inclusion of a GAT-style attention mechanism significantly improves performance over the baseline BPNN architecture.",
          "supported_by": [
            "exp_4",
            "exp_5",
            "exp_7"
          ],
          "strength": "strong"
        },
        {
          "claim": "The model is data-efficient, requiring only 1000 small random formulae for pre-training and minimal data for fine-tuning.",
          "supported_by": [
            "exp_1",
            "exp_3"
          ],
          "strength": "strong"
        },
        {
          "claim": "The model is time-efficient, providing fast approximations compared to traditional solvers.",
          "supported_by": "Figure 2 (runtime comparison in Sec 4.2)",
          "strength": "moderate (shown qualitatively via runtime plot)"
        }
      ],
      "limitations_acknowledged": [
        "BPGAT lacks the theoretical (PAC) guarantees provided by solvers like ApproxMC.",
        "BPGAT can produce a few outlier predictions with large errors, leading to higher RMSE than ApproxMC in scalability tests, despite better average MRE.",
        "Comparison with other guarantee-less approximate solvers (ApproxCount, SampleApprox) was not possible due to lack of open-source implementations."
      ],
      "key_takeaways": [
        "Graph Attention Networks, when integrated into a BP-inspired framework (BPGAT), form a powerful and practical approximate solver for the #SAT problem.",
        "A key to practical application is a two-stage strategy: pre-training on cheaply generated random data, followed by light fine-tuning on the target distribution.",
        "The attention mechanism is critical, providing measurable gains in accuracy and generalization over a non-attentive neural BP baseline (BPNN).",
        "BPGAT achieves accuracy competitive with a state-of-the-art approximate solver (ApproxMC) while being fundamentally faster and more data-adaptive."
      ]
    }
  },
  {
    "filename": "2208.10227_One_Model_Any_CSP_Graph_Neural_Networks_as_Fast_Gl.md",
    "paper_title": "One Model, Any CSP: Graph Neural Networks as Fast Global Search Heuristics for Constraint Satisfaction",
    "paper_focus": "Proposes a universal Graph Neural Network architecture (ANYCSP) that can be trained as an end-to-end global search heuristic for any Constraint Satisfaction Problem (CSP) using unsupervised policy gradient descent.",
    "experiment_inventory": {
      "total_experiments": 7,
      "main_experiments": 5,
      "ablation_studies": 1,
      "other_experiments": 1,
      "experiments": [
        {
          "id": "exp_1",
          "name": "MODEL RB Benchmark Evaluation",
          "paper_section": "Section 5 (Experiments), Appendix B.1",
          "type": "main_comparison"
        },
        {
          "id": "exp_2",
          "name": "Graph Coloring Benchmark Evaluation",
          "paper_section": "Section 5 (Experiments), Appendix B.2",
          "type": "main_comparison"
        },
        {
          "id": "exp_3",
          "name": "MAXCUT Benchmark Evaluation",
          "paper_section": "Section 5 (Experiments), Appendix B.3",
          "type": "main_comparison"
        },
        {
          "id": "exp_4",
          "name": "3-SAT Benchmark Evaluation",
          "paper_section": "Section 5 (Experiments), Appendix B.4",
          "type": "main_comparison"
        },
        {
          "id": "exp_5",
          "name": "MAX-k-SAT Benchmark Evaluation",
          "paper_section": "Section 5 (Experiments), Appendix B.5",
          "type": "main_comparison"
        },
        {
          "id": "exp_6",
          "name": "Ablation Study on Design Choices",
          "paper_section": "Appendix C",
          "type": "ablation"
        },
        {
          "id": "exp_7",
          "name": "Cross-Comparison of Trained Models",
          "paper_section": "Appendix B.6",
          "type": "other"
        }
      ]
    },
    "experiments": {
      "exp_1": {
        "name": "MODEL RB Benchmark Evaluation",
        "paper_section": "Section 5, Appendix B.1",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate ANYCSP on general CSP benchmark instances and compare against state-of-the-art CSP solvers.",
        "benchmarks": [
          {
            "name": "RB50",
            "variant": "Satisfiable MODEL RB instances from XCSP project",
            "problem_sizes": "50 variables, domain size 23, ~500 constraints (arity 2)"
          }
        ],
        "baselines": [
          {
            "name": "Picat",
            "description": "SAT-based solver, winner of XCSP Competition",
            "source": "Zhou (2022)"
          },
          {
            "name": "ACE",
            "description": "Constraint propagation-based solver",
            "source": "Lecoutre (2022)"
          },
          {
            "name": "CoSoCo",
            "description": "Constraint propagation-based solver, strong on binary MODEL RB",
            "source": "Audemard (2018)"
          }
        ],
        "conclusion": {
          "main_finding": "ANYCSP solves the most instances (42) within a 20-minute timeout, outperforming all baselines.",
          "supporting_findings": [
            "CoSoCo solves 34 instances, Picat and ACE solve fewer.",
            "ANYCSP generalizes to searches over 10,000 times longer than during training (T_train=40 vs 500K test iterations)."
          ],
          "winner": "ANYCSP"
        },
        "detailed_description": "This experiment tests ANYCSP on the RB50 dataset, a standard benchmark of 50 satisfiable MODEL RB instances with 50 variables, domain size 23, and about 500 binary constraints. These instances are known to be challenging due to dense, random constraint relations at the satisfiability threshold. ANYCSP is trained on a distribution Î©_RB of random MODEL RB instances with 30 variables and arity 2. All methods are run once per instance with a 20-minute timeout. ANYCSP solves 42 instances, CoSoCo 34, while Picat and ACE solve fewer. The trained policy generalizes remarkably, as test searches (500K iterations) are over 10,000 times longer than training searches (40 iterations). The result demonstrates that ANYCSP can outperform specialized CSP solvers on hard, structured instances despite being trained only on random data.\n"
      },
      "exp_2": {
        "name": "Graph Coloring Benchmark Evaluation",
        "paper_section": "Section 5, Appendix B.2",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate ANYCSP on structured graph coloring benchmarks and compare against problem-specific heuristics and neural baselines.",
        "benchmarks": [
          {
            "name": "Structured Graph Coloring Benchmarks",
            "variant": "COL<10 (50 instances with chromatic number <10), COLâ‰¥10 (50 instances with chromatic number â‰¥10)",
            "problem_sizes": "Up to 1K vertices, 19K edges, chromatic number up to 73"
          }
        ],
        "baselines": [
          {
            "name": "RUNCSP",
            "description": "Neural baseline for binary CSPs, requires fixed domain size",
            "source": "TÃ¶nshoff et al. (2021)"
          },
          {
            "name": "CoSoCo",
            "description": "CSP solver",
            "source": "Audemard (2018)"
          },
          {
            "name": "Picat",
            "description": "CSP solver",
            "source": "Zhou (2022)"
          },
          {
            "name": "GREEDY",
            "description": "Simple greedy coloring heuristic",
            "source": "Lewis et al. (2012)"
          },
          {
            "name": "DSATUR",
            "description": "Classic coloring heuristic",
            "source": "BrÃ©laz (1979)"
          },
          {
            "name": "HYBRIDEA",
            "description": "State-of-the-art hybrid evolutionary algorithm",
            "source": "Galinier and Hao (1999)"
          }
        ],
        "conclusion": {
          "main_finding": "ANYCSP is on par with the best baseline (HYBRIDEA), solving all COL<10 instances and 40/50 COLâ‰¥10 instances.",
          "supporting_findings": [
            "ANYCSP generalizes to domains significantly larger (up to 73 colors) than those seen during training (3-10 colors).",
            "RUNCSP performs worse, solving only 33/50 COL<10 instances and cannot handle COLâ‰¥10 due to fixed domain size.",
            "Classical heuristics (GREEDY, DSATUR) perform poorly."
          ],
          "winner": "ANYCSP and HYBRIDEA (tie)"
        },
        "detailed_description": "ANYCSP is evaluated on 100 structured graph coloring benchmarks split by chromatic number: COL<10 (50 instances) and COLâ‰¥10 (50 instances, up to 1K vertices and chromatic number 73). ANYCSP is trained on a distribution Î©_COL of random graphs (ErdÅ‘s-RÃ©nyi, BarabÃ¡si-Albert, random geometric) with 50 vertices and 3-10 colors. The model must color each graph with one color less than a greedy heuristic, producing both satisfiable and unsatisfiable instances. During testing, ANYCSP solves all COL<10 instances and 40/50 COLâ‰¥10 instances, matching the performance of the state-of-the-art HYBRIDEA. Notably, ANYCSP generalizes to chromatic numbers up to 73 despite training with at most 10 colors. The neural baseline RUNCSP, which requires a fixed domain size, performs significantly worse (33/50 COL<10) and cannot handle COLâ‰¥10. CSP solvers (CoSoCo, Picat) perform well but are outperformed by ANYCSP and HYBRIDEA on the harder COLâ‰¥10 set.\n"
      },
      "exp_3": {
        "name": "MAXCUT Benchmark Evaluation",
        "paper_section": "Section 5, Appendix B.3",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate ANYCSP on the MAXCUT problem using the Gset benchmark and compare against neural and classical baselines.",
        "benchmarks": [
          {
            "name": "Gset",
            "variant": "Unweighted MAXCUT instances",
            "problem_sizes": "Graphs with |V| = 800, 1000, 2000, â‰¥3000 (up to 10,000 vertices)"
          }
        ],
        "baselines": [
          {
            "name": "GREEDY",
            "description": "Constructive greedy algorithm",
            "source": "Mehta (2019)"
          },
          {
            "name": "SDP",
            "description": "Semidefinite programming approximation algorithm",
            "source": "Goemans and Williamson (1995), Mehta (2019)"
          },
          {
            "name": "RUNCSP",
            "description": "Neural baseline trained on soft relaxation of MAXCUT",
            "source": "TÃ¶nshoff et al. (2021)"
          },
          {
            "name": "ECO-DQN",
            "description": "Neural local search heuristic",
            "source": "Barrett et al. (2020)"
          },
          {
            "name": "ECORD",
            "description": "Neural heuristic with GNN initialization and fast local search",
            "source": "Barrett, Parsonson, and Laterre (2022)"
          }
        ],
        "conclusion": {
          "main_finding": "ANYCSP outperforms all baselines by a large margin across all graph sizes, achieving the smallest mean deviation from best-known cut sizes.",
          "supporting_findings": [
            "On graphs with |V|=800, ANYCSP mean deviation is 1.22 vs 8.67 for ECORD (next best).",
            "ANYCSP scales well to large graphs (|V|â‰¥3000) where it maintains a substantial lead.",
            "Classical SDP is not run on larger graphs due to timeout (3 hours)."
          ],
          "winner": "ANYCSP"
        },
        "detailed_description": "ANYCSP is evaluated on the Gset benchmark, a collection of unweighted MAXCUT instances with 800 to 10,000 vertices. ANYCSP is trained on a distribution Î©_MCUT of ErdÅ‘s-RÃ©nyi graphs with 100 vertices. Neural methods (ANYCSP, RUNCSP, ECO-DQN, ECORD) run with 20 parallel runs and a 180s timeout. Classical baselines (GREEDY, SDP) are also included. Performance is measured as the mean deviation from the best-known cut sizes (from Benlic and Hao 2013) across groups of graphs by vertex count. ANYCSP achieves the smallest deviation in every group: 1.22 (|V|=800), 2.44 (1000), 13.11 (2000), 51.63 (â‰¥3000). The next best method, ECORD, has deviations of 8.67, 8.78, 39.22, 187.75 respectively. ANYCSP's global search approach outperforms both neural local search (ECO-DQN, ECORD) and the soft relaxation approach (RUNCSP). The results demonstrate that ANYCSP can find near-optimal cuts on large, structured graphs despite training on small random graphs.\n"
      },
      "exp_4": {
        "name": "3-SAT Benchmark Evaluation",
        "paper_section": "Section 5, Appendix B.4",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate ANYCSP on the 3-SAT decision problem using SATLIB benchmarks and compare against neural and classical local search heuristics.",
        "benchmarks": [
          {
            "name": "SATLIB",
            "variant": "Uniform random 3-SAT instances at satisfiability threshold",
            "problem_sizes": "Instances with 50, 100, 150, 200, 250 variables (100 each)"
          }
        ],
        "baselines": [
          {
            "name": "RLSAT",
            "description": "Neural local search heuristic for SAT",
            "source": "Yolcu and PÃ³czos (2019)"
          },
          {
            "name": "PDP",
            "description": "Neural solver based on differentiable relaxation",
            "source": "Amizadeh, Matusevych, and Weimer (2019)"
          },
          {
            "name": "WALKSAT",
            "description": "Classical local search based on random walks",
            "source": "Selman et al. (1993)"
          },
          {
            "name": "PROBSAT",
            "description": "Modern probabilistic local search heuristic",
            "source": "Balint and SchÃ¶ning (2018)"
          }
        ],
        "conclusion": {
          "main_finding": "ANYCSP outperforms all neural and classical baselines on larger instances (200 and 250 variables), solving 97 and 99 out of 100 instances respectively.",
          "supporting_findings": [
            "All methods perform well on small instances (50-100 variables).",
            "Neural baselines (RLSAT, PDP) performance drops significantly as instance size increases.",
            "ANYCSP even outperforms classical heuristics (WALKSAT, PROBSAT) on the largest instances."
          ],
          "winner": "ANYCSP"
        },
        "detailed_description": "ANYCSP is evaluated on SATLIB benchmarks: 100 uniform random 3-SAT instances each for 50, 100, 150, 200, and 250 variables, at the satisfiability threshold. ANYCSP is trained on a distribution Î©_3SAT of random 3-SAT instances with 100 variables and clause/variable ratio 4-5. Baselines include neural methods (RLSAT, PDP) and classical local search heuristics (WALKSAT, PROBSAT). Following RLSAT's setup, each method runs 10 times for 10K steps per instance (except deterministic PDP which runs once). The metric is the number of solved instances (satisfying assignment found). ANYCSP solves all 100 instances for 50 and 100 variables, and 100, 97, 99 for 150, 200, 250 variables respectively. RLSAT and PDP performance drops sharply (e.g., RLSAT solves only 12/250-variable instances). ANYCSP even surpasses classical heuristics on 200 and 250 variable instances. The results show that ANYCSP scales effectively to larger instances and outperforms both neural and classical approaches.\n"
      },
      "exp_5": {
        "name": "MAX-k-SAT Benchmark Evaluation",
        "paper_section": "Section 5, Appendix B.5",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate ANYCSP on the MAX-k-SAT maximization problem on very large random instances and compare against state-of-the-art classical heuristics.",
        "benchmarks": [
          {
            "name": "Random k-CNF formulas",
            "variant": "Uniform random k-CNF for k=3,4,5",
            "problem_sizes": "50 instances each with 10,000 variables (100x larger than training), 75K (3), 150K (4), 300K (5) clauses"
          }
        ],
        "baselines": [
          {
            "name": "WALKSAT",
            "description": "Classical local search adapted for MAXSAT",
            "source": "Selman et al. (1993)"
          },
          {
            "name": "CCLS",
            "description": "State-of-the-art MAXSAT local search heuristic",
            "source": "Luo et al. (2015)"
          },
          {
            "name": "SATLike",
            "description": "State-of-the-art MAXSAT local search heuristic",
            "source": "Cai and Lei (2020)"
          }
        ],
        "conclusion": {
          "main_finding": "ANYCSP outperforms all classical baselines, finding solutions with fewer unsatisfied clauses on average, despite performing 3 orders of magnitude fewer search steps.",
          "supporting_findings": [
            "For 5-CNF, ANYCSP averages 1103.14 unsatisfied clauses vs 1152.88 for SATLike (best baseline).",
            "Classical heuristics perform over 100M search steps in 20 minutes; ANYCSP performs <100K steps.",
            "When restricted to local search (ANYCSP_loc), performance drops below SATLike."
          ],
          "winner": "ANYCSP"
        },
        "detailed_description": "ANYCSP is evaluated on MAX-k-SAT with k=3,4,5 using random 10,000-variable formulas, which are 100 times larger than the training data (100 variables). Training uses distribution Î©_MSAT with k=3,4 and denser formulas. Classical heuristics (WALKSAT, CCLS, SATLike) are state-of-the-art for MAXSAT. All methods run with a 20-minute timeout. Performance is measured by the mean number of unsatisfied clauses over 50 instances. ANYCSP achieves 1537.46 (3-CNF), 1126.44 (4-CNF), 1103.14 (5-CNF) unsatisfied clauses, outperforming all baselines (e.g., SATLike: 1595.86, 1188.56, 1152.88). Remarkably, classical heuristics perform over 100M search steps in the timeout, while ANYCSP performs <100K steps (3 orders of magnitude fewer). A key insight is that ANYCSP's global search allows parallel refinements across the instance, compensating for slower iteration speed. An ablation (ANYCSP_loc, local search variant) performs worse, confirming the importance of global search. ANYCSP also generalizes to unseen arity (k=5).\n"
      },
      "exp_6": {
        "name": "Ablation Study on Design Choices",
        "paper_section": "Appendix C",
        "experiment_type": "ablation",
        "purpose": "To evaluate the importance of two key design choices: global search action space and the proposed reward scheme.",
        "benchmarks": [
          {
            "name": "Graph Coloring",
            "problem_sizes": "COL<10, COLâ‰¥10"
          },
          {
            "name": "MAXCUT",
            "problem_sizes": "Gset graphs grouped by |V|"
          },
          {
            "name": "MAX-5-SAT",
            "problem_sizes": "50 random 5-CNF instances with 10K variables"
          }
        ],
        "baselines": [
          {
            "name": "ANYCSP (full)",
            "description": "Full proposed method with global search and proposed reward",
            "source": "proposed method"
          },
          {
            "name": "ANYCSP_loc",
            "description": "Local search variant: only one variable changed per step",
            "source": "ablation variant"
          },
          {
            "name": "ANYCSP_qual",
            "description": "Variant using simple quality-based reward (Q_I(Î±^(t)))",
            "source": "ablation variant"
          }
        ],
        "conclusion": {
          "main_finding": "Both global search and the proposed reward scheme are crucial; the full ANYCSP outperforms its ablated variants across all problems.",
          "supporting_findings": [
            "On graph coloring, ANYCSP_qual performs significantly worse, solving only 37/50 COL<10 instances.",
            "On MAXCUT and MAX-5-SAT, both ablated variants perform worse than full ANYCSP.",
            "ANYCSP_qual converges quickly but stagnates in local optima; ANYCSP_loc converges slowly due to local steps."
          ],
          "winner": "ANYCSP (full)"
        },
        "detailed_description": "This ablation study evaluates two modifications of ANYCSP: (1) ANYCSP_loc, a local search variant that changes only one variable per step (achieved by applying softmax over all values globally rather than per domain). (2) ANYCSP_qual, which uses a simple reward proportional to assignment quality (with a baseline) instead of the proposed improvement-based reward. Both variants are trained with the same data and hyperparameters as the full ANYCSP and evaluated on graph coloring, MAXCUT, and MAX-5-SAT. Results: On graph coloring, ANYCSP_loc solves 49/50 COL<10 and 37/50 COLâ‰¥10, while ANYCSP_qual solves only 37 and 25 respectively. On MAXCUT, both ablated variants have much higher deviation from best-known cuts than full ANYCSP. On MAX-5-SAT, both variants yield more unsatisfied clauses. Analysis shows that ANYCSP_qual initially converges fast but stagnates due to punishment for leaving local optima, while ANYCSP_loc converges slowly due to limited changes per step. The full ANYCSP combines the benefits of global search (fast parallel refinements) and the proposed reward (encouraging exploration), leading to superior performance.\n"
      },
      "exp_7": {
        "name": "Cross-Comparison of Trained Models",
        "paper_section": "Appendix B.6",
        "experiment_type": "other",
        "purpose": "To study the transferability of ANYCSP models trained on one CSP to other, different CSPs.",
        "benchmarks": [
          {
            "name": "RB50",
            "problem_sizes": "50 MODEL RB instances"
          },
          {
            "name": "COL<10",
            "problem_sizes": "50 graph coloring instances"
          },
          {
            "name": "Gset800",
            "problem_sizes": "Gset graphs with |V|=800"
          },
          {
            "name": "SL250",
            "problem_sizes": "100 3-SAT instances with 250 variables"
          },
          {
            "name": "MAX-5-CNF",
            "problem_sizes": "50 random 5-CNF instances with 10K variables"
          }
        ],
        "baselines": [
          {
            "name": "ANYCSP trained on Î©_RB (MODEL RB)",
            "description": "Model trained for MODEL RB",
            "source": "proposed method (different training)"
          },
          {
            "name": "ANYCSP trained on Î©_COL (Graph Coloring)",
            "description": "Model trained for graph coloring",
            "source": "proposed method (different training)"
          },
          {
            "name": "ANYCSP trained on Î©_MCUT (MAXCUT)",
            "description": "Model trained for MAXCUT",
            "source": "proposed method (different training)"
          },
          {
            "name": "ANYCSP trained on Î©_SAT (3-SAT)",
            "description": "Model trained for 3-SAT",
            "source": "proposed method (different training)"
          },
          {
            "name": "ANYCSP trained on Î©_MSAT (MAX-k-SAT)",
            "description": "Model trained for MAX-k-SAT",
            "source": "proposed method (different training)"
          }
        ],
        "conclusion": {
          "main_finding": "Models perform best on their own training distribution, but some generalize well to other CSPs (e.g., MODEL RB model solves all COL<10 instances); aggregation function choice (MAX vs MEAN) is critical for generalization.",
          "supporting_findings": [
            "The MODEL RB model (MAX aggregation) solves all COL<10 instances and performs well on 3-SAT (98/100 SL250).",
            "The MAXCUT model (SUM aggregation) performs poorly on all other problems, indicating high specialization.",
            "The MAX-k-SAT model (MEAN aggregation) underperforms on 3-SAT compared to MODEL RB and coloring models, suggesting aggregation function matters more than data similarity."
          ],
          "winner": "Each model on its own distribution"
        },
        "detailed_description": "This experiment tests the universality of ANYCSP by applying models trained on one CSP to test datasets of other CSPs. Five models (trained on MODEL RB, graph coloring, MAXCUT, 3-SAT, MAX-k-SAT) are evaluated on each other's test sets using the original evaluation metrics. Key findings: (1) Each model performs best on its own distribution. (2) The MODEL RB model (trained with MAX aggregation) generalizes surprisingly well, solving all COL<10 instances and 98/100 SL250 (3-SAT) instances. The graph coloring model (MAX aggregation) also generalizes well to 3-SAT (96/100). (3) The MAXCUT model (SUM aggregation) fails on all other problems, likely because SUM aggregation is less robust to distribution shifts. (4) The MAX-k-SAT model (MEAN aggregation) performs worse on 3-SAT (66/100) than the MODEL RB and coloring models, indicating that the choice of aggregation function (MAX for decision problems, MEAN/SUM for maximization) is crucial for cross-domain generalization. This shows that while ANYCSP is universal, trained models can exhibit varying degrees of specialization and generalization depending on the aggregation function and training distribution.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "MODEL RB Benchmark Evaluation",
        "benchmarks": "RB50 (50 satisfiable MODEL RB instances from XCSP project)",
        "baselines": "Picat, ACE, CoSoCo (state-of-the-art CSP solvers)",
        "key_conclusion": "ANYCSP solves the most instances (42) within a 20-minute timeout, outperforming all baselines."
      },
      {
        "experiment": "Graph Coloring Benchmark Evaluation",
        "benchmarks": "COL<10 (50 instances with chromatic number <10), COLâ‰¥10 (50 instances with chromatic number â‰¥10) from structured graph coloring benchmarks",
        "baselines": "RUNCSP, CoSoCo, Picat, GREEDY, DSATUR, HYBRIDEA",
        "key_conclusion": "ANYCSP is on par with the best baseline (HYBRIDEA), solving all 50 COL<10 instances and 40/50 COLâ‰¥10 instances, and generalizes to larger domains (up to 73 colors) not seen during training."
      },
      {
        "experiment": "MAXCUT Benchmark Evaluation",
        "benchmarks": "Gset graphs (unweighted instances with 800 to 10,000 vertices)",
        "baselines": "GREEDY, SDP, RUNCSP, ECO-DQN, ECORD",
        "key_conclusion": "ANYCSP outperforms all baselines by a large margin across all graph sizes, achieving the smallest mean deviation from best-known cut sizes."
      },
      {
        "experiment": "3-SAT Benchmark Evaluation",
        "benchmarks": "SATLIB uniform 3-SAT instances (SL50, SL100, SL150, SL200, SL250, 100 instances each)",
        "baselines": "RLSAT, PDP, WALKSAT, PROBSAT",
        "key_conclusion": "ANYCSP outperforms all neural and classical baselines on larger instances (SL200, SL250), solving 97 and 99 instances respectively."
      },
      {
        "experiment": "MAX-k-SAT Benchmark Evaluation",
        "benchmarks": "Random k-CNF formulas (50 instances each for k=3,4,5 with 10K variables, 100x larger than training)",
        "baselines": "WALKSAT, CCLS, SATLike",
        "key_conclusion": "ANYCSP outperforms all classical baselines, finding solutions with fewer unsatisfied clauses, despite performing 3 orders of magnitude fewer search steps."
      },
      {
        "experiment": "Ablation Study on Design Choices",
        "benchmarks": "Graph Coloring (COL<10, COLâ‰¥10), MAXCUT (Gset), MAX-5-SAT (random 5-CNF)",
        "baselines": "ANYCSP (full), ANYCSP_loc (local search variant), ANYCSP_qual (alternative reward)",
        "key_conclusion": "Both global search and the proposed reward scheme are crucial; the full ANYCSP outperforms its ablated variants across all problems."
      },
      {
        "experiment": "Cross-Comparison of Trained Models",
        "benchmarks": "RB50, COL<10, Gset800, SL250, MAX-5-CNF",
        "baselines": "ANYCSP models trained on different CSP distributions (Î©_RB, Î©_COL, Î©_MCUT, Î©_SAT, Î©_MSAT)",
        "key_conclusion": "Models perform best on their own training distribution, but some generalize well to other CSPs (e.g., MODEL RB model solves all COL<10 instances); aggregation function choice (MAX vs MEAN) is critical for generalization."
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "MODEL RB (RB50)",
          "used_in_experiments": [
            "exp_1",
            "exp_7"
          ]
        },
        {
          "benchmark": "Graph Coloring (COL<10, COLâ‰¥10)",
          "used_in_experiments": [
            "exp_2",
            "exp_6",
            "exp_7"
          ]
        },
        {
          "benchmark": "MAXCUT (Gset)",
          "used_in_experiments": [
            "exp_3",
            "exp_6",
            "exp_7"
          ]
        },
        {
          "benchmark": "3-SAT (SATLIB)",
          "used_in_experiments": [
            "exp_4",
            "exp_7"
          ]
        },
        {
          "benchmark": "MAX-k-SAT (random k-CNF)",
          "used_in_experiments": [
            "exp_5",
            "exp_6",
            "exp_7"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "RUNCSP",
          "compared_in_experiments": [
            "exp_2",
            "exp_3"
          ]
        },
        {
          "baseline": "ECO-DQN",
          "compared_in_experiments": [
            "exp_3"
          ]
        },
        {
          "baseline": "ECORD",
          "compared_in_experiments": [
            "exp_3"
          ]
        },
        {
          "baseline": "RLSAT",
          "compared_in_experiments": [
            "exp_4"
          ]
        },
        {
          "baseline": "PDP",
          "compared_in_experiments": [
            "exp_4"
          ]
        },
        {
          "baseline": "Classical CSP solvers (Picat, ACE, CoSoCo)",
          "compared_in_experiments": [
            "exp_1",
            "exp_2"
          ]
        },
        {
          "baseline": "Classical heuristics (GREEDY, DSATUR, HYBRIDEA, WALKSAT, PROBSAT, CCLS, SATLike)",
          "compared_in_experiments": [
            "exp_2",
            "exp_3",
            "exp_4",
            "exp_5"
          ]
        }
      ],
      "overall_narrative": "The experiments collectively demonstrate that ANYCSP is a powerful, generic framework for learning CSP heuristics. Across five diverse CSPs (MODEL RB, graph coloring, MAXCUT, 3-SAT, MAX-k-SAT), ANYCSP consistently outperforms prior neural approaches and competes with or surpasses state-of-the-art classical heuristics. Key strengths include: (1) Generalization to instances much larger and structurally different than training data (e.g., 100x larger variables in MAX-k-SAT, up to 73 colors in coloring). (2) The global search paradigm, which leverages GNN parallelism to make large steps, compensating for slower iteration speed compared to classical local search. (3) The improvement-based reward scheme, which encourages exploration and prevents stagnation. Ablation studies confirm the necessity of both global search and the proposed reward. The cross-comparison shows that while models specialize to their training distribution, some generalize well to other CSPs, with aggregation function choice playing a critical role. Overall, ANYCSP validates the feasibility of training a single GNN architecture via reinforcement learning to perform effective global search across a wide range of combinatorial problems.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "ANYCSP can learn effective search heuristics for any CSP in a purely data-driven manner using a universal GNN architecture.",
          "supported_by": [
            "exp_1",
            "exp_2",
            "exp_3",
            "exp_4",
            "exp_5"
          ],
          "strength": "strong"
        },
        {
          "claim": "Global search (allowing any number of variable changes per step) is crucial for leveraging GNN parallelism and scaling to large instances.",
          "supported_by": [
            "exp_3",
            "exp_5",
            "exp_6"
          ],
          "strength": "strong"
        },
        {
          "claim": "The proposed improvement-based reward scheme encourages exploration and prevents stagnation in local optima.",
          "supported_by": [
            "exp_6"
          ],
          "strength": "strong"
        },
        {
          "claim": "ANYCSP generalizes to instances significantly larger and structurally more complex than training data.",
          "supported_by": [
            "exp_2",
            "exp_3",
            "exp_5"
          ],
          "strength": "strong"
        },
        {
          "claim": "ANYCSP outperforms prior neural combinatorial optimization approaches by a substantial margin.",
          "supported_by": [
            "exp_2",
            "exp_3",
            "exp_4"
          ],
          "strength": "strong"
        },
        {
          "claim": "ANYCSP can compete with or improve upon conventional search heuristics.",
          "supported_by": [
            "exp_1",
            "exp_2",
            "exp_4",
            "exp_5"
          ],
          "strength": "strong"
        }
      ],
      "limitations_acknowledged": [
        "GPU memory is a primary bottleneck, limiting the maximum instance size processable (depends on constraint arity and domain size).",
        "Models can become highly specialized to their training distribution (e.g., MAXCUT model with SUM aggregation does not generalize).",
        "Training requires 24-48 hours on a GPU, though inference is efficient."
      ],
      "key_takeaways": [
        "A single GNN architecture can be trained via policy gradient to perform global search across diverse CSPs, achieving state-of-the-art results.",
        "Global search and a carefully designed reward are critical for overcoming the slow iteration speed of GNNs compared to classical heuristics.",
        "The method generalizes exceptionally well to larger, more complex instances, demonstrating the power of learned heuristics."
      ]
    }
  },
  {
    "filename": "2211.14405_Learning_Branching_Heuristics_from_Graph_Neural_Ne.md",
    "paper_title": "Learning Branching Heuristics from Graph Neural Networks",
    "paper_focus": "Proposes a probabilistic-method Graph Neural Network to learn branching heuristics for exact backtracking solvers, applied to the Dominating-Clique and Maximum-Clique problems.",
    "experiment_inventory": {
      "total_experiments": 3,
      "main_experiments": 3,
      "ablation_studies": 0,
      "other_experiments": 0
    },
    "experiments": {
      "exp_1": {
        "name": "Finding Maximum Cliques",
        "paper_section": "Section 4.1, Table 1",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate the performance of the proposed probabilistic-method GNN (with loss function Eq. 4) against the baseline GNN from Karalias et al. [8] on the maximum clique problem using a greedy approximation algorithm.",
        "benchmarks": [
          {
            "name": "Twitter",
            "variant": "Real-world network dataset",
            "problem_sizes": "Average ~120 nodes"
          },
          {
            "name": "COLLAB",
            "variant": "Real-world scientific collaboration dataset",
            "problem_sizes": "Not specified"
          },
          {
            "name": "IMDB",
            "variant": "Real-world movie collaboration dataset",
            "problem_sizes": "Not specified"
          },
          {
            "name": "G(n,p)",
            "variant": "Sparse random graphs",
            "problem_sizes": "n similar to Twitter (~120), p âˆˆ [0.2, 0.4]"
          },
          {
            "name": "G(n,p)",
            "variant": "Dense random graphs",
            "problem_sizes": "n similar to Twitter (~120), p âˆˆ [0.6, 0.8]"
          }
        ],
        "baselines": [
          {
            "name": "GNN model from Karalias et al. [8]",
            "description": "Probabilistic GNN using the first-moment method loss function.",
            "source": "Karalias and Loukas [8]"
          }
        ],
        "conclusion": {
          "main_finding": "The proposed GNN model (using loss function Eq. 4) achieves comparable approximation ratios to the baseline GNN model from [8] across all tested datasets.",
          "supporting_findings": [
            "On the Twitter dataset, the baseline model's 'slow' decoder yields the highest average ratio (0.956), while the proposed model's 'slow' decoder is close (0.931).",
            "On COLLAB and IMDB datasets, both models achieve near-perfect (1.000) approximation ratios.",
            "On synthetic G(n,p) graphs, performance is very similar between the two models, with no clear winner."
          ],
          "winner": "The baseline GNN model has a slight edge on Twitter with its 'slow' decoder, but overall performance is very similar."
        },
        "detailed_description": "This experiment evaluates the new probabilistic-method GNN model (proposed in Section 3.1) on the Maximum Clique problem. The goal is to see if the new loss function (Eq. 4), designed to avoid the drawbacks of the first-moment method, produces probability distributions that are useful for finding large cliques. The model outputs vertex probabilities, which are used by a greedy approximation algorithm (from [8]) to decode a clique. Two decoding strategies are tested: 'fast' (quick but less thorough) and 'slow' (more thorough). The evaluation metric is the approximation ratio: size of found clique / size of maximum clique (found by Gurobi). Experiments are run on three real-world graph datasets (Twitter, COLLAB, IMDB) and two classes of synthetic G(n,p) graphs (sparse and dense). The proposed model is compared directly against the GNN model from Karalias et al. [8], which uses a different loss function. Results in Table 1 show that the two models perform very similarly across all datasets. The baseline model shows a minor advantage on the Twitter dataset with its 'slow' decoder. The experiment validates that the proposed GNN framework is functional and competitive for a locally-defined problem (maximum clique) before being applied to the more complex, globally-constrained Dominating Clique problem.\n"
      },
      "exp_2": {
        "name": "Finding Dominating Cliques",
        "paper_section": "Section 4.2, Figure 1, Appendix Tables 2-4",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate whether the branching heuristic derived from the GNN model (with loss function Eq. 5) improves the performance of the exact Dominating-Clique backtracking solver compared to the original MRV heuristic.",
        "benchmarks": [
          {
            "name": "G(n,p) random graphs",
            "variant": "Hard instances at phase transition",
            "problem_sizes": "n ranging from 75 to 800, p â‰ˆ 0.371"
          }
        ],
        "baselines": [
          {
            "name": "Original Dominating-Clique Solver (Culberson et al. [9])",
            "description": "Backtracking solver using the Minimum-Remaining-Values (MRV) heuristic.",
            "source": "Culberson et al. [9]"
          }
        ],
        "conclusion": {
          "main_finding": "The GNN-enhanced solvers (using information entropy from learned probabilities) outperform the original MRV-based solver, particularly on instances where no dominating clique exists.",
          "supporting_findings": [
            "On 'no solution' instances, the GNN-enhanced solvers achieve higher winning ratios (Figures 1a, 1b) and have a lower geometric average number of branches (indicating more effective pruning).",
            "On 'solution exists' instances, the performance advantage is less pronounced, as the solver can return early upon finding a solution (Figures 1c, 1d).",
            "The 'accurate' version of the joint entropy calculation sometimes outperforms the 'fast' version, but not consistently across all graph sizes.",
            "Detailed results in Tables 2-4 (Appendix D) show the GNN-enhanced solvers often win more head-to-head comparisons against the original solver (O vs F, O vs A columns)."
          ],
          "winner": "The GNN-enhanced solvers (both fast and accurate entropy versions) generally outperform the original MRV-based solver, especially on unsatisfiable instances."
        },
        "detailed_description": "This is the core experiment testing the paper's main contribution: using a GNN-learned probability distribution to create a branching heuristic for an exact solver. The experiment modifies the dominant-clique solver from Culberson et al. [9]. The original solver uses the MRV heuristic (line 6 of Algorithm 1: pick the clause C with minimum |Câˆ©S|). The proposed modification replaces the cardinality |Câˆ©S| with the joint information entropy of the variables in Câˆ©S, calculated from probabilities provided by a GNN trained with loss function Eq. (5). Two methods for calculating joint entropy are tested: a 'fast' approximation and a more 'accurate' one (Eq. 8). The evaluation is performed on hard SAT-encoded instances of the dominating clique problem, generated from G(n,p) random graphs with p around the phase transition (0.371). The primary performance metric is the number of branches explored in the backtracking search tree, with lower numbers indicating better pruning. Results are split between instances with and without a dominating clique. The GNN-enhanced solvers consistently explore fewer branches on average than the original solver, with the advantage being more significant on instances with no solution (where the entire search tree must be explored). This demonstrates that the learned heuristic more effectively guides the search away from dead ends. The raw data in Appendix Tables 2-4 provides win/loss counts and average branch counts for various graph sizes (n) on training, validation, and test sets, confirming the robustness of the improvement.\n"
      },
      "exp_3": {
        "name": "Finding Minimum Dominating Cliques",
        "paper_section": "Section 4.3, Figure 2, Appendix Tables 5-7",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate GNN-enhanced solvers for finding the *minimum* dominating clique, comparing different loss functions (Eq. 5, Eq. 6 with simple expectation, Eq. 6 with refined expectation Eq. 7) and entropy calculation methods.",
        "benchmarks": [
          {
            "name": "G(n,p) random graphs",
            "variant": "Instances with few minimum-sized solutions",
            "problem_sizes": "n ranging from 100 to 800, p âˆˆ (0.4, 0.41)"
          }
        ],
        "baselines": [
          {
            "name": "Extended Original Solver (O)",
            "description": "The original Culberson et al. solver [9] augmented with backjumping and solution recording to find the minimum dominating clique.",
            "source": "Extended from Culberson et al. [9]"
          }
        ],
        "conclusion": {
          "main_finding": "GNN models trained with loss functions that incorporate a term for expected solution size (Eq. 6) lead to better-performing solvers than the model using the basic loss function (Eq. 5), and both outperform the extended original solver.",
          "supporting_findings": [
            "Solvers using probabilities from models with loss functions (6) and (7) have higher winning ratios against the original solver than the solver using model (5) (Fig 2a, 2b).",
            "The refined expected size calculation (Eq. 7) sometimes offers an advantage over the simple sum of probabilities (Fig 2c: 'Accurate E(|S|)' vs 'Simple E(|S|)' lines).",
            "Similarly, the 'accurate' joint entropy calculation sometimes offers an advantage over the 'fast' calculation (Fig 2c, comparing points within each loss function group).",
            "The combination of accurate entropy calculation and refined expected size calculation tends to perform the best."
          ],
          "winner": "The GNN-enhanced solver using probabilities from the model trained with loss function Eq. (6) and the refined E(|S|) calculation (Eq. 7), combined with the accurate joint entropy calculation, generally performs best."
        },
        "detailed_description": "This experiment extends the dominating clique finding to the optimization variant: finding the *minimum* dominating clique. The original solver is extended with features to record the smallest solution found and use backjumping. Three GNN models are trained with different loss functions: the basic one from Exp. 2 (Eq. 5), and two variants of Eq. 6 which add a term to minimize the expected size of the solution set S. One variant uses the simple expectation E(|S|)=Î£p_i, the other uses a more sophisticated, permutation-based expectation designed to more closely approximate the expected size of a dominating clique (Eq. 7). Each set of learned probabilities is then plugged into the solver using both the 'fast' and 'accurate' joint entropy calculations, resulting in six GNN-enhanced solvers. These are compared against the extended original solver on a new set of G(n,p) instances with p between 0.4 and 0.41, chosen because they are likely to have a dominating clique but only a few of minimum size, making the search non-trivial. Performance is measured by the number of branches in the search tree. Figures 2a and 2b show that solvers using probabilities from models with the expected-size term (Eq. 6) win more often against the original solver than those using the basic model (Eq. 5). Figure 2c shows the geometric average branch ratio, confirming that improvements come from both more accurate entropy calculation and the refined expected size term. The results demonstrate that tailoring the GNN's loss function to the specific optimization objective (minimum size) further enhances the utility of the learned branching heuristic.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "Finding Maximum Cliques",
        "benchmarks": "Twitter, COLLAB, IMDB, G(n,p) Sparse, G(n,p) Dense",
        "baselines": "GNN with loss function from Karalias et al. [8]",
        "key_conclusion": "The proposed GNN model performs comparably to the baseline GNN on real-world and synthetic datasets."
      },
      {
        "experiment": "Finding Dominating Cliques",
        "benchmarks": "G(n,p) graphs with p ~ 0.371 (hard instances, phase transition)",
        "baselines": "Original Dominating-Clique solver (Culberson et al. [9])",
        "key_conclusion": "GNN-enhanced solvers outperform the original MRV heuristic, especially on instances with no solution, by reducing search tree branches."
      },
      {
        "experiment": "Finding Minimum Dominating Cliques",
        "benchmarks": "G(n,p) graphs with p âˆˆ (0.4, 0.41)",
        "baselines": "Original Dominating-Clique solver (Culberson et al. [9]) extended with backjumping",
        "key_conclusion": "GNN models with loss functions incorporating expected solution size (Eq. 6 & 7) outperform the basic model (Eq. 5) and the original solver."
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "Real-world graphs (Twitter, COLLAB, IMDB)",
          "used_in_experiments": [
            "exp_1"
          ]
        },
        {
          "benchmark": "Synthetic G(n,p) graphs",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "GNN from Karalias et al. [8]",
          "compared_in_experiments": [
            "exp_1"
          ]
        },
        {
          "baseline": "Original Dominating-Clique Solver (Culberson et al. [9])",
          "compared_in_experiments": [
            "exp_2",
            "exp_3"
          ]
        }
      ],
      "overall_narrative": "The experiments collectively demonstrate a progressive validation of the proposed GNN framework. First, Experiment 1 shows the GNN with the new probabilistic-method loss function works on a standard problem (Max Clique), establishing baseline competence. Experiment 2 applies the framework to a harder problem (Dominating Clique) with global constraints, showing that the learned branching heuristic significantly improves an exact solver over a classical heuristic (MRV), especially on unsatisfiable instances. Experiment 3 refines the approach for an optimization variant (Minimum Dominating Clique), showing that incorporating the objective (size minimization) directly into the GNN's loss function yields further gains. The narrative is that GNNs can learn effective, problem-aware heuristics that enhance the core backtracking mechanism of exact combinatorial solvers.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "A probabilistic-method GNN can learn probability distributions useful for constructing branching heuristics.",
          "supported_by": [
            "exp_2",
            "exp_3"
          ],
          "strength": "strong"
        },
        {
          "claim": "The learned branching heuristic outperforms the human-designed MRV heuristic for the Dominating-Clique problem.",
          "supported_by": [
            "exp_2",
            "exp_3"
          ],
          "strength": "strong"
        },
        {
          "claim": "Incorporating the optimization objective (e.g., minimizing size) into the GNN loss function improves solver performance for that objective.",
          "supported_by": [
            "exp_3"
          ],
          "strength": "strong"
        }
      ],
      "limitations_acknowledged": [
        "Performance improvement is more pronounced on instances where no solution exists (requires full tree traversal) versus where a solution exists (allows early stopping).",
        "The method is tested primarily on synthetic G(n,p) instances; generalizability to other instance distributions is not extensively explored."
      ],
      "key_takeaways": [
        "GNNs can be effectively used to learn branching heuristics for exact backtracking algorithms, moving beyond just approximation algorithms or reinforcement learning.",
        "The probabilistic method provides a principled way to design loss functions for GNNs targeting combinatorial problems with complex constraints.",
        "For the Dominating-Clique problem, a heuristic based on information entropy from learned probabilities is more effective than the standard MRV heuristic."
      ]
    }
  },
  {
    "filename": "2304.08738_Addressing_Variable_Dependency_in_GNN-based_SAT_So.md",
    "paper_title": "Addressing Variable Dependency in GNN-based SAT Solving",
    "paper_focus": "Proposes AsymSAT, a GNN architecture with RNN for sequential variable assignment prediction to address variable dependency in SAT solving.",
    "experiment_inventory": {
      "total_experiments": 6,
      "main_experiments": 4,
      "ablation_studies": 2,
      "other_experiments": 0
    },
    "experiments": {
      "exp_1": {
        "name": "Effectiveness of RNN decoding layer (symmetric circuits)",
        "paper_section": "Section 5.2, Experiments for AsymSAT Configurations",
        "experiment_type": "ablation",
        "purpose": "To test whether the RNN decoding layer enables solving symmetric circuits with asymmetric solutions.",
        "benchmarks": [
          {
            "name": "Small-scale symmetric AIG circuits",
            "variant": "10 manually constructed circuits",
            "problem_sizes": "â‰¤3 inputs, symmetric nodes requiring distinct assignments"
          }
        ],
        "baselines": [
          {
            "name": "AsymSAT with LSTM",
            "description": "Proposed method with bidirectional LSTM in R layer",
            "source": "proposed method"
          },
          {
            "name": "AsymSAT with GRU",
            "description": "Proposed method with bidirectional GRU in R layer",
            "source": "proposed method"
          },
          {
            "name": "AsymSAT without R layer",
            "description": "Proposed method without the RNN decoding layer",
            "source": "proposed method (ablated)"
          },
          {
            "name": "NeuroSAT",
            "description": "GNN-based SAT solver predicting concurrently",
            "source": "Selsam et al., 2018"
          },
          {
            "name": "DG-DAGRNN",
            "description": "GNN-based Circuit-SAT solver with sequential message passing but concurrent prediction",
            "source": "Amizadeh et al., 2018"
          }
        ],
        "conclusion": {
          "main_finding": "Only AsymSAT variants with the RNN decoding layer (using either LSTM or GRU) can solve symmetric circuits.",
          "supporting_findings": [
            "AsymSAT with LSTM and GRU both achieve 100% solution rate on the symmetric circuit dataset.",
            "AsymSAT without the R layer, NeuroSAT, and DG-DAGRNN all achieve 0% solution rate.",
            "This confirms that concurrent prediction methods cannot handle variable dependency in symmetric problems."
          ],
          "winner": "AsymSAT with LSTM or GRU"
        },
        "detailed_description": "This experiment tests the core hypothesis of the paper: that variable dependency in symmetric SAT problems requires sequential prediction. The authors constructed 10 small circuits where at least two input nodes are symmetric but require distinct assignments in the satisfying solution (e.g., XOR-type problems). All models were trained on this dataset. The results show a stark contrast: both AsymSAT variants with the RNN decoding layer (using either LSTM or GRU) achieve perfect 100% solution rate, while all concurrent prediction methods (AsymSAT without R layer, NeuroSAT, DG-DAGRNN) completely fail with 0% solution rate. This directly validates that the RNN layer is necessary and sufficient for breaking symmetry and handling variable dependency, addressing the fundamental limitation identified in prior work.\n"
      },
      "exp_2": {
        "name": "Effect of iteration count in message-passing",
        "paper_section": "Section 5.2, Experiments for AsymSAT Configurations",
        "experiment_type": "ablation",
        "purpose": "To determine the optimal number of message-passing iterations in the GNN embedding layers.",
        "benchmarks": [
          {
            "name": "Mixed SR problems",
            "variant": "Uniformly sampled from SR(3) to SR(10)",
            "problem_sizes": "3 to 10 variables"
          }
        ],
        "baselines": [
          {
            "name": "AsymSAT with GRU (5 iterations)",
            "description": "Proposed method with GRU in R layer, 5 message-passing iterations",
            "source": "proposed method"
          },
          {
            "name": "AsymSAT with GRU (10 iterations)",
            "description": "Proposed method with GRU in R layer, 10 message-passing iterations",
            "source": "proposed method"
          },
          {
            "name": "AsymSAT with GRU (15 iterations)",
            "description": "Proposed method with GRU in R layer, 15 message-passing iterations",
            "source": "proposed method"
          },
          {
            "name": "AsymSAT with GRU (20 iterations)",
            "description": "Proposed method with GRU in R layer, 20 message-passing iterations",
            "source": "proposed method"
          },
          {
            "name": "AsymSAT with LSTM (5 iterations)",
            "description": "Proposed method with LSTM in R layer, 5 message-passing iterations",
            "source": "proposed method"
          },
          {
            "name": "AsymSAT with LSTM (10 iterations)",
            "description": "Proposed method with LSTM in R layer, 10 message-passing iterations",
            "source": "proposed method"
          },
          {
            "name": "AsymSAT with LSTM (15 iterations)",
            "description": "Proposed method with LSTM in R layer, 15 message-passing iterations",
            "source": "proposed method"
          },
          {
            "name": "AsymSAT with LSTM (20 iterations)",
            "description": "Proposed method with LSTM in R layer, 20 message-passing iterations",
            "source": "proposed method"
          }
        ],
        "conclusion": {
          "main_finding": "Increasing the number of iterations improves solution rate up to a point, with LSTM showing slightly better peak performance than GRU.",
          "supporting_findings": [
            "For AsymSAT with GRU, accuracy improves greatly from 80.63% (5 iterations) to 90.32% (10 iterations), then plateaus.",
            "For AsymSAT with LSTM, accuracy peaks at 93.07% (15 iterations) before slightly declining.",
            "Both variants show diminishing returns beyond 10-15 iterations."
          ],
          "winner": "AsymSAT with LSTM (15 iterations)"
        },
        "detailed_description": "This ablation study investigates the impact of the number of message-passing iterations in the graph embedding layers. The authors test both AsymSAT with GRU and AsymSAT with LSTM variants across iteration counts of 5, 10, 15, and 20 on a mixed dataset of SR problems (3 to 10 variables). Results show that increasing iterations from 5 to 10 provides substantial accuracy gains for both variants (~10 percentage points). Beyond 10 iterations, improvements are minimal, with AsymSAT-LSTM peaking at 15 iterations (93.07%) and AsymSAT-GRU plateauing after 10 iterations (~90%). The slight drop for LSTM at 20 iterations suggests potential overfitting or optimization issues. This experiment helps establish optimal hyperparameters for the main comparisons and shows that LSTM may have slightly higher capacity than GRU in this architecture.\n"
      },
      "exp_3": {
        "name": "Comparison on SR(n) problems (medium-size CNF)",
        "paper_section": "Section 5.2, Experiments for SAT solving",
        "experiment_type": "main_comparison",
        "purpose": "To compare AsymSAT against baselines on medium-size randomly generated CNF formulas.",
        "benchmarks": [
          {
            "name": "SR problems",
            "variant": "Random 3-SAT instances",
            "problem_sizes": "SR(3), SR(4), SR(5), SR(6), SR(7), SR(8), SR(9), SR(10), SR(20), SR(40), SR(60), SR(80)"
          }
        ],
        "baselines": [
          {
            "name": "AsymSAT (LSTM)",
            "description": "Proposed method with bidirectional LSTM in R layer (15 iterations)",
            "source": "proposed method"
          },
          {
            "name": "NeuroSAT",
            "description": "GNN-based SAT solver that predicts satisfiability concurrently",
            "source": "Selsam et al., 2018"
          },
          {
            "name": "DG-DAGRNN",
            "description": "GNN-based Circuit-SAT solver with sequential message passing",
            "source": "Amizadeh et al., 2018"
          }
        ],
        "conclusion": {
          "main_finding": "AsymSAT significantly outperforms both NeuroSAT and DG-DAGRNN across all SR problem sizes.",
          "supporting_findings": [
            "On SR(3) to SR(10), AsymSAT achieves >90% average solution rate vs ~60% for NeuroSAT.",
            "DG-DAGRNN performs poorly (mostly <10%), possibly due to vanishing gradient when trained on CNF-converted circuits.",
            "On larger problems (SR(20) to SR(80)), AsymSAT maintains consistent superiority over NeuroSAT (e.g., 27.2% vs 19.6% on SR(40)).",
            "All methods' accuracy declines with problem size, but AsymSAT decays slower."
          ],
          "winner": "AsymSAT (LSTM)"
        },
        "detailed_description": "This is the primary comparison experiment on medium-scale CNF problems. The authors train all models on 8K SR problems (3-8 variables) and test on 1.5K problems ranging from 3 to 10 variables, with additional tests on larger instances up to 80 variables. AsymSAT demonstrates clear superiority: on the main test set (SR(3)-SR(10)), it achieves an average solution rate over 90%, compared to ~60% for NeuroSAT and <10% for DG-DAGRNN. The performance gap persists on larger problems (SR(20)-SR(80)), where AsymSAT consistently outperforms NeuroSAT by several percentage points (e.g., 27.2% vs 19.6% on SR(40)). The authors note that DG-DAGRNN's poor performance may stem from vanishing gradient issues when trained on circuits converted from CNF. This experiment validates AsymSAT's effectiveness on standard SAT benchmarks and shows it scales better than prior methods.\n"
      },
      "exp_4": {
        "name": "Comparison on V(n) problems (large random AIGs)",
        "paper_section": "Section 5.2, Experiments for SAT solving",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate methods on large, nontrivial random circuits that challenge existing GNN-based solvers.",
        "benchmarks": [
          {
            "name": "V problems",
            "variant": "Random combinational AIG circuits generated by AIGEN",
            "problem_sizes": "V(3), V(4), V(5), V(6), V(7), V(8), V(9), V(10)"
          }
        ],
        "baselines": [
          {
            "name": "AsymSAT (LSTM)",
            "description": "Proposed method with bidirectional LSTM in R layer",
            "source": "proposed method"
          },
          {
            "name": "NeuroSAT",
            "description": "GNN-based SAT solver",
            "source": "Selsam et al., 2018"
          },
          {
            "name": "DG-DAGRNN",
            "description": "GNN-based Circuit-SAT solver",
            "source": "Amizadeh et al., 2018"
          }
        ],
        "conclusion": {
          "main_finding": "AsymSAT maintains reasonable solution rates (~50%) on large circuits where NeuroSAT completely fails and outperforms DG-DAGRNN on most sizes.",
          "supporting_findings": [
            "NeuroSAT achieves near-zero solution rates (0-0.025%) on all V(n) problems, likely due to the large CNF size after Tseitin transformation.",
            "DG-DAGRNN performs better than NeuroSAT (30-47.5%) but is inconsistent across problem sizes.",
            "AsymSAT achieves 45-81.58% solution rates, showing robustness to problem scale and structure."
          ],
          "winner": "AsymSAT (LSTM)"
        },
        "detailed_description": "This experiment tests the models on large random circuits (V problems) that represent more realistic, challenging instances. Each V(10) problem contains ~1K logic gates, leading to CNF formulas with >1K variables after conversionâ€”much larger than typical SR problems. The results reveal stark differences: NeuroSAT essentially fails (0-0.025% solution rate), likely overwhelmed by the problem scale. DG-DAGRNN performs moderately (30-47.5%) but inconsistently. In contrast, AsymSAT maintains solution rates between 45% and 81.58% across problem sizes, demonstrating its ability to handle large, complex circuits. This experiment highlights AsymSAT's practical advantage on problems that better reflect real-world SAT/Circuit-SAT instances.\n"
      },
      "exp_5": {
        "name": "Generalization on larger V(n) problems",
        "paper_section": "Section 5.2, Experiments for SAT solving (Table 4)",
        "experiment_type": "main_comparison",
        "purpose": "To test AsymSAT's generalization to problems much larger than those seen in training.",
        "benchmarks": [
          {
            "name": "V problems",
            "variant": "Random combinational AIG circuits",
            "problem_sizes": "V(11), V(12), V(13), V(14), V(15)"
          }
        ],
        "baselines": [
          {
            "name": "AsymSAT trained on SR(3..10) only",
            "description": "Proposed method trained only on medium CNF problems",
            "source": "proposed method"
          },
          {
            "name": "AsymSAT trained on SR(3..10) + V(3..8)",
            "description": "Proposed method trained on mixed CNF and circuit problems",
            "source": "proposed method"
          }
        ],
        "conclusion": {
          "main_finding": "AsymSAT maintains ~50% solution rate on problems up to 128x larger than training instances, regardless of training data composition.",
          "supporting_findings": [
            "Both training regimes yield similar performance (45-60% solution rates) on V(11)-V(15).",
            "The model generalizes well even when trained only on SR problems (CNF domain).",
            "Performance does not drop dramatically despite the large size gap between training and test problems."
          ],
          "winner": "Both AsymSAT variants perform similarly"
        },
        "detailed_description": "This experiment evaluates AsymSAT's ability to generalize to much larger problems than those in its training set. The authors test two training configurations: one using only SR (CNF) problems (3-10 variables) and another adding some V (circuit) problems (3-8 variables). Both models are then evaluated on V(11)-V(15) problems, where V(15) is approximately 128 times larger than the largest V problem in the mixed training set. Remarkably, both models maintain solution rates between 45% and 60%, with no clear advantage for either training regime. This demonstrates that AsymSAT learns robust representations that transfer across problem sizes and domains (CNF vs circuits), a key requirement for practical SAT solving.\n"
      },
      "exp_6": {
        "name": "Scalability on larger SR(n) problems",
        "paper_section": "Section 5.2, Experiments for SAT solving (Table 6)",
        "experiment_type": "main_comparison",
        "purpose": "To compare AsymSAT and NeuroSAT on very large CNF problems beyond the typical training range.",
        "benchmarks": [
          {
            "name": "SR problems",
            "variant": "Random 3-SAT instances",
            "problem_sizes": "SR(20), SR(40), SR(60), SR(80)"
          }
        ],
        "baselines": [
          {
            "name": "AsymSAT (LSTM)",
            "description": "Proposed method trained on SR(3)-SR(10)",
            "source": "proposed method"
          },
          {
            "name": "NeuroSAT",
            "description": "Baseline GNN-based SAT solver",
            "source": "Selsam et al., 2018"
          }
        ],
        "conclusion": {
          "main_finding": "AsymSAT consistently outperforms NeuroSAT on all larger problem sizes, though both methods' accuracy declines with increasing size.",
          "supporting_findings": [
            "On SR(20), AsymSAT achieves 55.4% vs NeuroSAT's 33.9%.",
            "On SR(40), AsymSAT achieves 27.2% vs NeuroSAT's 19.6%.",
            "On SR(80), AsymSAT still leads (5.0% vs 4.5%).",
            "The relative advantage of AsymSAT persists even as absolute performance drops."
          ],
          "winner": "AsymSAT (LSTM)"
        },
        "detailed_description": "This experiment tests scalability by evaluating models on SR problems with 20 to 80 variablesâ€”significantly larger than the training set (3-10 variables). Both AsymSAT and NeuroSAT show declining solution rates with problem size, reflecting the inherent difficulty of larger SAT instances. However, AsymSAT maintains a consistent performance advantage across all sizes: it beats NeuroSAT by 21.5 percentage points on SR(20), 7.6 points on SR(40), and 0.5 points on SR(80). While the absolute gap narrows on the largest problems, AsymSAT's relative superiority remains clear. This experiment confirms that AsymSAT's architectural improvements provide better scaling to large problems than concurrent prediction methods.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "Effectiveness of RNN decoding layer (symmetric circuits)",
        "benchmarks": "Small-scale symmetric AIG circuits",
        "baselines": "AsymSAT (LSTM/GRU), AsymSAT w/o R layer, NeuroSAT, DG-DAGRNN",
        "key_conclusion": "Only AsymSAT with RNN layer solves symmetric circuits (100% vs 0%)."
      },
      {
        "experiment": "Effect of iteration count in message-passing",
        "benchmarks": "Mixed SR(3) to SR(10) problems",
        "baselines": "AsymSAT with GRU, AsymSAT with LSTM (varying iterations)",
        "key_conclusion": "Increasing iterations improves accuracy, peaks at 10-15 iterations; LSTM slightly outperforms GRU."
      },
      {
        "experiment": "Comparison on SR(n) problems (medium-size CNF)",
        "benchmarks": "SR(3) to SR(10) problems, SR(20/40/60/80) problems",
        "baselines": "AsymSAT (LSTM), NeuroSAT, DG-DAGRNN",
        "key_conclusion": "AsymSAT significantly outperforms baselines across all problem sizes (avg >90% vs 60%)."
      },
      {
        "experiment": "Comparison on V(n) problems (large random AIGs)",
        "benchmarks": "V(3) to V(10) problems, V(11) to V(15) problems",
        "baselines": "AsymSAT (LSTM), NeuroSAT, DG-DAGRNN",
        "key_conclusion": "AsymSAT maintains ~50% solution rate on large circuits where NeuroSAT fails completely."
      },
      {
        "experiment": "Generalization on larger V(n) problems",
        "benchmarks": "V(11) to V(15) problems",
        "baselines": "AsymSAT trained on SR-only vs SR+V mixed",
        "key_conclusion": "AsymSAT maintains performance (~50%) on much larger problems regardless of training data composition."
      },
      {
        "experiment": "Scalability on larger SR(n) problems",
        "benchmarks": "SR(20), SR(40), SR(60), SR(80) problems",
        "baselines": "AsymSAT (LSTM), NeuroSAT",
        "key_conclusion": "AsymSAT consistently outperforms NeuroSAT on all larger problem sizes, though accuracy declines with size."
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "Small-scale symmetric AIG circuits",
          "used_in_experiments": [
            "exp_1"
          ]
        },
        {
          "benchmark": "Mixed SR problems (3-10 variables)",
          "used_in_experiments": [
            "exp_2",
            "exp_3"
          ]
        },
        {
          "benchmark": "SR problems (20-80 variables)",
          "used_in_experiments": [
            "exp_3",
            "exp_6"
          ]
        },
        {
          "benchmark": "V problems (3-10 variables)",
          "used_in_experiments": [
            "exp_4"
          ]
        },
        {
          "benchmark": "V problems (11-15 variables)",
          "used_in_experiments": [
            "exp_5"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "AsymSAT (LSTM/GRU)",
          "compared_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3",
            "exp_4",
            "exp_5",
            "exp_6"
          ]
        },
        {
          "baseline": "NeuroSAT",
          "compared_in_experiments": [
            "exp_1",
            "exp_3",
            "exp_4",
            "exp_6"
          ]
        },
        {
          "baseline": "DG-DAGRNN",
          "compared_in_experiments": [
            "exp_1",
            "exp_3",
            "exp_4"
          ]
        }
      ],
      "overall_narrative": "The experiments collectively tell a coherent story: (1) The RNN decoding layer in AsymSAT is essential for solving symmetric SAT problems with variable dependency (exp_1, ablation_1). (2) AsymSAT consistently outperforms concurrent prediction baselines (NeuroSAT, DG-DAGRNN) across both medium CNF problems (exp_3) and large random circuits (exp_4). (3) AsymSAT scales better to larger problem sizes (exp_6) and generalizes well to problems much larger than those in training (exp_5). (4) The architecture is robust to hyperparameter choices like iteration count (exp_2, ablation_2). Together, these results validate the paper's core claim that addressing variable dependency via sequential prediction significantly improves GNN-based SAT solving.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "Variable dependency must be addressed in GNN-based SAT solving",
          "supported_by": [
            "exp_1",
            "ablation_1"
          ],
          "strength": "strong"
        },
        {
          "claim": "Sequential prediction via RNN enables solving symmetric problems",
          "supported_by": [
            "exp_1",
            "ablation_1"
          ],
          "strength": "strong"
        },
        {
          "claim": "AsymSAT achieves higher solution rates than prior GNN-based methods",
          "supported_by": [
            "exp_3",
            "exp_4",
            "exp_6"
          ],
          "strength": "strong"
        },
        {
          "claim": "AsymSAT generalizes well to large, unseen problems",
          "supported_by": [
            "exp_5"
          ],
          "strength": "moderate"
        }
      ],
      "limitations_acknowledged": [
        "All methods' accuracy declines with problem size (observed in exp_3, exp_6).",
        "Training data requirements are not directly compared (mentioned that NeuroSAT needs orders of magnitude more data).",
        "Comparison with preferential labeling method is relegated to appendix (not in provided content)."
      ],
      "key_takeaways": [
        "Concurrent prediction fails on symmetric SAT problems due to variable dependency.",
        "Adding an RNN layer for sequential prediction solves this limitation and improves overall performance.",
        "AsymSAT outperforms NeuroSAT and DG-DAGRNN across diverse benchmarks and scales.",
        "The improvement is consistent across problem sizes, from small symmetric cases to large random circuits."
      ]
    }
  },
  {
    "filename": "2305.16373_DeepGate2_Functionality-Aware_Circuit_Representati.md",
    "paper_title": "DeepGate2: Functionality-Aware Circuit Representation Learning",
    "paper_focus": "Proposes DeepGate2, a functionality-aware circuit representation learning framework using pairwise truth table differences as supervision and a one-round GNN for improved effectiveness and efficiency.",
    "experiment_inventory": {
      "total_experiments": 6,
      "main_experiments": 4,
      "ablation_studies": 2,
      "other_experiments": 0
    },
    "experiments": {
      "exp_1": {
        "name": "Comparison with DeepGate on Probability Prediction",
        "paper_section": "IV-B",
        "experiment_type": "main_comparison",
        "purpose": "To compare the accuracy and efficiency of DeepGate2 against the original DeepGate on logic probability prediction.",
        "benchmarks": [
          {
            "name": "Industrial circuits",
            "variant": "10 circuits (D1-D10)",
            "problem_sizes": "3,183 to 40,496 gates"
          }
        ],
        "baselines": [
          {
            "name": "DeepGate",
            "description": "Original DeepGate model using logic-1 probability as supervision with multi-round GNN",
            "source": "[7]"
          }
        ],
        "conclusion": {
          "main_finding": "DeepGate2 achieves lower prediction error and is significantly faster than DeepGate.",
          "supporting_findings": [
            "Average prediction error (PE) reduced by 13.08%.",
            "Runtime reduced by 16.43x on average (order of magnitude speedup)."
          ],
          "winner": "DeepGate2"
        },
        "detailed_description": "This experiment compares DeepGate2 with the original DeepGate on the task of predicting the logic probability (probability of logic-1) for each gate in a circuit. The evaluation uses 10 industrial circuits (D1-D10) with sizes ranging from 3,183 to 40,496 gates. The performance is measured by the average prediction error (PE) and the inference runtime. Results show that DeepGate2 reduces the PE by 13.08% on average compared to DeepGate. Additionally, DeepGate2 is much faster, achieving an average speedup of 16.43x. This speedup is attributed to DeepGate2's one-round GNN design versus DeepGate's 20-round (10 forward + 10 backward) propagation. The experiment demonstrates that DeepGate2 is both more accurate and efficient for logic probability prediction.\n"
      },
      "exp_2": {
        "name": "Comparison with other Models on Logic Equivalence Gates Identification",
        "paper_section": "IV-C",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate DeepGate2's ability to identify logic equivalence gates compared to DeepGate and FGNN.",
        "benchmarks": [
          {
            "name": "Industrial circuits",
            "variant": "10 circuits (D1-D10)",
            "problem_sizes": "3,183 to 40,496 gates"
          }
        ],
        "baselines": [
          {
            "name": "DeepGate",
            "description": "Original DeepGate model using logic-1 probability as supervision",
            "source": "[7]"
          },
          {
            "name": "FGNN",
            "description": "Functionality-aware GNN using contrastive learning for equivalence identification",
            "source": "[8]"
          }
        ],
        "conclusion": {
          "main_finding": "DeepGate2 significantly outperforms both DeepGate and FGNN in identifying logic equivalence gates.",
          "supporting_findings": [
            "DeepGate2 achieves average F1-Score of 0.9434, vs. 0.6778 for DeepGate and 0.4402 for FGNN.",
            "DeepGate2 has high recall (98.73%) and precision (90.49%).",
            "DeepGate has high recall (91.46%) but low precision (54.00%), leading to many false positives.",
            "FGNN performs poorly due to reliance on structural similarity rather than functionality."
          ],
          "winner": "DeepGate2"
        },
        "detailed_description": "This experiment evaluates the ability of DeepGate2 to identify pairs of logic gates that are functionally equivalent (i.e., have identical truth tables) within a circuit. The comparison is made against two baselines: DeepGate and FGNN. The test uses the same 10 industrial circuits (D1-D10). Performance is measured by Recall, Precision, F1-Score, and AUC. DeepGate2 achieves an average F1-Score of 0.9434, substantially higher than DeepGate (0.6778) and FGNN (0.4402). DeepGate2 maintains high recall (98.73%) and precision (90.49%). DeepGate, while having high recall (91.46%), suffers from low precision (54.00%), meaning it incorrectly identifies many non-equivalent pairs as equivalent. The authors note that 80.83% of DeepGate's false positives are pairs with similar logic probability. FGNN's poor performance is attributed to its self-supervised training, which focuses on structural perturbations and fails to generalize to functionally equivalent but structurally different gates. This experiment validates DeepGate2's superior functionality-aware representation learning.\n"
      },
      "exp_3": {
        "name": "Effectiveness of PI Encoding Strategy",
        "paper_section": "IV-D",
        "experiment_type": "ablation",
        "purpose": "To assess the importance of the Primary Input (PI) encoding strategy in DeepGate2.",
        "benchmarks": [
          {
            "name": "Industrial circuits",
            "variant": "10 circuits (D1-D10)",
            "problem_sizes": "3,183 to 40,496 gates"
          }
        ],
        "baselines": [
          {
            "name": "DeepGate2 without PI encoding (w/o PIE)",
            "description": "DeepGate2 variant where PIs are not assigned unique orthogonal embeddings",
            "source": "proposed method ablation"
          }
        ],
        "conclusion": {
          "main_finding": "The PI encoding strategy is crucial for identifying reconvergence structures and learning functionality.",
          "supporting_findings": [
            "Removing PI encoding reduces F1-Score for equivalence identification from 0.9434 to 0.7541 (20.07% reduction).",
            "Accuracy for reconvergence structure identification drops from 93.22% to 74.56% without PI encoding."
          ],
          "winner": "DeepGate2 with PI encoding"
        },
        "detailed_description": "This ablation study evaluates the contribution of the Primary Input (PI) encoding strategy in DeepGate2. The PI encoding assigns unique orthogonal vectors as initial structural embeddings to each PI, providing unique identifiers. An ablated version (w/o PIE) is trained without this strategy, using uniform initial embeddings for PIs instead. The model is evaluated on the logic equivalence gates identification task across the 10 industrial circuits. Results show that removing PI encoding causes a significant drop in performance: the average F1-Score decreases from 0.9434 to 0.7541, a 20.07% reduction. Further analysis reveals that the accuracy for identifying reconvergence structures (gates with common predecessors) drops from 93.22% to 74.56%. Since functionality depends on both fan-in gate functionality and reconvergence, the inability to model reconvergence accurately hinders functionality learning. This ablation confirms that PI encoding is essential for capturing structural correlations and, consequently, functionality.\n"
      },
      "exp_4": {
        "name": "Effectiveness of Training Strategies",
        "paper_section": "IV-E",
        "experiment_type": "ablation",
        "purpose": "To evaluate the multi-stage training strategy of DeepGate2.",
        "benchmarks": [
          {
            "name": "Industrial circuits",
            "variant": "10 circuits (D1-D10)",
            "problem_sizes": "3,183 to 40,496 gates"
          }
        ],
        "baselines": [
          {
            "name": "DeepGate2 without multi-stage training (w/o multi-stage)",
            "description": "DeepGate2 trained with all losses (probability, reconvergence, functionality) in a single stage",
            "source": "proposed method ablation"
          }
        ],
        "conclusion": {
          "main_finding": "Multi-stage training is essential for effectively learning functionality.",
          "supporting_findings": [
            "Multi-stage training reduces functionality loss (L_func) by 51.47% compared to single-stage training.",
            "F1-Score for equivalence identification drops from 0.9434 to 0.7137 without multi-stage training.",
            "Losses for probability prediction (L_prob) and reconvergence identification (L_rc) are similar in both settings."
          ],
          "winner": "DeepGate2 with multi-stage training"
        },
        "detailed_description": "This ablation study investigates the effectiveness of the proposed multi-stage training strategy. DeepGate2 is trained in two stages: Stage 1 learns probability prediction and reconvergence identification; Stage 2 adds the functionality-aware loss. The ablated version (w/o multi-stage) is trained with all three losses simultaneously in a single stage. Both models are evaluated on the logic equivalence gates identification task. The multi-stage model achieves an average F1-Score of 0.9434, while the single-stage model scores only 0.7137. Analysis of the loss values shows that both models perform similarly on probability prediction (L_prob) and reconvergence identification (L_rc). However, the functionality-aware loss (L_func) is 51.47% lower for the multi-stage model (0.0594 vs. 0.1224). The authors explain that functionality learning is complex and costly; by first learning easier tasks (probability and reconvergence), the model can then focus on functionality within groups of gates with similar probability. This staged approach is crucial for effective functionality learning.\n"
      },
      "exp_5": {
        "name": "Logic Synthesis (Downstream Task)",
        "paper_section": "V-A",
        "experiment_type": "main_comparison",
        "purpose": "To apply DeepGate2 to guide a SAT-sweeping engine for logic synthesis and evaluate its efficiency.",
        "benchmarks": [
          {
            "name": "Industrial circuits",
            "variant": "6 circuits (C1-C6)",
            "problem_sizes": "703 to 57,247 AND gates"
          }
        ],
        "baselines": [
          {
            "name": "Original SAT-sweeping engine (&frag)",
            "description": "State-of-the-art SAT sweeper in ABC tool, selecting equivalence classes based on structure",
            "source": "[14], [30]"
          }
        ],
        "conclusion": {
          "main_finding": "DeepGate2 guidance significantly reduces the number of SAT calls and total runtime in SAT sweeping.",
          "supporting_findings": [
            "Average reduction in SAT calls: 53.37% (max 95.88%).",
            "Average reduction in total runtime: 49.46% (max 57.77%).",
            "DeepGate2 prioritizes gates with high functional similarity, leading to more effective merges and counterexamples."
          ],
          "winner": "DeepGate2-guided SAT sweeper"
        },
        "detailed_description": "This experiment integrates DeepGate2 into a SAT-sweeping engine (specifically, the &frag command in ABC) for logic synthesis. The goal is to use DeepGate2's functional similarity predictions to guide the selection of candidate equivalence classes (ECs) for formal verification by a SAT solver. The baseline is the original &frag engine, which selects ECs based on circuit structure without functional guidance. Six industrial circuits (C1-C6) are used, with sizes ranging from 703 to 57,247 AND gates. The evaluation compares the number of satisfiable SAT calls (which return counterexamples and do not merge gates) and the total runtime. Results show that the DeepGate2-guided sweeper reduces the number of SAT calls by an average of 53.37% (up to 95.88%) and reduces total runtime by an average of 49.46% (up to 57.77%). By prioritizing gates with high functional similarity, the sweeper makes more effective SAT solver calls: either proving equivalence (merging gates) or generating counterexamples that provide more conflicts for refining ECs. This demonstrates DeepGate2's practical utility in accelerating logic synthesis.\n"
      },
      "exp_6": {
        "name": "Boolean Satisfiability Solving (Downstream Task)",
        "paper_section": "V-B",
        "experiment_type": "main_comparison",
        "purpose": "To integrate DeepGate2 into a SAT solver (CaDiCal) for circuit-based SAT instances and evaluate runtime improvement.",
        "benchmarks": [
          {
            "name": "Industrial SAT instances",
            "variant": "5 instances (I1-I5) from logic equivalence checking",
            "problem_sizes": "17,495 to 28,672 variables (circuit gates)"
          }
        ],
        "baselines": [
          {
            "name": "CaDiCal SAT solver (Baseline)",
            "description": "Modern SAT solver without DeepGate2 integration",
            "source": "[15]"
          }
        ],
        "conclusion": {
          "main_finding": "DeepGate2 integration reduces total runtime for SAT solving by 40.05% on average, with low overhead.",
          "supporting_findings": [
            "Model inference time is less than 10% of overall runtime on average.",
            "Instance I1 shows the highest reduction (63.62%).",
            "Reduction varies with instance characteristics, not solely size."
          ],
          "winner": "CaDiCal with DeepGate2 integration"
        },
        "detailed_description": "This experiment integrates DeepGate2 into the CaDiCal SAT solver to accelerate solving of circuit-based SAT instances from logic equivalence checking. DeepGate2 provides functional similarity between gates, which is translated to variable correlation in the SAT instance. The integration uses Algorithm 1: when a variable is assigned, correlated variables (with similarity > 1-Î´) are assigned the opposite value to potentially cause conflicts and prune the search space. The baseline is the original CaDiCal solver. Five industrial instances (I1-I5) are tested, with sizes from 17,495 to 28,672 variables. The total runtime includes DeepGate2 model inference time and the solver runtime. Results show an average total runtime reduction of 40.05%, with instance I1 achieving 63.62% reduction. The model inference time is small (e.g., 1.77s for I1) and constitutes less than 10% of overall runtime on average. The reduction is not strictly tied to instance size; for example, I2 (21,952 variables) shows a reduction, while larger instances I4 and I5 show less reduction, indicating that instance characteristics influence the effectiveness. This demonstrates DeepGate2's ability to provide useful functional correlations to speed up SAT solving.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "Comparison with DeepGate on Probability Prediction",
        "benchmarks": "10 industrial circuits (D1-D10)",
        "baselines": "DeepGate",
        "key_conclusion": "DeepGate2 reduces probability prediction error by 13.08% and is 16.43x faster on average."
      },
      {
        "experiment": "Comparison with other Models on Logic Equivalence Gates Identification",
        "benchmarks": "10 industrial circuits (D1-D10)",
        "baselines": "DeepGate, FGNN",
        "key_conclusion": "DeepGate2 achieves an average F1-Score of 0.9434, significantly outperforming DeepGate (0.6778) and FGNN (0.4402)."
      },
      {
        "experiment": "Effectiveness of PI Encoding Strategy (Ablation)",
        "benchmarks": "10 industrial circuits (D1-D10)",
        "baselines": "DeepGate2 without PI encoding (w/o PIE)",
        "key_conclusion": "Removing PI encoding reduces F1-Score by 20.07%, showing its importance for modeling structural information."
      },
      {
        "experiment": "Effectiveness of Training Strategies (Ablation)",
        "benchmarks": "10 industrial circuits (D1-D10)",
        "baselines": "DeepGate2 without multi-stage training (w/o multi-stage)",
        "key_conclusion": "Multi-stage training reduces functionality loss (L_func) by 51.47%, crucial for learning functional similarity."
      },
      {
        "experiment": "Logic Synthesis (Downstream Task)",
        "benchmarks": "6 industrial circuits (C1-C6)",
        "baselines": "Original SAT-sweeping engine (&frag)",
        "key_conclusion": "DeepGate2-guided SAT sweeper reduces SAT calls by 53.37% and runtime by 49.46% on average."
      },
      {
        "experiment": "Boolean Satisfiability Solving (Downstream Task)",
        "benchmarks": "5 industrial SAT instances (I1-I5)",
        "baselines": "CaDiCal SAT solver",
        "key_conclusion": "DeepGate2 integration reduces total runtime by 40.05% on average, with model inference taking <10% of overall time."
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "Industrial circuits (D1-D10)",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3",
            "exp_4"
          ]
        },
        {
          "benchmark": "Industrial circuits (C1-C6)",
          "used_in_experiments": [
            "exp_5"
          ]
        },
        {
          "benchmark": "Industrial SAT instances (I1-I5)",
          "used_in_experiments": [
            "exp_6"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "DeepGate",
          "compared_in_experiments": [
            "exp_1",
            "exp_2"
          ]
        },
        {
          "baseline": "FGNN",
          "compared_in_experiments": [
            "exp_2"
          ]
        },
        {
          "baseline": "DeepGate2 without PI encoding (w/o PIE)",
          "compared_in_experiments": [
            "exp_3"
          ]
        },
        {
          "baseline": "DeepGate2 without multi-stage training (w/o multi-stage)",
          "compared_in_experiments": [
            "exp_4"
          ]
        },
        {
          "baseline": "Original SAT-sweeping engine (&frag)",
          "compared_in_experiments": [
            "exp_5"
          ]
        },
        {
          "baseline": "CaDiCal SAT solver",
          "compared_in_experiments": [
            "exp_6"
          ]
        }
      ],
      "overall_narrative": "The experiments collectively demonstrate that DeepGate2 significantly improves upon DeepGate in both effectiveness and efficiency for circuit representation learning. First, exp_1 and exp_2 show that DeepGate2 outperforms DeepGate and FGNN on core tasks (probability prediction and equivalence identification) while being much faster. The ablation studies (exp_3 and exp_4) validate key design choices: PI encoding and multi-stage training are crucial for learning functionality. Finally, the downstream task applications (exp_5 and exp_6) prove that DeepGate2's representations are practical, reducing runtime in logic synthesis by 49.46% and SAT solving by 40.05%. Together, these experiments support the paper's thesis that DeepGate2 is a superior functionality-aware framework with real-world utility in EDA.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "DeepGate2 learns better functionality-aware representations than DeepGate.",
          "supported_by": [
            "exp_1",
            "exp_2"
          ],
          "strength": "strong"
        },
        {
          "claim": "DeepGate2 is more efficient than DeepGate (order of magnitude speedup).",
          "supported_by": [
            "exp_1"
          ],
          "strength": "strong"
        },
        {
          "claim": "PI encoding and multi-stage training are essential components of DeepGate2.",
          "supported_by": [
            "exp_3",
            "exp_4"
          ],
          "strength": "strong"
        },
        {
          "claim": "DeepGate2 improves performance in downstream EDA tasks (logic synthesis and SAT solving).",
          "supported_by": [
            "exp_5",
            "exp_6"
          ],
          "strength": "strong"
        }
      ],
      "limitations_acknowledged": [
        "The reduction in SAT solving runtime varies with instance characteristics, not solely size.",
        "Functionality learning relies on incomplete truth tables from limited simulations (15,000 patterns)."
      ],
      "key_takeaways": [
        "DeepGate2 uses pairwise truth table differences as supervision and a one-round GNN to achieve superior functionality-aware circuit representations.",
        "DeepGate2 is 16.43x faster than DeepGate for inference and reduces prediction error by 13.08%.",
        "DeepGate2 significantly outperforms baselines in logic equivalence identification (F1-Score 0.9434 vs. 0.6778).",
        "DeepGate2's representations are effective in real-world tasks, reducing logic synthesis runtime by 49.46% and SAT solving runtime by 40.05%."
      ]
    }
  },
  {
    "filename": "2309.11452_Using_deep_learning_to_construct_stochastic_local_.md",
    "paper_title": "USING DEEP LEARNING TO CONSTRUCT STOCHASTIC LOCAL SEARCH SAT SOLVERS WITH PERFORMANCE BOUNDS",
    "paper_focus": "The paper proposes using Graph Neural Networks as oracle factories to generate instance-specific distributions that boost the performance of stochastic local search SAT solvers, with performance guarantees motivated by theoretical computer science results.",
    "experiment_inventory": {
      "total_experiments": 6,
      "main_experiments": 4,
      "ablation_studies": 2,
      "other_experiments": 0
    },
    "experiments": {
      "exp_1": {
        "name": "MT Algorithm Variants Comparison",
        "paper_section": "Section 3, Section 4, Figure 4, Table 1",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate the performance improvement of MT algorithm variants when using GNN-based oracles compared to uniform baselines.",
        "benchmarks": [
          {
            "name": "Random 3-SAT",
            "variant": "CNF form with up to 3 literals per clause",
            "problem_sizes": "n=100,200,300 variables; Î±=m/n from 1 to 4.82"
          }
        ],
        "baselines": [
          {
            "name": "Uniform MT",
            "description": "Moser-Tardos algorithm using uniform random sampling",
            "source": "Standard MT algorithm"
          },
          {
            "name": "Hybrid MT",
            "description": "Uses trained oracle only for initialization, then switches to uniform updating",
            "source": "Proposed method variant"
          }
        ],
        "conclusion": {
          "main_finding": "Boosted MT (full oracle access) significantly outperforms both uniform and hybrid variants across all metrics.",
          "supporting_findings": [
            "Solves instances with 17% higher average Î± value (1.90 vs 1.50 for uniform)",
            "Requires 35% fewer steps on average",
            "8x improvement in median number of steps (134 vs 1163)",
            "Solves 80.3% of instances vs 69.1% for uniform"
          ],
          "winner": "Boosted MT (full oracle access)"
        },
        "detailed_description": "This experiment compares three variants of the Moser-Tardos algorithm on random 3-SAT instances of varying difficulty: uniform MT (baseline), hybrid MT (oracle for initialization only), and boosted MT (full oracle access for both initialization and updating). The test set consists of 2052 instances equally distributed across n=100,200,300 variables and Î± values from 1 to 4.82. Each algorithm was run for up to 1 million steps with 5 runs per instance. Performance was measured using average steps, median steps across instances and runs, and percentage of instances solved. The results show that the boosted MT algorithm dramatically outperforms the other variants, particularly in median steps where it shows an 8x improvement. The algorithm also solves instances with higher Î± values on average, indicating it can handle more difficult problems. Interestingly, the hybrid variant that only uses the oracle for initialization quickly loses its advantage, demonstrating the importance of continuous oracle access during the search process.\n"
      },
      "exp_2": {
        "name": "WalkSAT Algorithm Variants Comparison",
        "paper_section": "Section 3, Section 4, Figure 5, Table 1",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate the performance improvement of WalkSAT algorithm variants when using GNN-based oracles compared to uniform baselines.",
        "benchmarks": [
          {
            "name": "Random 3-SAT",
            "variant": "CNF form with up to 3 literals per clause",
            "problem_sizes": "n=100,200,300 variables; Î±=m/n from 1 to 4.82"
          }
        ],
        "baselines": [
          {
            "name": "Uniform WalkSAT",
            "description": "WalkSAT algorithm using uniform random sampling",
            "source": "Standard WalkSAT algorithm"
          },
          {
            "name": "Hybrid WalkSAT",
            "description": "Uses trained oracle only for initialization, then switches to uniform updating",
            "source": "Proposed method variant"
          }
        ],
        "conclusion": {
          "main_finding": "Boosted WalkSAT (full oracle access) significantly outperforms both uniform and hybrid variants across all metrics.",
          "supporting_findings": [
            "Solves instances with 17% higher average Î± value (2.12 vs 1.96 for uniform)",
            "5x improvement in median number of steps (93 vs 462)",
            "Solves 87.0% of instances vs 81.2% for uniform",
            "WalkSAT variants generally outperform corresponding MT variants"
          ],
          "winner": "Boosted WalkSAT (full oracle access)"
        },
        "detailed_description": "This experiment compares three variants of the WalkSAT algorithm on the same random 3-SAT test set: uniform WalkSAT, hybrid WalkSAT (oracle for initialization only), and boosted WalkSAT (full oracle access). The experimental setup matches that of the MT comparison, with 1 million step cutoff and 5 runs per instance. Results show that the boosted WalkSAT provides substantial improvements over the uniform baseline, with a 5x improvement in median steps and the ability to solve instances with higher Î± values. The hybrid variant again shows limited benefit compared to the full oracle access. Notably, the WalkSAT algorithm in all variants outperforms the corresponding MT variants, which is consistent with known performance characteristics of these algorithms. The results demonstrate that the GNN-based oracle provides significant benefits to both types of SLS solvers.\n"
      },
      "exp_3": {
        "name": "Loss Function Ablation for MT Algorithm",
        "paper_section": "Section 4, Figure 6",
        "experiment_type": "ablation",
        "purpose": "To analyze the contribution of individual loss terms (LLL loss and Gibbs loss) to the performance of the boosted MT algorithm.",
        "benchmarks": [
          {
            "name": "Random 3-SAT",
            "variant": "CNF form with up to 3 literals per clause",
            "problem_sizes": "n=100,200,300 variables; Î±=m/n from 1 to 4.82"
          }
        ],
        "baselines": [
          {
            "name": "Uniform MT",
            "description": "Standard MT algorithm without oracle",
            "source": "Standard baseline"
          }
        ],
        "conclusion": {
          "main_finding": "The combination of LLL loss and Gibbs loss produces the best results, but LLL loss contributes more significantly to performance improvement.",
          "supporting_findings": [
            "Models trained with only LLL loss perform better than those with only Gibbs loss",
            "The combination of both losses yields optimal performance",
            "LLL loss has theoretical connection to MT performance guarantees"
          ],
          "winner": "Model trained with both LLL and Gibbs losses"
        },
        "detailed_description": "This ablation study investigates the impact of the two loss components used to train the GNN oracle factory: the LovÃ¡sz Local Lemma (LLL) loss (motivated by theoretical performance guarantees) and the Gibbs loss (cross-entropy with thermal distribution). The experiment compares the boosted MT algorithm when the underlying model was trained with: (1) only LLL loss, (2) only Gibbs loss, (3) both losses (equally weighted as in main experiments), against the uniform MT baseline. Results show that while the combination of both losses produces the best performance, the LLL loss contributes more substantially to the improvement. This is expected since the LLL loss is directly motivated by theoretical results that guarantee better performance for the MT algorithm when the condition in Proposition 2.1 is better satisfied. The Gibbs loss appears to provide additional regularization benefits but is less critical to the performance improvement.\n"
      },
      "exp_4": {
        "name": "Loss Function Ablation for WalkSAT Algorithm",
        "paper_section": "Section 4, Figure 7",
        "experiment_type": "ablation",
        "purpose": "To analyze the contribution of individual loss terms to the performance of the boosted WalkSAT algorithm.",
        "benchmarks": [
          {
            "name": "Random 3-SAT",
            "variant": "CNF form with up to 3 literals per clause",
            "problem_sizes": "n=100,200,300 variables; Î±=m/n from 1 to 4.82"
          }
        ],
        "baselines": [
          {
            "name": "Uniform WalkSAT",
            "description": "Standard WalkSAT algorithm without oracle",
            "source": "Standard baseline"
          }
        ],
        "conclusion": {
          "main_finding": "The LLL loss unexpectedly contributes significantly to WalkSAT performance improvement despite no known theoretical connection.",
          "supporting_findings": [
            "LLL loss contributes more than Gibbs loss to WalkSAT improvement",
            "Combination of both losses yields best results",
            "Surprising since no theoretical result connects WalkSAT to LLL conditions"
          ],
          "winner": "Model trained with both LLL and Gibbs losses"
        },
        "detailed_description": "This ablation study examines the impact of the loss components on the WalkSAT algorithm performance, using the same three training configurations as for MT: LLL loss only, Gibbs loss only, and both losses. Surprisingly, the results show that the LLL loss contributes significantly to the performance improvement of WalkSAT, even though there is no known theoretical result connecting the WalkSAT algorithm to the conditions in Propositions 2.1 and 2.2 that motivate the LLL loss. As with MT, the combination of both losses produces the best results, but the LLL loss appears to be the more important component. This suggests that the properties captured by the LLL loss (related to exploiting local structure and dependency relationships in the SAT instance) are beneficial to SLS solvers in general, not just those with theoretical performance guarantees.\n"
      },
      "exp_5": {
        "name": "Continuous Oracle Access vs Initialization Only",
        "paper_section": "Section 4, Figures 4 and 5",
        "experiment_type": "main_comparison",
        "purpose": "To compare the effectiveness of continuous oracle access during search versus using the oracle only for initialization.",
        "benchmarks": [
          {
            "name": "Random 3-SAT",
            "variant": "CNF form with up to 3 literals per clause",
            "problem_sizes": "n=100,200,300 variables; Î±=m/n from 1 to 4.82"
          }
        ],
        "baselines": [
          {
            "name": "Hybrid MT",
            "description": "Uses oracle only for initialization, then uniform updates",
            "source": "Proposed method variant"
          },
          {
            "name": "Hybrid WalkSAT",
            "description": "Uses oracle only for initialization, then uniform updates",
            "source": "Proposed method variant"
          }
        ],
        "conclusion": {
          "main_finding": "Continuous access to the oracle throughout the search process provides significantly better results than using the oracle only for initialization.",
          "supporting_findings": [
            "Hybrid variants quickly lose initial advantage over uniform variants",
            "Full oracle access provides both fewer steps and ability to solve more instances",
            "The advantage is consistent across both MT and WalkSAT algorithms"
          ],
          "winner": "Full oracle access variants (Boosted MT and Boosted WalkSAT)"
        },
        "detailed_description": "This experiment compares the hybrid variants (which use the GNN-based oracle only to initialize the candidate assignment, then switch to uniform updating) against the full oracle access variants. The results clearly show that the hybrid approach provides only marginal improvement over the uniform algorithms and quickly loses any initial advantage. In contrast, the full oracle access variants maintain their advantage throughout the search process. This is evident from the curves showing clauses violated vs steps and solved instances vs steps, where the hybrid variants' performance curves are much closer to the uniform variants than to the full oracle variants. This demonstrates that the benefit of the learned oracle comes not just from providing a better starting point, but from guiding the search process throughout, allowing the solver to exploit instance-specific structure at each step.\n"
      },
      "exp_6": {
        "name": "Detailed Performance Analysis (Appendix)",
        "paper_section": "Appendix A.2, Figures 8 and 9",
        "experiment_type": "main_comparison",
        "purpose": "To provide detailed analysis of the performance characteristics of boosted algorithms compared to uniform baselines.",
        "benchmarks": [
          {
            "name": "Random 3-SAT",
            "variant": "CNF form with up to 3 literals per clause",
            "problem_sizes": "n=100,200,300 variables; Î±=m/n from 1 to 4.82"
          }
        ],
        "baselines": [
          {
            "name": "Uniform MT",
            "description": "Standard MT algorithm without oracle",
            "source": "Standard baseline"
          },
          {
            "name": "Uniform WalkSAT",
            "description": "Standard WalkSAT algorithm without oracle",
            "source": "Standard baseline"
          }
        ],
        "conclusion": {
          "main_finding": "Boosted algorithms shift the hardness curve to the right, solving instances at higher Î± values where uniform algorithms fail.",
          "supporting_findings": [
            "Boosted MT solves instances efficiently up to Î±â‰ˆ3.0-3.2 vs â‰ˆ2.5 for uniform MT",
            "Boosted WalkSAT shows similar rightward shift in hardness curve",
            "Performance improvements are most dramatic in median steps rather than mean"
          ],
          "winner": "Boosted variants (MT and WalkSAT)"
        },
        "detailed_description": "This detailed analysis in the appendix provides additional plots comparing the uniform and boosted versions of both algorithms. The plots show mean and median steps needed as functions of Î±, percentage of instances solved vs Î±, steps comparison scatter plots, clauses violated vs steps, and solved instances vs steps. The key observation is that the boosted algorithms effectively \"shift the hardness curve to the right\" - they can solve instances efficiently at higher Î± values where the uniform algorithms either fail or require exponentially more steps. For the MT algorithm, the uniform version experiences a hardness barrier around Î±=2.45, while the boosted version extends this to around Î±=3.0-3.2. The WalkSAT algorithm shows a similar pattern. The scatter plots comparing steps taken by uniform vs boosted algorithms show that most points lie below the diagonal, indicating the boosted version is faster for most instances, with the advantage being more pronounced at higher Î± values.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "MT Algorithm Variants Comparison",
        "benchmarks": "Random 3-SAT instances (n=100,200,300, Î±=1-4.82)",
        "baselines": "Uniform MT, Hybrid MT (oracle initialization only)",
        "key_conclusion": "Boosted MT (full oracle access) solves 17% harder instances with 8x median step improvement"
      },
      {
        "experiment": "WalkSAT Algorithm Variants Comparison",
        "benchmarks": "Random 3-SAT instances (n=100,200,300, Î±=1-4.82)",
        "baselines": "Uniform WalkSAT, Hybrid WalkSAT (oracle initialization only)",
        "key_conclusion": "Boosted WalkSAT solves 17% harder instances with 5x median step improvement"
      },
      {
        "experiment": "Loss Function Ablation for MT Algorithm",
        "benchmarks": "Random 3-SAT instances (n=100,200,300, Î±=1-4.82)",
        "baselines": "Uniform MT",
        "key_conclusion": "LLL loss contributes more than Gibbs loss; combination works best for MT"
      },
      {
        "experiment": "Loss Function Ablation for WalkSAT Algorithm",
        "benchmarks": "Random 3-SAT instances (n=100,200,300, Î±=1-4.82)",
        "baselines": "Uniform WalkSAT",
        "key_conclusion": "LLL loss unexpectedly contributes significantly despite no theoretical connection"
      },
      {
        "experiment": "Continuous Oracle Access vs Initialization Only",
        "benchmarks": "Random 3-SAT instances (n=100,200,300, Î±=1-4.82)",
        "baselines": "Hybrid variants (oracle initialization only)",
        "key_conclusion": "Continuous oracle access provides significantly better performance than just initialization"
      },
      {
        "experiment": "Detailed Performance Analysis (Appendix)",
        "benchmarks": "Random 3-SAT instances (n=100,200,300, Î±=1-4.82)",
        "baselines": "Uniform MT, Uniform WalkSAT",
        "key_conclusion": "Boosted algorithms shift the hardness curve rightward, solving instances at higher Î± values"
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "Random 3-SAT",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3",
            "exp_4",
            "exp_5",
            "exp_6"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "Uniform MT",
          "compared_in_experiments": [
            "exp_1",
            "exp_3",
            "exp_6"
          ]
        },
        {
          "baseline": "Uniform WalkSAT",
          "compared_in_experiments": [
            "exp_2",
            "exp_4",
            "exp_6"
          ]
        },
        {
          "baseline": "Hybrid MT",
          "compared_in_experiments": [
            "exp_1",
            "exp_5"
          ]
        },
        {
          "baseline": "Hybrid WalkSAT",
          "compared_in_experiments": [
            "exp_2",
            "exp_5"
          ]
        }
      ],
      "overall_narrative": "The experiments together demonstrate that GNN-based oracles significantly boost the performance of both MT and WalkSAT SLS solvers on random 3-SAT instances. The boost manifests as: (1) ability to solve instances with higher clause-to-variable ratios (Î±), (2) fewer steps required, especially in median performance, and (3) higher percentage of instances solved within the step limit. The ablation studies show that the theoretically-motivated LLL loss is a key contributor to this improvement, even for algorithms without theoretical guarantees. The comparison between hybrid and full oracle access shows that continuous guidance throughout the search is essential, not just better initialization. Overall, the experiments validate the paper's core claim that deep learning can be used to construct SAT solvers with performance improvements that are motivated by theoretical computer science results.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "GNN-based oracles can significantly boost SLS solver performance",
          "supported_by": [
            "exp_1",
            "exp_2",
            "exp_6"
          ],
          "strength": "strong"
        },
        {
          "claim": "The LLL loss (motivated by theoretical guarantees) contributes to performance improvement",
          "supported_by": [
            "exp_3",
            "exp_4"
          ],
          "strength": "strong"
        },
        {
          "claim": "Continuous oracle access is better than just initialization",
          "supported_by": [
            "exp_5"
          ],
          "strength": "strong"
        },
        {
          "claim": "The approach works for both MT (with theoretical guarantees) and WalkSAT (without theoretical connection)",
          "supported_by": [
            "exp_1",
            "exp_2",
            "exp_3",
            "exp_4"
          ],
          "strength": "strong"
        }
      ],
      "limitations_acknowledged": [
        "Experiments limited to random 3-SAT instances; practical application to real-world SAT instances not tested",
        "Solvers are not yet competitive with state-of-the-art SAT solvers",
        "Training requires satisfiable instances for the Gibbs loss component"
      ],
      "key_takeaways": [
        "GNN-based oracles boost SLS solver performance by 17% in terms of solvable Î± values and reduce steps by 35% on average",
        "The theoretically-motivated LLL loss is effective in practice, even for algorithms without theoretical guarantees",
        "Continuous oracle access throughout search is essential for maximum benefit",
        "The approach bridges theoretical computer science results with practical deep learning applications"
      ]
    }
  },
  {
    "filename": "2309.16941_G4SATBench_Benchmarking_and_Advancing_SAT_Solving_.md",
    "paper_title": "G4SATBench: Benchmarking and Advancing SAT Solving with Graph Neural Networks",
    "paper_focus": "Proposes G4SATBench, a comprehensive benchmark study to evaluate and compare Graph Neural Network (GNN)-based SAT solvers across diverse datasets, tasks, and architectures.",
    "experiment_inventory": {
      "total_experiments": 6,
      "experiments": [
        {
          "id": "exp_1",
          "name": "Satisfiability Prediction Benchmarking (Identical Distribution)",
          "paper_section": "5.1, Table 1, Figure 3, Figure 4",
          "type": "main_comparison"
        },
        {
          "id": "exp_2",
          "name": "Satisfying Assignment Prediction Benchmarking (Training Loss Comparison)",
          "paper_section": "5.2, Table 2, Table 10",
          "type": "main_comparison"
        },
        {
          "id": "exp_3",
          "name": "Unsat-core Variable Prediction Benchmarking",
          "paper_section": "5.3, Table 3",
          "type": "main_comparison"
        },
        {
          "id": "exp_4",
          "name": "Comparison with CDCL Heuristic (Clause-Augmented & Contrastive Pretraining)",
          "paper_section": "6.1, Table 4, Table 5",
          "type": "main_comparison"
        },
        {
          "id": "exp_5",
          "name": "Comparison with LS Heuristic (Random Initialization & Assignment Analysis)",
          "paper_section": "6.2, Table 6, Figure 5",
          "type": "main_comparison"
        },
        {
          "id": "exp_6",
          "name": "Comparison with State-of-the-Art SAT Solvers",
          "paper_section": "Appendix D.2, Table 12",
          "type": "scalability"
        }
      ]
    },
    "experiments": {
      "exp_1": {
        "name": "Satisfiability Prediction Benchmarking (Identical Distribution)",
        "paper_section": "5.1, Table 1, Figures 3 & 4",
        "experiment_type": "main_comparison",
        "purpose": "To benchmark the performance of various GNN models on the task of predicting satisfiability when trained and tested on datasets from the same distribution.",
        "benchmarks": [
          {
            "name": "SR",
            "variant": "Easy (n=10-40), Medium (n=40-200)",
            "problem_sizes": "Variables: 10-40 (Easy), 40-200 (Medium)"
          },
          {
            "name": "3-SAT",
            "variant": "Easy (n=10-40), Medium (n=40-200)",
            "problem_sizes": "Variables: 10-40 (Easy), 40-200 (Medium)"
          },
          {
            "name": "CA (Community Attachment)",
            "variant": "Easy (n=10-40), Medium (n=40-200)",
            "problem_sizes": "Variables: 10-40 (Easy), 40-200 (Medium)"
          },
          {
            "name": "PS (Popularity-Similarity)",
            "variant": "Easy (n=10-40), Medium (n=40-200)",
            "problem_sizes": "Variables: 10-40 (Easy), 40-200 (Medium)"
          },
          {
            "name": "k-Clique",
            "variant": "Easy (v=5-15, k=3-4), Medium (v=15-20, k=3-5)",
            "problem_sizes": "Vertices: 5-15 (Easy), 15-20 (Medium)"
          },
          {
            "name": "k-Domset",
            "variant": "Easy (v=5-15, k=2-3), Medium (v=15-20, k=3-5)",
            "problem_sizes": "Vertices: 5-15 (Easy), 15-20 (Medium)"
          },
          {
            "name": "k-Vercov",
            "variant": "Easy (v=5-15, k=3-5), Medium (v=10-20, k=6-8)",
            "problem_sizes": "Vertices: 5-15 (Easy), 10-20 (Medium)"
          }
        ],
        "baselines": [
          {
            "name": "NeuroSAT",
            "description": "Original NeuroSAT model using LayerNormLSTM as update function.",
            "source": "Selsam et al., 2019"
          },
          {
            "name": "GCN (Graph Convolutional Network)",
            "description": "Heterogeneous version adapted for LCG* and VCG* graphs.",
            "source": "Kipf & Welling, 2017"
          },
          {
            "name": "GGNN (Gated Graph Neural Network)",
            "description": "Heterogeneous version adapted for LCG* and VCG* graphs.",
            "source": "Li et al., 2016"
          },
          {
            "name": "GIN (Graph Isomorphism Network)",
            "description": "Heterogeneous version adapted for LCG* and VCG* graphs.",
            "source": "Xu et al., 2019"
          }
        ],
        "conclusion": {
          "main_finding": "All GNN models achieve strong classification accuracy (>94% on most easy/medium datasets) except on the medium SR dataset (~66-85%), where the subtle difference between formula pairs poses a significant challenge.",
          "supporting_findings": [
            "The choice of graph construction (LCG* vs. VCG*) does not significantly impact performance.",
            "NeuroSAT (on LCG*) and GGNN (on VCG*) achieve the best overall performance.",
            "Models trained on SR dataset show better cross-dataset generalization (Figure 3).",
            "Models generalize poorly to larger formulas beyond their training size but perform relatively better on smaller instances (Figure 4,6).",
            "Training with 32 message-passing iterations yields optimal performance; models generalize well to nearby iteration counts (16, 64) at test time (Figure 7)."
          ],
          "winner": "NeuroSAT (LCG*) and GGNN (VCG*) achieve the best overall performance across datasets."
        },
        "detailed_description": "This experiment serves as the primary benchmarking evaluation for the satisfiability prediction task. All GNN models (NeuroSAT, GCN, GGNN, GIN) were implemented on both LCG* and VCG* graph encodings (except NeuroSAT only on LCG*). They were trained and tested on datasets from the same distribution and difficulty level. Hyperparameters were tuned via grid search, and results were averaged over 3 random seeds. The primary metric was classification accuracy. Results in Table 1 show high performance across most datasets, with a notable dip for the medium SR dataset. This is attributed to the dataset's construction, where satisfiable/unsatisfiable pairs differ by only a single literal, making discrimination difficult. Additional experiments in Figures 3, 4, 6, and 7 explore generalization across distributions, difficulty levels, and the impact of message-passing iterations. Key insights are that models generalize better when trained on the SR dataset, struggle with out-of-distribution size generalization (though perform better on smaller instances), and that T=32 is the optimal training iteration count.\n"
      },
      "exp_2": {
        "name": "Satisfying Assignment Prediction Benchmarking (Training Loss Comparison)",
        "paper_section": "5.2, Table 2, Table 10 (Appendix)",
        "experiment_type": "main_comparison",
        "purpose": "To benchmark GNN models on the task of constructing a satisfying assignment and to compare the efficacy of supervised versus unsupervised training objectives.",
        "benchmarks": [
          {
            "name": "SR",
            "variant": "Easy, Medium",
            "problem_sizes": "Variables: 10-40 (Easy), 40-200 (Medium)"
          },
          {
            "name": "3-SAT",
            "variant": "Easy, Medium",
            "problem_sizes": "Variables: 10-40 (Easy), 40-200 (Medium)"
          },
          {
            "name": "CA",
            "variant": "Easy, Medium",
            "problem_sizes": "Variables: 10-40 (Easy), 40-200 (Medium)"
          },
          {
            "name": "PS",
            "variant": "Easy, Medium",
            "problem_sizes": "Variables: 10-40 (Easy), 40-200 (Medium)"
          },
          {
            "name": "k-Clique",
            "variant": "Easy, Medium",
            "problem_sizes": "Vertices: 5-15 (Easy), 15-20 (Medium)"
          },
          {
            "name": "k-Domset",
            "variant": "Easy, Medium",
            "problem_sizes": "Vertices: 5-15 (Easy), 15-20 (Medium)"
          },
          {
            "name": "k-Vercov",
            "variant": "Easy, Medium",
            "problem_sizes": "Vertices: 5-15 (Easy), 10-20 (Medium)"
          }
        ],
        "baselines": [
          {
            "name": "NeuroSAT (LCG*)",
            "description": "Evaluated with SUP (supervised), UNS1, and UNS2 losses.",
            "source": "Selsam et al., 2019"
          },
          {
            "name": "GGNN (VCG*)",
            "description": "Evaluated with SUP (supervised), UNS1, and UNS2 losses.",
            "source": "Li et al., 2016"
          },
          {
            "name": "Other GNNs (GCN, GGNN, GIN on LCG*/VCG*)",
            "description": "Full results for all models with all losses are in Appendix Table 10.",
            "source": "Various (see paper)"
          }
        ],
        "conclusion": {
          "main_finding": "Unsupervised training methods (particularly the UNS2 loss) consistently outperform supervised learning for the satisfying assignment prediction task across most datasets.",
          "supporting_findings": [
            "Supervised training biases models towards a specific satisfying solution, harming generalization, especially on medium CA and PS datasets.",
            "The UNS1 loss can lead to unstable training and failure to converge in some cases.",
            "The UNS2 loss demonstrates strong and stable performance across all datasets.",
            "NeuroSAT trained with UNS2 achieves the highest solving accuracy on most medium datasets.",
            "Models generalize better when trained on medium datasets compared to easy ones (Figure 8).",
            "Models trained on combinatorial datasets (k-Clique, k-Domset, k-Vercov) struggle to generalize to other domains, suggesting overfitting to specific graph structures (Figure 9).",
            "Different inference algorithms (standard readout, 2-clustering, multiple predictions) yield similar performance, indicating GNNs learn to identify satisfying assignments in latent space (Figure 10).",
            "Training on noisy datasets (50% unsatisfiable instances) with UNS2 loss does not significantly harm performance, showing robustness (Table 11)."
          ],
          "winner": "NeuroSAT with UNS2 loss generally performs best."
        },
        "detailed_description": "This experiment evaluates GNN models on the task of predicting a satisfying assignment for satisfiable instances. Three training objectives are compared: supervised cross-entropy loss (SUP) and two unsupervised losses (UNS1 from Amizadeh et al. 2019a and UNS2 from Ozolins et al. 2022). The primary models are NeuroSAT (LCG*) and GGNN (VCG*), with full results for all GNN baselines in Appendix Table 10. The key metric is solving accuracy (whether any predicted assignment satisfies the formula). Results in Table 2 show UNS2 loss generally yields the highest accuracy, followed by UNS1, with SUP performing worst. The paper hypothesizes that SUP overfits to a specific solution in instances with many satisfying assignments. Additional analyses explore generalization across difficulty levels (Figure 8) and datasets (Figure 9), the impact of inference algorithms (Figure 10), and training on noisy data (Table 11). A critical finding is that models trained on combinatorial problems fail to generalize to other domains, indicating they may learn dataset-specific features rather than a general solving strategy.\n"
      },
      "exp_3": {
        "name": "Unsat-core Variable Prediction Benchmarking",
        "paper_section": "5.3, Table 3",
        "experiment_type": "main_comparison",
        "purpose": "To benchmark GNN models on the task of predicting which variables belong to an unsatisfiable core, a task useful for guiding CDCL solvers.",
        "benchmarks": [
          {
            "name": "SR",
            "variant": "Easy, Medium",
            "problem_sizes": "Variables: 10-40 (Easy), 40-200 (Medium)"
          },
          {
            "name": "3-SAT",
            "variant": "Easy, Medium",
            "problem_sizes": "Variables: 10-40 (Easy), 40-200 (Medium)"
          },
          {
            "name": "CA",
            "variant": "Easy, Medium",
            "problem_sizes": "Variables: 10-40 (Easy), 40-200 (Medium)"
          },
          {
            "name": "PS",
            "variant": "Easy, Medium",
            "problem_sizes": "Variables: 10-40 (Easy), 40-200 (Medium)"
          },
          {
            "name": "k-Clique",
            "variant": "Easy, Medium",
            "problem_sizes": "Vertices: 5-15 (Easy), 15-20 (Medium)"
          },
          {
            "name": "k-Domset",
            "variant": "Easy, Medium",
            "problem_sizes": "Vertices: 5-15 (Easy), 15-20 (Medium)"
          },
          {
            "name": "k-Vercov",
            "variant": "Easy, Medium",
            "problem_sizes": "Vertices: 5-15 (Easy), 10-20 (Medium)"
          }
        ],
        "baselines": [
          {
            "name": "NeuroSAT (LCG*)",
            "description": "",
            "source": "Selsam et al., 2019"
          },
          {
            "name": "GCN (LCG*)",
            "description": "",
            "source": "Kipf & Welling, 2017"
          },
          {
            "name": "GGNN (LCG*)",
            "description": "",
            "source": "Li et al., 2016"
          },
          {
            "name": "GIN (LCG*)",
            "description": "",
            "source": "Xu et al., 2019"
          },
          {
            "name": "GCN (VCG*)",
            "description": "",
            "source": "Kipf & Welling, 2017"
          },
          {
            "name": "GGNN (VCG*)",
            "description": "",
            "source": "Li et al., 2016"
          },
          {
            "name": "GIN (VCG*)",
            "description": "",
            "source": "Xu et al., 2019"
          }
        ],
        "conclusion": {
          "main_finding": "All GNN models achieve very high classification accuracy for unsat-core variable prediction (>82% on most datasets, often >99%), with NeuroSAT consistently performing best.",
          "supporting_findings": [
            "Performance is uniformly excellent across all models and datasets, indicating this is a relatively easier task for GNNs.",
            "Models show strong generalization ability across difficulty levels (Figure 11).",
            "Models generalize well across different datasets except for CA, where the distribution of unsat-core variables is highly imbalanced (Figure 12)."
          ],
          "winner": "NeuroSAT achieves the best results across most datasets."
        },
        "detailed_description": "This experiment benchmarks GNN models on the binary node classification task of predicting unsat-core variables for unsatisfiable instances. All seven GNN baselines (on LCG* and VCG*) are evaluated. The metric is classification accuracy. Results in Table 3 show exceptionally high accuracy across all datasets and models, often exceeding 99%. The paper notes that even imperfect predictions (e.g., 90% accuracy) have been shown to be effective for guiding CDCL solvers. Additional experiments in Appendix C.4 (Figures 11, 12) show models generalize well when trained on easy or medium datasets and across most datasets, except for the CA dataset. The failure on CA is attributed to its unique property of having very few unsat-core variables compared to non-core variables, a distribution not seen in other training sets.\n"
      },
      "exp_4": {
        "name": "Comparison with CDCL Heuristic (Clause-Augmented & Contrastive Pretraining)",
        "paper_section": "6.1, Table 4, Table 5",
        "experiment_type": "main_comparison",
        "purpose": "To investigate whether GNN-based SAT solvers can learn and benefit from the backtracking search with Conflict-Driven Clause Learning (CDCL) heuristic.",
        "benchmarks": [
          {
            "name": "SR",
            "variant": "Easy, Medium",
            "problem_sizes": "Variables: 10-40 (Easy), 40-200 (Medium)"
          },
          {
            "name": "3-SAT",
            "variant": "Easy, Medium",
            "problem_sizes": "Variables: 10-40 (Easy), 40-200 (Medium)"
          }
        ],
        "baselines": [
          {
            "name": "NeuroSAT (LCG*)",
            "description": "Tested on original and clause-augmented instances, with and without contrastive pretraining.",
            "source": "Selsam et al., 2019"
          },
          {
            "name": "GGNN (VCG*)",
            "description": "Tested on original and clause-augmented instances, with and without contrastive pretraining.",
            "source": "Li et al., 2016"
          }
        ],
        "conclusion": {
          "main_finding": "GNN models show improved performance when trained on clause-augmented instances but fail to implicitly learn the CDCL heuristic when trained on original instances, and contrastive pretraining offers minimal performance gains.",
          "supporting_findings": [
            "Training on augmented instances (with learned clauses) leads to significant performance gains in both satisfiability (T1) and assignment prediction (T2) tasks.",
            "However, when trained on original instances, models perform poorly on augmented instances, even though these are easily solvable by CDCL.",
            "This suggests GNNs do not learn the CDCL heuristic implicitly.",
            "Using SimCLR-style contrastive pretraining to align original and augmented formula representations yields only minor improvements (<1% in most cases), indicating difficulty in embedding CDCL heuristic in latent space.",
            "This aligns with prior work suggesting static GNNs struggle with dynamic graph changes induced by clause learning."
          ],
          "winner": "Models trained directly on augmented data perform best, but this does not indicate learned CDCL reasoning."
        },
        "detailed_description": "This experiment probes whether GNNs can learn the backtracking search strategy of CDCL solvers. Two sub-experiments are conducted. First, formulas are augmented with \"learned clauses\" (up to 1000) derived from CaDiCaL's solving traces. Models are trained and tested on both original and augmented datasets. Results in Table 4 show models trained on augmented data perform near-perfectly on augmented test sets (100% accuracy for T1 on easy SR), but models trained on original data perform poorly on augmented test sets. This discrepancy indicates GNNs benefit from the structural simplification provided by learned clauses but do not learn the clause generation process itself. Second, contrastive pretraining (SimCLR) is used to pretrain GNNs so that the latent representation of an original formula is close to its augmented version. Results in Table 5 show very limited improvement (<1% in most cases), reinforcing the conclusion that GNNs struggle to learn the CDCL heuristic in their latent representations, likely due to the dynamic nature of clause learning.\n"
      },
      "exp_5": {
        "name": "Comparison with LS Heuristic (Random Initialization & Assignment Analysis)",
        "paper_section": "6.2, Table 6, Figure 5",
        "experiment_type": "main_comparison",
        "purpose": "To compare the solving behavior of GNNs with Local Search (LS) heuristics by analyzing their predictions under random initialization and across message-passing iterations.",
        "benchmarks": [
          {
            "name": "SR",
            "variant": "Easy, Medium",
            "problem_sizes": "Variables: 10-40 (Easy), 40-200 (Medium)"
          },
          {
            "name": "3-SAT",
            "variant": "Easy, Medium",
            "problem_sizes": "Variables: 10-40 (Easy), 40-200 (Medium)"
          }
        ],
        "baselines": [
          {
            "name": "NeuroSAT (LCG*) for T1",
            "description": "Trained for satisfiability prediction (NeuroSAT*).",
            "source": "Selsam et al., 2019"
          },
          {
            "name": "NeuroSAT (LCG*) for T2",
            "description": "Trained for satisfying assignment prediction with UNS2 loss.",
            "source": "Selsam et al., 2019"
          },
          {
            "name": "GGNN (VCG*) for T2",
            "description": "Trained for satisfying assignment prediction with UNS2 loss.",
            "source": "Li et al., 2016"
          }
        ],
        "conclusion": {
          "main_finding": "GNNs develop a solving strategy similar to greedy local search (like GSAT), making many variable flips initially to reduce unsatisfied clauses, but then converge and struggle to escape local minima.",
          "supporting_findings": [
            "Using random initialization (instead of learned embeddings) has minimal impact on final performance (Table 6), indicating GNNs do not rely on fixed formula representations but exploit graph structure dynamically.",
            "Analysis of predicted assignments across iterations (Figure 5) shows: (a) The number of distinct predicted assignments decreases over time. (b) The number of flipped variables between iterations drops sharply after initial steps. (c) The number of unsatisfied clauses decreases rapidly initially and then plateaus.",
            "This behavior mirrors GSAT's greedy strategy of flipping variables to minimize unsatisfied clauses, but GNNs flip many variables simultaneously and can get stuck.",
            "NeuroSAT* (trained for satisfiability) shows similar assignment-prediction behavior, indicating GNNs implicitly search for assignments even when trained only for satisfiability classification."
          ],
          "winner": "N/A - This is an analysis of behavior, not a performance comparison."
        },
        "detailed_description": "This experiment analyzes the internal solving process of GNNs to understand if it resembles local search (LS). First, models are trained and tested with random Kaiming initialization of node embeddings instead of learned initial embeddings. Results in Table 6 show negligible performance change, suggesting GNNs do not memorize fixed embeddings but learn a dynamic strategy. Second, the authors analyze the sequence of assignment predictions decoded from the latent space across message-passing iterations (T=1 to 32). For NeuroSAT* (trained for satisfiability), they use 2-clustering decoding at each step. For NeuroSAT and GGNN (trained for assignment prediction), they use multiple-prediction decoding. They track three metrics per iteration: number of distinct assignments, number of flipped variables, and number of unsatisfied clauses. Figure 5 shows all models start by generating many distinct assignments and flipping many variables, rapidly reducing unsatisfied clauses. As iterations progress, flips diminish and models converge to a specific (often unsatisfying) assignment, akin to GSAT getting stuck in a local optimum. Unlike GSAT, which flips one variable at a time and uses random restarts, GNNs flip many variables simultaneously and lack an explicit mechanism to escape local minima. The fact that NeuroSAT* exhibits this behavior shows that satisfiability-predicting GNNs also implicitly perform assignment search.\n"
      },
      "exp_6": {
        "name": "Comparison with State-of-the-Art SAT Solvers",
        "paper_section": "Appendix D.2, Table 12",
        "experiment_type": "scalability",
        "purpose": "To compare the practical solving capability and runtime of a GNN-based solver (NeuroSAT) against modern CDCL and LS SAT solvers.",
        "benchmarks": [
          {
            "name": "SR",
            "variant": "Easy, Medium",
            "problem_sizes": "Variables: 10-40 (Easy), 40-200 (Medium)"
          },
          {
            "name": "3-SAT",
            "variant": "Easy, Medium",
            "problem_sizes": "Variables: 10-40 (Easy), 40-200 (Medium)"
          },
          {
            "name": "CA",
            "variant": "Easy, Medium",
            "problem_sizes": "Variables: 10-40 (Easy), 40-200 (Medium)"
          },
          {
            "name": "PS",
            "variant": "Easy, Medium",
            "problem_sizes": "Variables: 10-40 (Easy), 40-200 (Medium)"
          },
          {
            "name": "k-Clique",
            "variant": "Easy, Medium",
            "problem_sizes": "Vertices: 5-15 (Easy), 15-20 (Medium)"
          },
          {
            "name": "k-Domset",
            "variant": "Easy, Medium",
            "problem_sizes": "Vertices: 5-15 (Easy), 15-20 (Medium)"
          },
          {
            "name": "k-Vercov",
            "variant": "Easy, Medium",
            "problem_sizes": "Vertices: 5-15 (Easy), 10-20 (Medium)"
          }
        ],
        "baselines": [
          {
            "name": "NeuroSAT",
            "description": "Trained with UNS2 loss, limited to 32 message-passing steps (implicit flips).",
            "source": "Proposed method (Selsam et al., 2019)"
          },
          {
            "name": "Sparrow*",
            "description": "State-of-the-art Local Search solver, limited to 32 flips for fair comparison.",
            "source": "Balint & FrÃ¶hlich, 2010"
          },
          {
            "name": "Sparrow",
            "description": "Unlimited version of the Sparrow LS solver.",
            "source": "Balint & FrÃ¶hlich, 2010"
          },
          {
            "name": "CaDiCaL",
            "description": "State-of-the-art CDCL solver running without constraints.",
            "source": "Fleury & Heisinger, 2020"
          }
        ],
        "conclusion": {
          "main_finding": "When constrained to a limited number of steps/flips (32), NeuroSAT often outperforms Sparrow in solving accuracy and is significantly faster, but when constraints are lifted, both Sparrow and CaDiCaL solve all satisfiable instances while NeuroSAT cannot.",
          "supporting_findings": [
            "NeuroSAT consistently and significantly outperforms Sparrow* (limited to 32 flips) in solving accuracy across all datasets.",
            "NeuroSAT is an order of magnitude faster than Sparrow* and CaDiCaL per instance (0.002-0.01s vs. 0.005-0.043s).",
            "However, unlimited Sparrow and CaDiCaL achieve 100% solving accuracy on all satisfiable instances in the benchmark, which NeuroSAT cannot match.",
            "This highlights the limited exploration capacity of GNNs observed in Exp 5, as they converge and cannot continue searching.",
            "The paper suggests GNN predictions could serve as high-quality initializations for traditional solvers to improve performance."
          ],
          "winner": "For constrained solving (32 steps/flips): NeuroSAT. For unlimited solving: Sparrow and CaDiCaL."
        },
        "detailed_description": "This experiment places NeuroSAT in direct competition with traditional SAT solvers. For a fair comparison in a constrained setting, the local search solver Sparrow is limited to 32 flips (matching NeuroSAT's 32 message-passing iterations). NeuroSAT uses its UNS2-trained model. The unlimited versions of Sparrow and the CDCL solver CaDiCaL are also run. Metrics are solving accuracy (%) and runtime per instance (seconds). NeuroSAT processes instances in batches on GPU, so per-instance runtime is calculated by dividing total time by batch size. Results in Table 12 show NeuroSAT vastly outperforms Sparrow* in accuracy (e.g., 79.79% vs 56.03% on easy SR) while being faster. However, unlimited Sparrow and CaDiCaL solve all instances, demonstrating their superior search capability. NeuroSAT's runtime is consistently lower. The experiment underscores a key trade-off: GNNs are extremely fast and can find solutions quickly with limited steps, but lack the complete search capability of traditional solvers. Their predictions may be best used to warm-start traditional solvers.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "Satisfiability Prediction Benchmarking (Identical Distribution)",
        "benchmarks": "SR_easy, 3-SAT_easy, CA_easy, PS_easy, k-Clique_easy, k-Domset_easy, k-Vercov_easy, SR_medium, 3-SAT_medium, CA_medium, PS_medium, k-Clique_medium, k-Domset_medium, k-Vercov_medium",
        "baselines": "NeuroSAT (LCG*), GCN (LCG*), GGNN (LCG*), GIN (LCG*), GCN (VCG*), GGNN (VCG*), GIN (VCG*)",
        "key_conclusion": "All GNN models perform strongly except on medium SR; graph construction choice has minor impact; NeuroSAT and GGNN perform best overall."
      },
      {
        "experiment": "Satisfying Assignment Prediction Benchmarking (Training Loss Comparison)",
        "benchmarks": "All 7 datasets (SR, 3-SAT, CA, PS, k-Clique, k-Domset, k-Vercov) at easy and medium difficulty",
        "baselines": "NeuroSAT (LCG*), GGNN (VCG*) (and others in Appendix Table 10: GCN, GGNN, GIN on both LCG*/VCG*)",
        "key_conclusion": "Unsupervised training (especially UNS2 loss) outperforms supervised training; UNS1 can be unstable."
      },
      {
        "experiment": "Unsat-core Variable Prediction Benchmarking",
        "benchmarks": "All 7 datasets (SR, 3-SAT, CA, PS, k-Clique, k-Domset, k-Vercov) at easy and medium difficulty",
        "baselines": "NeuroSAT (LCG*), GCN (LCG*), GGNN (LCG*), GIN (LCG*), GCN (VCG*), GGNN (VCG*), GIN (VCG*)",
        "key_conclusion": "All GNN models perform excellently; NeuroSAT consistently best; high accuracy is sufficient for guiding solvers."
      },
      {
        "experiment": "Comparison with CDCL Heuristic (Clause-Augmented & Contrastive Pretraining)",
        "benchmarks": "SR (easy/medium), 3-SAT (easy/medium)",
        "baselines": "NeuroSAT (LCG*), GGNN (VCG*)",
        "key_conclusion": "GNNs benefit from training on clause-augmented data but do not implicitly learn the CDCL heuristic; contrastive pretraining offers minimal gains."
      },
      {
        "experiment": "Comparison with LS Heuristic (Random Initialization & Assignment Analysis)",
        "benchmarks": "SR (easy/medium), 3-SAT (easy/medium)",
        "baselines": "NeuroSAT (LCG*), GGNN (VCG*)",
        "key_conclusion": "GNNs develop a solving strategy akin to greedy local search (GSAT), flipping many variables initially then converging, but struggle to escape local minima."
      },
      {
        "experiment": "Comparison with State-of-the-Art SAT Solvers",
        "benchmarks": "All 7 datasets at easy and medium difficulty",
        "baselines": "NeuroSAT (with UNS2 loss), Sparrow (limited flips), Sparrow (unlimited), CaDiCaL",
        "key_conclusion": "GNNs can outperform local search solvers when constrained to few flips but cannot match unlimited search-based solvers; GNN predictions could serve as good initializations."
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "SR",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3",
            "exp_4",
            "exp_5",
            "exp_6"
          ]
        },
        {
          "benchmark": "3-SAT",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3",
            "exp_4",
            "exp_5",
            "exp_6"
          ]
        },
        {
          "benchmark": "CA",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3"
          ]
        },
        {
          "benchmark": "PS",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3"
          ]
        },
        {
          "benchmark": "k-Clique",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3",
            "exp_6"
          ]
        },
        {
          "benchmark": "k-Domset",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3",
            "exp_6"
          ]
        },
        {
          "benchmark": "k-Vercov",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3",
            "exp_6"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "NeuroSAT (LCG*)",
          "compared_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3",
            "exp_4",
            "exp_5",
            "exp_6"
          ]
        },
        {
          "baseline": "GGNN (VCG*)",
          "compared_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3",
            "exp_4",
            "exp_5"
          ]
        },
        {
          "baseline": "GCN, GIN variants",
          "compared_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3"
          ]
        },
        {
          "baseline": "Traditional SAT Solvers (Sparrow, CaDiCaL)",
          "compared_in_experiments": [
            "exp_6"
          ]
        }
      ],
      "overall_narrative": "The experiments together paint a comprehensive picture of the capabilities and limitations of GNN-based SAT solvers. The core benchmarking (Exp 1-3) establishes strong performance on all three tasks (satisfiability, assignment, unsat-core prediction) within distribution, with unsupervised training (UNS2) being superior for assignment prediction. The generalization experiments reveal that performance degrades on out-of-distribution data, especially for larger sizes or structurally different problems (combinatorial vs. random). The analysis against CDCL (Exp 4) shows GNNs cannot implicitly learn backtracking search, benefiting only from the simplified formulas produced by clause learning. The comparison with LS (Exp 5) reveals GNNs develop a greedy, local-search-like strategy but lack mechanisms to escape local optima. Finally, the comparison with traditional solvers (Exp 6) confirms that while GNNs are very fast and effective for a limited number of steps, they cannot match the completeness of systematic or stochastic search when given unlimited time. The overarching conclusion is that current GNNs excel at fast, greedy approximation but struggle with the systematic exploration and backtracking required for complete SAT solving.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "G4SATBench provides a fair and comprehensive framework for evaluating GNN-based SAT solvers.",
          "supported_by": [
            "exp_1",
            "exp_2",
            "exp_3"
          ],
          "strength": "strong"
        },
        {
          "claim": "Unsupervised training objectives (especially UNS2) are more effective than supervised learning for satisfying assignment prediction.",
          "supported_by": [
            "exp_2"
          ],
          "strength": "strong"
        },
        {
          "claim": "GNN-based SAT solvers develop a solving heuristic akin to greedy local search but struggle to learn backtracking search.",
          "supported_by": [
            "exp_4",
            "exp_5"
          ],
          "strength": "strong"
        },
        {
          "claim": "GNNs show limited generalization ability, especially to larger instances and structurally different problem domains.",
          "supported_by": [
            "exp_1",
            "exp_2"
          ],
          "strength": "strong"
        },
        {
          "claim": "GNNs can outperform local search solvers under a constrained step budget but cannot compete with unlimited traditional solvers.",
          "supported_by": [
            "exp_6"
          ],
          "strength": "strong"
        }
      ],
      "limitations_acknowledged": [
        "Primarily evaluates standalone neural solvers, not neural-guided solvers that integrate with traditional solvers.",
        "Instances in G4SATBench are smaller than real-world SAT problems.",
        "Focuses on general GNNs, not domain-specific architectures (e.g., for CircuitSAT).",
        "All models are static GNNs, which may inherently limit learning of dynamic CDCL heuristics."
      ],
      "key_takeaways": [
        "Unsupervised loss (UNS2) is the recommended training objective for GNN-based assignment prediction.",
        "Current GNNs effectively learn a fast, greedy, local-search-like strategy but lack systematic search and backtracking capabilities.",
        "GNNs show promise as very fast heuristic generators or for providing high-quality initial assignments to traditional solvers.",
        "Generalization remains a significant challenge, requiring training on diverse and challenging datasets.",
        "The unsat-core variable prediction task is well-suited for GNNs, achieving high accuracy, which is useful for guiding CDCL solvers."
      ]
    }
  },
  {
    "filename": "2312.11547_A_unified_pre-training_and_adaptation_framework_fo.md",
    "paper_title": "A Unified Pre-training and Adaptation Framework for Combinatorial Optimization on Graphs",
    "paper_focus": "Proposes a unified pre-training and adaptation framework using Maximum Satisfiability (Max-SAT) to learn transferable features for solving different Combinatorial Optimization (CO) problems on graphs.",
    "experiment_inventory": {
      "total_experiments": 4,
      "main_experiments": 3,
      "ablation_studies": 1,
      "other_experiments": 1
    },
    "experiments": {
      "exp_1": {
        "name": "Main comparison: Solving Max-Cut",
        "paper_section": "4.2 Performance of Solving COs",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate the effectiveness of the proposed framework on the Max-Cut problem and answer Q1: whether Max-SAT can learn transferable features to improve CO solving.",
        "benchmarks": [
          {
            "name": "Random regular graphs",
            "variant": "Generated with different node counts and degrees",
            "problem_sizes": "n=100, 200, 500, 800, 1000; Î³ (degree) = 3, 5, 10"
          },
          {
            "name": "GSET benchmark",
            "variant": "Standard benchmark graphs",
            "problem_sizes": "G14 (V=800, E=4694), G15 (V=800, E=4661), G22 (V=2000, E=19990), G55 (V=5000, E=12468)"
          }
        ],
        "baselines": [
          {
            "name": "SDP",
            "description": "Semi-definite programming approach",
            "source": "[52]"
          },
          {
            "name": "EO",
            "description": "Extremal optimization method",
            "source": "[53]"
          },
          {
            "name": "BLS",
            "description": "Breakout local search method",
            "source": "[54]"
          },
          {
            "name": "ECO-DQN",
            "description": "Exploratory combinatorial optimization with reinforcement learning",
            "source": "[55]"
          },
          {
            "name": "GMC-A / GMC-B",
            "description": "Two unsupervised GNN architectures for Max-Cut with different loss functions",
            "source": "[56]"
          },
          {
            "name": "RUN-CSP",
            "description": "Recurrent unsupervised neural network for constraint satisfaction problems",
            "source": "[57]"
          },
          {
            "name": "PI-GNN",
            "description": "Physics-inspired GNNs for combinatorial optimization",
            "source": "[58]"
          },
          {
            "name": "MAX-SAT",
            "description": "GNN-based framework designed for Max-SAT problems",
            "source": "[59]"
          }
        ],
        "conclusion": {
          "main_finding": "The proposed framework achieves the best results among learning-based methods and is superior in most cases, especially on large-scale problems.",
          "supporting_findings": [
            "Max-SAT boosts the performance of solving COs, as evidenced by better results compared to baselines.",
            "The framework's advantages are most pronounced on large-scale problems (n=800, G22, G55) where it surpasses traditional methods.",
            "While traditional methods are competitive, the proposed method has significantly lower training and inference time."
          ],
          "winner": "Proposed framework (Ours)"
        },
        "detailed_description": "This experiment comprehensively evaluates the proposed framework on the Max-Cut problem across two benchmark types: random regular graphs and the GSET benchmark. For random graphs, mean p-values (a standardized performance metric) are reported across 1000 instances for various node counts (n=100,200,500,800,1000) and node degrees (Î³=3,5,10). The proposed method consistently achieves high p-values. On the GSET benchmark (specific graphs G14, G15, G22, G55), the metric is the number of cuts. The framework achieves 3052 (G14), 3018 (G15), 13269 (G22), and 10208 (G55) cuts, outperforming or matching all baselines. Notably, for large graphs G22 and G55, some traditional methods (EO, BLS) fail due to memory constraints, while the learning-based proposed method succeeds. The results collectively demonstrate that incorporating Max-SAT into the pre-training and adaptation pipeline successfully extracts transferable features that enhance the ability to solve Max-Cut, answering Q1 affirmatively.\n"
      },
      "exp_2": {
        "name": "Main comparison: Solving Maximum Independent Set (MIS)",
        "paper_section": "4.2 Performance of Solving COs",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate the framework's effectiveness on the Maximum Independent Set problem.",
        "benchmarks": [
          {
            "name": "frb benchmark",
            "variant": "Four instance scales",
            "problem_sizes": "frb30-15 (V=450, E=18k), frb40-19 (V=760, E=41k), frb50-23 (V=1150, E=80k), frb59-26 (V=1534, E=126k)"
          }
        ],
        "baselines": [
          {
            "name": "ReduMIS(Solver)",
            "description": "Reference solver for MIS",
            "source": "Not explicitly cited, likely a standard solver"
          },
          {
            "name": "Greedy",
            "description": "Classical greedy algorithm",
            "source": "Standard algorithm"
          },
          {
            "name": "GMC-A / GMC-B",
            "description": "Unsupervised GNNs for Max-Cut (adapted?)",
            "source": "[56]"
          },
          {
            "name": "RUN-CSP",
            "description": "Recurrent unsupervised neural network for CSP",
            "source": "[57]"
          },
          {
            "name": "PI-GNN",
            "description": "Physics-inspired GNNs",
            "source": "[58]"
          },
          {
            "name": "MAX-SAT",
            "description": "GNN-based Max-SAT framework",
            "source": "[59]"
          }
        ],
        "conclusion": {
          "main_finding": "The proposed framework achieves results that are relatively close to the solver (ReduMIS), demonstrating its effectiveness for MIS.",
          "supporting_findings": [
            "The method outperforms other learning-based baselines (GMC, RUN-CSP, PI-GNN, MAX-SAT) on most frb datasets.",
            "It shows consistent improvement over the simple Greedy algorithm."
          ],
          "winner": "Proposed framework (Ours) performs best among learning methods, close to the solver."
        },
        "detailed_description": "This experiment tests the framework on the Maximum Independent Set problem using the challenging frb benchmark, which consists of hard problem instances with increasing scale (frb30-15 to frb59-26). Performance is measured by the size of the independent set found (higher is better). The proposed method achieves scores of 27.8, 36.8, 44.8, and 52.2 across the four datasets, respectively. These results are superior to all other learning-based baselines (GMC-A/B, RUN-CSP, PI-GNN, MAX-SAT) and the Greedy algorithm. Most importantly, they are close to the results obtained by the dedicated ReduMIS solver (30.0, 39.4, 48.8, 57.4), indicating the framework's strong practical utility. This success on a different CO problem (MIS) further supports the claim that the Max-SAT-based framework learns transferable features beneficial for various COs.\n"
      },
      "exp_3": {
        "name": "Main comparison: Solving Minimum Dominated Set (MDS)",
        "paper_section": "4.2 Performance of Solving COs",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate the framework's effectiveness on the Minimum Dominated Set problem.",
        "benchmarks": [
          {
            "name": "frb benchmark",
            "variant": "Same four instance scales as MIS experiment",
            "problem_sizes": "frb30-15, frb40-19, frb50-23, frb59-26"
          }
        ],
        "baselines": [
          {
            "name": "Greedy",
            "description": "Classical greedy algorithm",
            "source": "Standard algorithm"
          },
          {
            "name": "HTS-DS",
            "description": "Heuristic method for Dominated Set",
            "source": "Not explicitly cited"
          },
          {
            "name": "MWDS-CRO",
            "description": "Another heuristic method for Dominated Set",
            "source": "Not explicitly cited"
          },
          {
            "name": "GMC-A / GMC-B",
            "description": "Unsupervised GNNs for Max-Cut",
            "source": "[56]"
          },
          {
            "name": "RUN-CSP",
            "description": "Recurrent unsupervised neural network for CSP",
            "source": "[57]"
          },
          {
            "name": "PI-GNN",
            "description": "Physics-inspired GNNs",
            "source": "[58]"
          },
          {
            "name": "MAX-SAT",
            "description": "GNN-based Max-SAT framework",
            "source": "[59]"
          }
        ],
        "conclusion": {
          "main_finding": "The proposed method achieves competitive or better results than baselines, particularly on the larger frb59-26 instance.",
          "supporting_findings": [
            "On frb59-26, the proposed method achieves the smallest dominated set size (278.1), outperforming all other methods.",
            "Performance is on par with or slightly better than strong heuristic methods (Greedy, HTS-DS, MWDS-CRO) on other instances.",
            "It consistently outperforms other learning-based methods (GMC, RUN-CSP, PI-GNN)."
          ],
          "winner": "Proposed framework (Ours), especially on the largest instance."
        },
        "detailed_description": "This experiment evaluates the framework on the Minimum Dominated Set problem, using the same frb benchmark datasets. The metric is the size of the dominated set found (lower is better). The proposed method obtains scores of 190.8, 255.1, 298.5, and 278.1. It performs comparably to strong heuristic baselines (Greedy, HTS-DS, MWDS-CRO) on the first three datasets, and achieves the best result on the largest and most complex frb59-26 dataset. It notably outperforms all other GNN-based learning methods (GMC-A/B, RUN-CSP, PI-GNN, MAX-SAT). This result on a third distinct CO problem (MDS) provides strong evidence for the generality and transferability of the framework's learned features, successfully addressing Q1.\n"
      },
      "exp_4": {
        "name": "Knowledge Transferability Analysis",
        "paper_section": "4.4 Effects of Different Knowledge Transfer Strategies",
        "experiment_type": "other",
        "purpose": "To verify the knowledge transfer ability of the model and answer Q3 regarding the impact of different source domains.",
        "benchmarks": [
          {
            "name": "MIS problem",
            "variant": "Fixed as the target domain",
            "problem_sizes": "Two scales: 450 variables, 760 variables"
          }
        ],
        "baselines": [
          {
            "name": "Knowledge transfer from Max-SAT",
            "description": "Using Max-SAT instances as source domain",
            "source": "Proposed framework configuration"
          },
          {
            "name": "Knowledge transfer from Max-Cut",
            "description": "Using Max-Cut problem as source domain",
            "source": "Proposed framework configuration"
          },
          {
            "name": "Knowledge transfer from MDS",
            "description": "Using MDS problem as source domain",
            "source": "Proposed framework configuration"
          }
        ],
        "conclusion": {
          "main_finding": "Max-SAT as the source domain yields the best transferability to the MIS target problem, demonstrating its general nature.",
          "supporting_findings": [
            "Transfer from MDS (a more relevant problem to MIS) performs better than transfer from Max-Cut (a less relevant problem).",
            "This shows that selecting an appropriate source domain matters, and irrelevant tasks can lead to negative transfer."
          ],
          "winner": "Transfer from Max-SAT domain"
        },
        "detailed_description": "This experiment investigates the core claim of transferability by fixing the target task as the MIS problem and pre-training/adapting the model using data from three different source domains: Max-SAT, Max-Cut, and MDS. Performance is visualized on heat maps for two problem scales (450 and 760 variables). The results show a clear hierarchy: transfer from Max-SAT consistently delivers the best performance (darkest color on the heat map). Transfer from MDS, which is structurally more similar to MIS (both involve node selection with constraints), performs second best. Transfer from Max-Cut, the least related problem, yields the weakest performance. This experiment directly answers Q3: different pre-training/adaptation strategies (i.e., choice of source domain) do matter. It critically validates that Max-SAT is the most general and effective bridge for knowledge transfer among different CO problems, as its logical formulation captures commonalities beyond superficial graph similarities.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "Main comparison: Solving Max-Cut",
        "benchmarks": "GSET benchmark, Random regular graphs (n=100,200,500,800,1000; Î³=3,5,10)",
        "baselines": "SDP, EO, BLS, ECO-DQN, GMC-A, GMC-B, RUN-CSP, PI-GNN, MAX-SAT",
        "key_conclusion": "The proposed framework achieves superior performance, especially on large-scale problems, demonstrating Max-SAT boosts CO solving ability."
      },
      {
        "experiment": "Main comparison: Solving Maximum Independent Set (MIS)",
        "benchmarks": "frb datasets (frb30-15, frb40-19, frb50-23, frb59-26)",
        "baselines": "ReduMIS(Solver), Greedy, GMC-A, GMC-B, RUN-CSP, PI-GNN, MAX-SAT",
        "key_conclusion": "The framework's results are closer to the solver's performance, showing effectiveness in solving MIS."
      },
      {
        "experiment": "Main comparison: Solving Minimum Dominated Set (MDS)",
        "benchmarks": "frb datasets (frb30-15, frb40-19, frb50-23, frb59-26)",
        "baselines": "Greedy, HTS-DS, MWDS-CRO, GMC-A, GMC-B, RUN-CSP, PI-GNN, MAX-SAT",
        "key_conclusion": "The proposed method achieves competitive or better results, particularly on larger frb instances."
      },
      {
        "experiment": "Ablation Study: Component Analysis",
        "benchmarks": "Random regular graphs for Max-Cut (p-values)",
        "baselines": "Ablated variants: w/o Pre-T, w/o DA, w/o Bi-G, w/o Att, w/o Mult-Att",
        "key_conclusion": "All modules (pre-training, domain adaptation, Max-SAT transfer, attention) contribute positively, with the Max-SAT transfer being most critical."
      },
      {
        "experiment": "Knowledge Transferability Analysis",
        "benchmarks": "MIS problem on two node scales (450, 760)",
        "baselines": "Transfer from different source domains: Max-SAT, Max-Cut, MDS",
        "key_conclusion": "Max-SAT has the strongest transferability to MIS; more relevant source domains (MDS) yield better transfer than less relevant ones (Max-Cut)."
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "Random regular graphs",
          "used_in_experiments": [
            "exp_1",
            "ablation_1"
          ]
        },
        {
          "benchmark": "GSET benchmark",
          "used_in_experiments": [
            "exp_1"
          ]
        },
        {
          "benchmark": "frb datasets",
          "used_in_experiments": [
            "exp_2",
            "exp_3"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "GMC-A / GMC-B",
          "compared_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3"
          ]
        },
        {
          "baseline": "RUN-CSP",
          "compared_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3"
          ]
        },
        {
          "baseline": "PI-GNN",
          "compared_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3"
          ]
        },
        {
          "baseline": "MAX-SAT (baseline method)",
          "compared_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3"
          ]
        }
      ],
      "overall_narrative": "The experiments collectively tell a compelling story of successful knowledge transfer. The main comparisons (exp_1, exp_2, exp_3) demonstrate that the proposed Max-SAT-based framework consistently outperforms or matches specialized baselines across three diverse CO problems (Max-Cut, MIS, MDS) on both synthetic and standard benchmarks. This directly validates the paper's core claim that Max-SAT can serve as a unified representation to extract transferable features. The ablation study (ablation_1) deconstructs this success, proving that each componentâ€”especially the Max-SAT transfer itselfâ€”is essential. Finally, the transferability analysis (exp_4) provides crucial insight into *why* it works: Max-SAT acts as a superior, general-purpose source domain for transfer compared to other CO problems, effectively capturing the underlying logical commonality. Together, these experiments strongly affirm that the unified pre-training and adaptation framework leverages Max-SAT to boost the ability to solve various combinatorial optimization problems on graphs.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "Max-SAT can bridge various COs on graphs and provide a unified form to capture logical information.",
          "supported_by": [
            "exp_1",
            "exp_2",
            "exp_3",
            "ablation_1"
          ],
          "strength": "strong"
        },
        {
          "claim": "The pre-training and domain adaptation framework can extract generalizable and transferable features to benefit solving different COs.",
          "supported_by": [
            "exp_1",
            "exp_2",
            "exp_3",
            "ablation_1",
            "exp_4"
          ],
          "strength": "strong"
        },
        {
          "claim": "The proposed framework is versatile and effective, boosting the ability to solve CO problems on graphs.",
          "supported_by": [
            "exp_1",
            "exp_2",
            "exp_3"
          ],
          "strength": "strong"
        }
      ],
      "limitations_acknowledged": [
        "The pre-training stage requires labeled data from a solver (supervised). The paper suggests exploring unsupervised pre-training as future work.",
        "The framework's performance, while strong, may not always exceed highly specialized traditional heuristics on every small-scale instance, though it offers speed advantages."
      ],
      "key_takeaways": [
        "Max-SAT is an effective intermediary for transferring knowledge between different combinatorial optimization problems on graphs.",
        "A two-stage pipeline of pre-training on Max-SAT followed by domain adaptation with a target CO problem yields superior performance compared to training from scratch or using other COs as the source.",
        "The proposed framework demonstrates practical value by achieving competitive results on standard benchmarks (GSET, frb) and scaling effectively to large problem instances."
      ]
    }
  },
  {
    "filename": "2403.03517_IB-Net_Initial_Branch_Network_for_Variable_Decisio.md",
    "paper_title": "IB-Net: Initial Branch Network for Variable Decision in Boolean Satisfiability",
    "paper_focus": "Proposes IB-Net, a GNN-based framework to predict UNSAT-core variables and guide CDCL SAT solvers, specifically targeting the imbalanced, predominantly unsatisfiable problems in Logic Equivalence Checking (LEC).",
    "experiment_inventory": {
      "total_experiments": 8,
      "main_experiments": 4,
      "ablation_studies": 3,
      "other_experiments": 1
    },
    "experiments": {
      "exp_1": {
        "name": "Solver Baseline Comparison",
        "paper_section": "4.2, Table 2",
        "experiment_type": "main_comparison",
        "purpose": "To select the best-performing CDCL solver for integration by comparing runtime performance of state-of-the-art solvers.",
        "benchmarks": [
          {
            "name": "LEC dataset",
            "variant": "Random 100 UNSAT samples",
            "problem_sizes": "Avg. ~1,220 variables"
          },
          {
            "name": "SAT Competition dataset",
            "variant": "Random 100 UNSAT samples",
            "problem_sizes": "Avg. ~3,481 variables"
          }
        ],
        "baselines": [
          {
            "name": "CaDiCaL",
            "description": "CDCL SAT solver",
            "source": "[Biere, 2019]"
          },
          {
            "name": "MiniSAT",
            "description": "CDCL SAT solver",
            "source": "[SÃ¶rensson and EÃ©n, 2005]"
          },
          {
            "name": "Glucose",
            "description": "CDCL SAT solver",
            "source": "[Audemard and Simon, 2018]"
          },
          {
            "name": "Kissat",
            "description": "State-of-the-art CDCL SAT solver",
            "source": "[Biere and Fleury, 2022]"
          }
        ],
        "conclusion": {
          "main_finding": "Kissat demonstrated the shortest average runtime on both datasets, establishing it as the superior solver for integration.",
          "supporting_findings": [
            "On LEC data, Kissat solved problems in 335s avg. vs. 350s (CaDiCaL), 810s (MiniSAT), 529s (Glucose).",
            "On SAT Competition data, Kissat solved problems in 195s avg. vs. 209s (CaDiCaL), 450s (MiniSAT), 308s (Glucose)."
          ],
          "winner": "Kissat"
        },
        "detailed_description": "This experiment established the performance baseline for standalone CDCL solvers on the target UNSAT problems. A random sample of 100 UNSAT instances was taken from both the industrial LEC dataset and the public SAT Competition dataset. Four state-of-the-art CDCL solvers were timed. Kissat consistently achieved the lowest average runtime (335s on LEC, 195s on SAT Comp), justifying its selection as the primary solver to be augmented by IB-Net. This choice ensures any performance gains from IB-Net are measured against a strong, modern baseline.\n"
      },
      "exp_2": {
        "name": "UNSAT-core Prediction Performance",
        "paper_section": "5.1, Table 3, Figure 3",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate the core prediction capability of IB-Net against existing neural models on both balanced and imbalanced datasets.",
        "benchmarks": [
          {
            "name": "LEC dataset",
            "variant": "Full test split (UNSAT instances)",
            "problem_sizes": "Avg. ~1,220 variables"
          },
          {
            "name": "SAT Competition dataset",
            "variant": "UNSAT subset of test split",
            "problem_sizes": "Avg. ~3,784 variables"
          }
        ],
        "baselines": [
          {
            "name": "NeuroCore",
            "description": "GNN model targeting UNSAT problems, interacts periodically with solver",
            "source": "[Selsam and BjÃ¸rner, 2019]"
          },
          {
            "name": "NeuroSAT",
            "description": "End-to-end GNN model for SAT solving",
            "source": "[Selsam et al., 2019]"
          },
          {
            "name": "NLocalSAT",
            "description": "NeuroSAT-based model for guiding SLS solvers, adapted for UNSAT-core prediction",
            "source": "[Zhang et al., 2021]"
          }
        ],
        "conclusion": {
          "main_finding": "IB-Net achieves the highest prediction accuracy and F1 scores on both datasets, with a particularly massive lead on the imbalanced LEC data.",
          "supporting_findings": [
            "On LEC data, IB-Net achieved 95% Acc, 97% Pos.F1 (UNSAT-core), and 71% Neg.F1 (non-core). Baselines struggled with non-core prediction (Neg.F1: 8-27%).",
            "On SAT Comp data, IB-Net achieved 91% Acc, 84% Pos.F1, and 93% Neg.F1, still leading all baselines.",
            "Figure 3 explains the challenge: >90% of variables are in the UNSAT-core in LEC problems, compared to ~40% in SAT Competition UNSAT problems."
          ],
          "winner": "IB-Net"
        },
        "detailed_description": "This experiment tests the fundamental prediction task that underpins IB-Net's guidance mechanism. All models were retrained/tuned to predict UNSAT-core variables. IB-Net's superior performance, especially its high Negative F1 score on the extremely imbalanced LEC data (where most variables *are* in the core), demonstrates the effectiveness of its WLIG encoding, WGCN architecture, and Focal loss. The poor performance of baselines like NeuroSAT (8% Neg.F1 on LEC) highlights their inadequacy for the specific LEC context. The strong correlation between prediction quality and later runtime improvement is established here.\n"
      },
      "exp_3": {
        "name": "End-to-End Runtime Performance (Main)",
        "paper_section": "5.1, Table 4",
        "experiment_type": "main_comparison",
        "purpose": "To measure the overall solving time improvement when integrating neural guidance into the Kissat solver, compared to the original solver and other neural approaches.",
        "benchmarks": [
          {
            "name": "LEC dataset",
            "variant": "Full test set",
            "problem_sizes": "62,481 CNFs, avg. ~1,220 vars"
          },
          {
            "name": "SAT Competition dataset",
            "variant": "Full test set",
            "problem_sizes": "3,271 CNFs, avg. ~3,481 vars"
          }
        ],
        "baselines": [
          {
            "name": "Kissat (original)",
            "description": "Standalone Kissat solver with default heuristics",
            "source": "Proposed baseline"
          },
          {
            "name": "NeuroCore",
            "description": "Kissat with periodic guidance from NeuroCore",
            "source": "[Selsam and BjÃ¸rner, 2019]"
          },
          {
            "name": "NeuroSAT",
            "description": "Kissat initialized with NeuroSAT's variable scores",
            "source": "[Selsam et al., 2019]"
          },
          {
            "name": "NLocalSAT",
            "description": "Kissat initialized with adapted NLocalSAT scores",
            "source": "[Zhang et al., 2021]"
          }
        ],
        "conclusion": {
          "main_finding": "IB-Net is the only method that consistently reduces average runtime versus the original Kissat solver on both datasets, while other methods often degrade performance.",
          "supporting_findings": [
            "On LEC: IB-Net reduced Avg. Runtime (A.RT) from 383s to 364s (5.0% improvement). Others increased A.RT (e.g., NeuroSAT to 673s) and caused thousands of extra timeouts (Halted).",
            "On SAT Comp: IB-Net reduced A.RT from 179s to 164s (8.3% improvement). Other methods showed minor improvements (2-3%) but IB-Net's was ~3x greater.",
            "IB-Net also reduced the percentage of halted (unsolved) instances on the SAT Comp dataset (34% vs. original 35.5%)."
          ],
          "winner": "IB-Net"
        },
        "detailed_description": "This is the primary end-to-end evaluation. The runtime includes graph construction, model inference, and solver execution. The results are striking: on the industrial LEC dataset, other neural methods severely harm performance (increasing runtime by up to 75% and causing 10% of problems to time out). In contrast, IB-Net provides a reliable 5% speedup. On the more balanced SAT Competition dataset, all neural methods provide some benefit, but IB-Net's 8.3% improvement is substantially larger. This confirms that IB-Net's design choices make it both robust to the harsh imbalance of LEC and generally effective on more standard benchmarks.\n"
      },
      "exp_4": {
        "name": "Runtime Performance on LEC Sub-circuits",
        "paper_section": "5.1, Table 5",
        "experiment_type": "main_comparison",
        "purpose": "To validate IB-Net's performance consistency across different specific circuits within the industrial LEC dataset.",
        "benchmarks": [
          {
            "name": "LEC Circuit 1",
            "variant": "22,050 CNFs",
            "problem_sizes": "Avg. ~1,002 variables"
          },
          {
            "name": "LEC Circuit 2",
            "variant": "14,840 CNFs",
            "problem_sizes": "Avg. ~952 variables"
          },
          {
            "name": "LEC Circuit 3",
            "variant": "25,591 CNFs",
            "problem_sizes": "Avg. ~1,563 variables"
          }
        ],
        "baselines": [
          {
            "name": "Kissat (original)",
            "description": "Standalone Kissat solver",
            "source": "Proposed baseline"
          },
          {
            "name": "NeuroCore",
            "description": "Kissat + NeuroCore guidance",
            "source": "[Selsam and BjÃ¸rner, 2019]"
          },
          {
            "name": "NeuroSAT",
            "description": "Kissat + NeuroSAT guidance",
            "source": "[Selsam et al., 2019]"
          },
          {
            "name": "NLocalSAT",
            "description": "Kissat + NLocalSAT guidance",
            "source": "[Zhang et al., 2021]"
          }
        ],
        "conclusion": {
          "main_finding": "IB-Net achieves runtime reductions across all three major circuit subsets, demonstrating its general applicability within the industrial domain.",
          "supporting_findings": [
            "Circuit 1: 4.5% improvement (442s -> 422s).",
            "Circuit 2: 4.2% improvement (335s -> 321s).",
            "Circuit 3: 5.2% improvement (360s -> 341s).",
            "All baseline methods increased runtime on every circuit, sometimes dramatically (e.g., NeuroSAT increased runtime by 89% on Circuit 2)."
          ],
          "winner": "IB-Net"
        },
        "detailed_description": "This experiment drills down into the industrial data to show IB-Net's effectiveness is not an artifact of averaging over the full dataset. Each circuit represents a different chip design with potentially distinct characteristics. IB-Net consistently provides a ~4-5% speedup on each one. The catastrophic performance of other neural methods on these circuits (e.g., near-doubling of runtime) underscores that they are not merely slightly worse but are fundamentally unsuitable for this domain. This case study strongly supports the paper's claim of practical industrial applicability.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "Solver Baseline Comparison",
        "benchmarks": "LEC dataset, SAT Competition dataset",
        "baselines": "CaDiCaL, MiniSAT, Glucose, Kissat",
        "key_conclusion": "Kissat is the fastest CDCL solver and is selected as the primary solver for integration."
      },
      {
        "experiment": "UNSAT-core Prediction Performance",
        "benchmarks": "LEC dataset, SAT Competition dataset",
        "baselines": "NeuroCore, NeuroSAT, NLocalSAT",
        "key_conclusion": "IB-Net significantly outperforms all baselines in accuracy and F1 scores for predicting UNSAT-core variables, especially on the imbalanced LEC dataset."
      },
      {
        "experiment": "End-to-End Runtime Performance (Main)",
        "benchmarks": "Full LEC dataset, SAT Competition dataset",
        "baselines": "Kissat (original), NeuroCore, NeuroSAT, NLocalSAT",
        "key_conclusion": "IB-Net reduces average runtime vs. original Kissat by 5.0% on LEC data and 8.3% on SAT Competition data; other baselines often increase runtime or cause more timeouts."
      },
      {
        "experiment": "Runtime Performance on LEC Sub-circuits",
        "benchmarks": "LEC Circuit 1, Circuit 2, Circuit 3",
        "baselines": "Kissat (original), NeuroCore, NeuroSAT, NLocalSAT",
        "key_conclusion": "IB-Net consistently achieves runtime reductions (4.2-5.2%) across all three major industrial circuits, validating its robustness."
      },
      {
        "experiment": "Ablation: Graph Construction",
        "benchmarks": "LEC dataset",
        "baselines": "Literal-Clause Graph (LCG)",
        "key_conclusion": "The proposed Weighted Literal-Incidence Graph (WLIG) outperforms the common LCG construction, especially in identifying variables outside the UNSAT-core."
      },
      {
        "experiment": "Ablation: Supervision Signal",
        "benchmarks": "LEC dataset",
        "baselines": "SAT/UNSAT prediction objective",
        "key_conclusion": "Using UNSAT-core prediction as the supervision signal is more effective for the LEC domain than predicting overall satisfiability."
      },
      {
        "experiment": "Ablation: Loss Function",
        "benchmarks": "LEC dataset",
        "baselines": "Cross-Entropy loss, KLDiv loss",
        "key_conclusion": "Focal loss outperforms standard classification losses (Cross-Entropy, KLDiv) due to its ability to handle the imbalanced class distribution in LEC problems."
      },
      {
        "experiment": "Scalability Analysis",
        "benchmarks": "Synthetic data samples of varying sizes",
        "baselines": "LSTM-based model",
        "key_conclusion": "The WGCN component of IB-Net is ~40% faster to train and uses ~70% less memory than an LSTM baseline, demonstrating superior scalability."
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "LEC Industrial Dataset",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3",
            "exp_4",
            "ablation_1",
            "ablation_2",
            "ablation_3"
          ]
        },
        {
          "benchmark": "SAT Competition Dataset",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3"
          ]
        },
        {
          "benchmark": "LEC Sub-circuits (1, 2, 3)",
          "used_in_experiments": [
            "exp_4"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "Kissat (original solver)",
          "compared_in_experiments": [
            "exp_1",
            "exp_3",
            "exp_4"
          ]
        },
        {
          "baseline": "NeuroCore",
          "compared_in_experiments": [
            "exp_2",
            "exp_3",
            "exp_4"
          ]
        },
        {
          "baseline": "NeuroSAT",
          "compared_in_experiments": [
            "exp_2",
            "exp_3",
            "exp_4"
          ]
        },
        {
          "baseline": "NLocalSAT",
          "compared_in_experiments": [
            "exp_2",
            "exp_3",
            "exp_4"
          ]
        }
      ],
      "overall_narrative": "The experiments collectively tell a story of domain-specific innovation and validation. First, the superior baseline performance of Kissat is established (exp_1). The core technical contributionâ€”IB-Net's ability to accurately predict UNSAT-core variables even under severe class imbalanceâ€”is then proven against neural baselines (exp_2). This prediction capability directly translates into the key result: reliable end-to-end runtime reduction when guiding Kissat, a feat other neural methods fail to achieve, especially on the target LEC domain (exp_3, exp_4). Three ablation studies (graph construction, supervision, loss function) systematically justify each major design choice of IB-Net, showing they are essential for its success on LEC problems. Finally, a scalability experiment (exp_5) confirms the practical efficiency of the architecture. The narrative is that IB-Net is a carefully engineered solution that addresses the unique challenges of SAT solving in LEC, outperforming both the standalone state-of-the-art solver and previous general-purpose neural approaches.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "IB-Net can effectively predict UNSAT-core variables, even in imbalanced datasets typical of LEC.",
          "supported_by": [
            "exp_2",
            "ablation_2",
            "ablation_3"
          ],
          "strength": "strong"
        },
        {
          "claim": "IB-Net accelerates the end-to-end solving time of CDCL solvers (Kissat) for UNSAT problems.",
          "supported_by": [
            "exp_3",
            "exp_4"
          ],
          "strength": "strong"
        },
        {
          "claim": "IB-Net's design choices (WLIG, UNSAT-core supervision, Focal loss) are crucial for its performance in the LEC context.",
          "supported_by": [
            "ablation_1",
            "ablation_2",
            "ablation_3"
          ],
          "strength": "strong"
        },
        {
          "claim": "The proposed method is scalable and efficient.",
          "supported_by": [
            "exp_5"
          ],
          "strength": "moderate"
        }
      ],
      "limitations_acknowledged": [
        "Experiments are limited to problems solvable by Kissat within 1000 seconds (filtered datasets).",
        "The UNSAT-core used for supervision is not guaranteed to be minimal (derived from DRAT-trim proofs).",
        "The method is specifically tailored and evaluated for UNSAT problems, particularly those in LEC."
      ],
      "key_takeaways": [
        "IB-Net achieves a 5.0% average runtime reduction on industrial LEC data and 8.3% on public SAT competition data when guiding the Kissat solver.",
        "Existing neural SAT methods (NeuroSAT, NeuroCore, NLocalSAT) degrade performance on industrial LEC problems, highlighting the need for domain-specific adaptations.",
        "The combination of WLIG graph encoding, direct UNSAT-core supervision, and Focal loss is essential for handling the severe class imbalance in LEC verification tasks.",
        "The one-time, offline guidance strategy of IB-Net is both effective and computationally lightweight compared to periodic interaction methods."
      ]
    }
  },
  {
    "filename": "2405.11024_GraSS_Combining_Graph_Neural_Networks_with_Expert_.md",
    "paper_title": "GraSS: Combining Graph Neural Networks with Expert Knowledge for SAT Solver Selection",
    "paper_focus": "Proposes GraSS, a GNN-based SAT solver selector using literal-clause graph representations with domain-specific features and positional encodings.",
    "experiment_inventory": {
      "total_experiments": 4,
      "main_experiments": 1,
      "ablation_studies": 2,
      "other_experiments": 1,
      "experiments": [
        {
          "id": "exp_1",
          "name": "Clause Ordering Sensitivity Study",
          "paper_section": "3.2.1, Figure 3",
          "type": "other"
        },
        {
          "id": "exp_2",
          "name": "Main Comparison: GraSS vs. Baselines",
          "paper_section": "4.5, Table 2",
          "type": "main_comparison"
        },
        {
          "id": "exp_3",
          "name": "Architecture Ablation Study",
          "paper_section": "4.6, Table 5",
          "type": "ablation"
        },
        {
          "id": "exp_4",
          "name": "Node Features Ablation Study",
          "paper_section": "4.6, Table 6",
          "type": "ablation"
        }
      ]
    },
    "experiments": {
      "exp_1": {
        "name": "Clause Ordering Sensitivity Study",
        "paper_section": "3.2.1, Figure 3",
        "experiment_type": "other",
        "purpose": "To investigate how solver runtime is affected by permutations of variable and clause order in SAT instances.",
        "benchmarks": [
          {
            "name": "SAT Competition instances",
            "variant": "Random sample",
            "problem_sizes": "30 instances"
          }
        ],
        "baselines": [
          {
            "name": "Kissat 3.0",
            "description": "Popular SAT solver used to measure runtime variations",
            "source": "Existing solver"
          }
        ],
        "conclusion": {
          "main_finding": "Shuffling clauses leads to large runtime variations, while shuffling variables has limited impact.",
          "supporting_findings": [
            "Runtime variations due to clause ordering can be substantial",
            "Variable ordering changes show minimal effect on runtime"
          ],
          "winner": "N/A (diagnostic study)"
        },
        "detailed_description": "This experiment aimed to understand the sensitivity of SAT solver runtime to the input ordering of clauses and variables. Thirty SAT instances were randomly sampled from SAT Competition data. For each instance, the order of variables and clauses were shuffled twenty times independently. The shuffled instances were solved by the Kissat 3.0 solver with a 5,000-second cutoff. Results showed that shuffling clauses introduced significant runtime variations (often large differences), while shuffling variables showed limited impact. This finding motivated the inclusion of positional encodings for clauses in the GraSS model to capture order-specific effects that influence solver performance.\n"
      },
      "exp_2": {
        "name": "Main Comparison: GraSS vs. Baselines",
        "paper_section": "4.5, Table 2",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate GraSS's performance against state-of-the-art SAT solver selection methods on industrial and competition benchmarks.",
        "benchmarks": [
          {
            "name": "Logic Equivalence Checking (LEC)",
            "variant": "Industrial circuit design dataset",
            "problem_sizes": "78,727 instances, variable counts 401-10,404, clauses 1,165-39,502"
          },
          {
            "name": "SAT Competition (SC)",
            "variant": "Anniversary Track subset from 2022 competition",
            "problem_sizes": "2,088 instances, grouped by best runtime quartiles"
          }
        ],
        "baselines": [
          {
            "name": "Best base solver (bulky)",
            "description": "Single best-performing solver from the portfolio on training data",
            "source": "Portfolio selection"
          },
          {
            "name": "SATzilla07",
            "description": "Ridge regression model using 33 global handcrafted features",
            "source": "Xu et al. 2008"
          },
          {
            "name": "SATzilla12",
            "description": "Random forest classifier using 55 features, pairwise comparisons",
            "source": "Xu et al. 2012"
          },
          {
            "name": "ArgoSmArT",
            "description": "k-Nearest Neighbors model with 29 features",
            "source": "NikoliÄ‡ et al. 2013"
          },
          {
            "name": "CNN",
            "description": "Convolutional neural network on ASCII-encoded CNF text as grayscale images",
            "source": "Loreggia et al. 2016"
          }
        ],
        "conclusion": {
          "main_finding": "GraSS achieves the best average runtime and highest percentage of solved instances on both benchmarks.",
          "supporting_findings": [
            "GraSS reduces average runtime compared to all baselines (LEC: 341.549s vs. 344.290s next best; SC: 220.251s vs. 222.146s next best)",
            "GraSS solves more instances within 500s cutoff (LEC: 77.7% vs. 77.2%; SC: 84.6% vs. 84.0%)",
            "GraSS has lower cost of wrong predictions when it makes errors (Figure 4)",
            "GraSS outperforms SATzilla12 on hard instances (75-100% runtime quantile) while trailing slightly on easy instances (0-25% quantile)"
          ],
          "winner": "GraSS"
        },
        "detailed_description": "This comprehensive experiment compares GraSS against five baseline methods on two distinct SAT benchmarks: an industrial Logic Equivalence Checking (LEC) dataset and a SAT Competition (SC) dataset. The evaluation uses 5-fold cross-validation, reporting average runtime, percentage of instances solved within 500 seconds, and classification accuracy (selecting the optimal solver). GraSS achieves statistically significant improvements (p<0.05) in average runtime on both benchmarks compared to the next best method (SATzilla12). Interestingly, while GraSS doesn't always have the highest classification accuracy, it demonstrates superior robustness: when it makes incorrect solver selections, the runtime penalty is substantially lower than for other methods (Figure 4). Additional analysis shows GraSS particularly excels on hard instances (those in the highest runtime quartile), where it outperforms SATzilla12 by 5.7 seconds on LEC and 15.5 seconds on SC. The method also has moderate feature computation time (Table 4): faster than SATzilla variants but slower than CNN on SC data.\n"
      },
      "exp_3": {
        "name": "Architecture Ablation Study",
        "paper_section": "4.6, Table 5",
        "experiment_type": "ablation",
        "purpose": "To evaluate the importance of the heterogeneous GNN architecture versus simpler homogeneous or NeuroSAT-style architectures.",
        "benchmarks": [
          {
            "name": "LEC benchmark",
            "variant": "Industrial circuit design dataset",
            "problem_sizes": "78,727 instances"
          }
        ],
        "baselines": [
          {
            "name": "Homogeneous GNN",
            "description": "Variant using same convolution weights for all edge types",
            "source": "Ablation variant"
          },
          {
            "name": "NeuroSAT variant",
            "description": "Adapted NeuroSAT architecture with 4 layers and modified output",
            "source": "Adapted from Selsam et al. 2019"
          },
          {
            "name": "GraSS",
            "description": "Proposed heterogeneous GNN with edge-type-specific convolutions",
            "source": "Proposed method"
          }
        ],
        "conclusion": {
          "main_finding": "GraSS's heterogeneous architecture outperforms both homogeneous and NeuroSAT-style alternatives across all metrics.",
          "supporting_findings": [
            "GraSS achieves lower average runtime (341.549s) vs. homogeneous (343.339s) and NeuroSAT variant (383.132s)",
            "GraSS solves more instances (77.7%) vs. homogeneous (77.4%) and NeuroSAT variant (74.3%)",
            "Classification accuracy is highest for GraSS (0.480) compared to alternatives"
          ],
          "winner": "GraSS (heterogeneous architecture)"
        },
        "detailed_description": "This ablation study evaluates the architectural choices in GraSS by comparing it against two alternatives on the LEC benchmark. The homogeneous variant uses the same convolution weights for all edge types (effectively treating the graph as homogeneous), while the NeuroSAT variant adapts the architecture from NeuroSAT (with 4 layers instead of 26 for tractability) and modifies the final layer for solver selection. Results show that GraSS's heterogeneous architecture, which uses edge-type-specific convolutions for clause-literal, literal-clause, and literal-literal edges, outperforms both alternatives in all three metrics. The homogeneous variant performs reasonably but worse than GraSS, indicating that modeling edge heterogeneity is beneficial. The NeuroSAT variant performs significantly worse, suggesting that architectures designed for satisfiability prediction don't transfer well to solver selection without careful adaptation.\n"
      },
      "exp_4": {
        "name": "Node Features Ablation Study",
        "paper_section": "4.6, Table 6",
        "experiment_type": "ablation",
        "purpose": "To assess the contribution of different node feature designs, particularly the custom hand-designed features and positional encodings.",
        "benchmarks": [
          {
            "name": "LEC benchmark",
            "variant": "Industrial circuit design dataset",
            "problem_sizes": "78,727 instances"
          }
        ],
        "baselines": [
          {
            "name": "Random features",
            "description": "Random normal values as initial node embeddings",
            "source": "Inspired by Selsam et al. 2019"
          },
          {
            "name": "Node-type features",
            "description": "One-hot vectors indicating clause, positive literal, or negative literal",
            "source": "Used in Li et al. 2023, Yolcu and PÃ³czos 2019"
          },
          {
            "name": "Laplacian PE",
            "description": "Laplacian positional encodings",
            "source": "Dwivedi et al. 2023"
          },
          {
            "name": "Custom features",
            "description": "Hand-designed features without positional encodings",
            "source": "Ablation variant"
          },
          {
            "name": "Custom + PE (GraSS)",
            "description": "Full GraSS features with custom features and positional encodings",
            "source": "Proposed method"
          }
        ],
        "conclusion": {
          "main_finding": "The combination of custom hand-designed features and positional encodings yields the best performance.",
          "supporting_findings": [
            "GraSS (Custom+PE) achieves best average runtime (341.549s) and solved percentage (77.7%)",
            "Custom features alone perform well but adding positional encodings provides further improvement",
            "Random features perform worst, highlighting the importance of domain knowledge",
            "Node-type features and Laplacian PE are outperformed by custom features"
          ],
          "winner": "GraSS (Custom features with positional encodings)"
        },
        "detailed_description": "This ablation study investigates the impact of different node feature representations on the LEC benchmark. Five variants are compared: random normal values (following NeuroSAT), simple node-type one-hot encodings, Laplacian positional encodings, custom hand-designed features without positional encodings, and the full GraSS features (custom + positional encodings). Results demonstrate that the custom features (inspired by SATzilla's feature engineering but adapted for node-level representation) significantly outperform generic alternatives like random or node-type features. Adding positional encodings to these custom features provides additional performance gains, validating the importance of capturing clause order information. The custom features alone already outperform Laplacian positional encodings, suggesting that domain-specific knowledge is more valuable than generic graph positional information for this task.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "Clause Ordering Sensitivity Study",
        "benchmarks": "30 random SAT instances from SAT Competition",
        "baselines": "Kissat 3.0 solver runtime comparison",
        "key_conclusion": "Clause shuffling causes large runtime variations; variable shuffling has minimal impact."
      },
      {
        "experiment": "Main Comparison: GraSS vs. Baselines",
        "benchmarks": "LEC (78,727 instances), SAT Competition (2,088 instances)",
        "baselines": "Best base solver (bulky), SATzilla07, SATzilla12, ArgoSmArT, CNN",
        "key_conclusion": "GraSS achieves best average runtime and solved percentage across both benchmarks."
      },
      {
        "experiment": "Architecture Ablation Study",
        "benchmarks": "LEC benchmark",
        "baselines": "Homogeneous GNN, NeuroSAT variant, GraSS",
        "key_conclusion": "GraSS's heterogeneous architecture outperforms homogeneous and NeuroSAT-style alternatives."
      },
      {
        "experiment": "Node Features Ablation Study",
        "benchmarks": "LEC benchmark",
        "baselines": "Random, Node-type, Laplacian PE, Custom features, Custom+PE",
        "key_conclusion": "Custom features with positional encodings (GraSS) yield best performance."
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "LEC (Logic Equivalence Checking)",
          "used_in_experiments": [
            "exp_2",
            "exp_3",
            "exp_4"
          ]
        },
        {
          "benchmark": "SAT Competition (SC)",
          "used_in_experiments": [
            "exp_1",
            "exp_2"
          ]
        },
        {
          "benchmark": "30 random SAT instances",
          "used_in_experiments": [
            "exp_1"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "SATzilla12",
          "compared_in_experiments": [
            "exp_2"
          ]
        },
        {
          "baseline": "Best base solver (bulky)",
          "compared_in_experiments": [
            "exp_2"
          ]
        },
        {
          "baseline": "Homogeneous GNN",
          "compared_in_experiments": [
            "exp_3"
          ]
        },
        {
          "baseline": "Custom features (without PE)",
          "compared_in_experiments": [
            "exp_4"
          ]
        }
      ],
      "overall_narrative": "The experiments collectively demonstrate that GraSS represents a significant advancement in SAT solver selection. The preliminary study establishes the problem motivation (clause order matters). The main experiment shows GraSS outperforms all existing methods on both industrial and competition benchmarks. The ablation studies systematically validate each key innovation: the heterogeneous GNN architecture is better than homogeneous or NeuroSAT-style alternatives, and the combination of domain-specific custom features with positional encodings is superior to generic feature representations. Together, these experiments validate the paper's core thesis that combining raw graph representations with expert domain knowledge leads to state-of-the-art performance.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "GNNs with literal-clause graphs outperform feature-based methods for SAT solver selection",
          "supported_by": [
            "exp_2"
          ],
          "strength": "strong"
        },
        {
          "claim": "Heterogeneous GNN architecture tailored to tripartite graphs is superior to homogeneous or NeuroSAT-style architectures",
          "supported_by": [
            "exp_3"
          ],
          "strength": "strong"
        },
        {
          "claim": "Domain-specific node features and positional encodings improve performance over generic feature representations",
          "supported_by": [
            "exp_1",
            "exp_4"
          ],
          "strength": "strong"
        },
        {
          "claim": "Runtime-sensitive loss function helps minimize solving time (evidenced by lower cost of wrong predictions)",
          "supported_by": [
            "exp_2"
          ],
          "strength": "moderate"
        }
      ],
      "limitations_acknowledged": [
        "Deep learning methods require large training datasets",
        "Not suitable for online learning scenarios",
        "Performance on very easy instances may be slightly worse than SATzilla12"
      ],
      "key_takeaways": [
        "GraSS achieves state-of-the-art performance on both industrial and competition SAT benchmarks",
        "The combination of graph representations with domain knowledge (features, positional encodings) is crucial",
        "When GraSS makes errors, they are less costly in runtime compared to other methods",
        "Clause order significantly affects solver runtime, justifying positional encodings"
      ]
    }
  },
  {
    "filename": "2408.15418_Understanding_GNNs_for_Boolean_Satisfiability_thro.md",
    "paper_title": "Understanding GNNs for Boolean Satisfiability through Approximation Algorithms",
    "paper_focus": "This paper proposes interpretability enhancements and performance improvements for Graph Neural Networks (GNNs) on Boolean Satisfiability (SAT) by drawing connections to Belief Propagation and Semidefinite Programming Relaxations, introducing curriculum training, embedding sampling, and a decimation procedure.",
    "experiment_inventory": {
      "total_experiments": 8,
      "experiments": [
        {
          "id": "exp_1",
          "name": "Training Convergence with Curriculum vs. Baselines",
          "paper_section": "5.3.1, Figure 3",
          "type": "main_comparison"
        },
        {
          "id": "exp_2",
          "name": "Sampling and Decimation Performance Evaluation",
          "paper_section": "5.3.2, Table 1",
          "type": "main_comparison"
        },
        {
          "id": "exp_3",
          "name": "SDP Objective Evolution on NeuroSAT Embeddings",
          "paper_section": "5.2.1, Figure 4, S.5, Figures 7,8,9",
          "type": "qualitative_analysis"
        },
        {
          "id": "exp_4",
          "name": "Training with SDP Objective vs. Classification Loss",
          "paper_section": "5.2.2, S.6",
          "type": "ablation"
        },
        {
          "id": "exp_5",
          "name": "Clustering-based Classification (Silhouette Score)",
          "paper_section": "4 (paragraph 3)",
          "type": "ablation"
        },
        {
          "id": "exp_6",
          "name": "Distance Histogram for Average True/False Vectors",
          "paper_section": "4, Figure 2",
          "type": "qualitative_analysis"
        },
        {
          "id": "exp_7",
          "name": "Multi-pass Decimation Study",
          "paper_section": "S.7, Table 2",
          "type": "scalability"
        },
        {
          "id": "exp_8",
          "name": "Comparison with Belief Propagation",
          "paper_section": "S.8, Table 3",
          "type": "main_comparison"
        }
      ]
    },
    "experiments": {
      "exp_1": {
        "name": "Training Convergence with Curriculum vs. Baselines",
        "paper_section": "5.3.1, Figure 3",
        "experiment_type": "main_comparison",
        "purpose": "To measure the impact of the proposed curriculum training procedure on training speed to reach a target accuracy.",
        "benchmarks": [
          {
            "name": "SR(40)",
            "variant": "Validation Set",
            "problem_sizes": "n=40 variables (test set)"
          }
        ],
        "baselines": [
          {
            "name": "Original NeuroSAT",
            "description": "Public implementation of the original NeuroSAT model from Selsam et al. (2019)",
            "source": "Selsam et al. [29]"
          },
          {
            "name": "Our model without curriculum",
            "description": "The authors' simplified NeuroSAT model (removed MLPs, reduced dims) trained on the full dataset from the start.",
            "source": "Proposed method variant"
          }
        ],
        "conclusion": {
          "main_finding": "The curriculum training procedure (incrementally increasing problem size and MP iterations) reduces training time by more than an order of magnitude.",
          "supporting_findings": [
            "Our model with curriculum reached 85% validation accuracy in ~30 minutes.",
            "The original NeuroSAT implementation took over 5 hours to reach the same accuracy.",
            "Our model without curriculum also took several hours, confirming the curriculum's necessity for the speedup."
          ],
          "winner": "Our model with curriculum"
        },
        "detailed_description": "This experiment compares the training efficiency of three models: the original NeuroSAT, a simplified version of NeuroSAT (the authors' model), and the simplified model trained with the proposed curriculum. All models were trained on the same data (10k formulas, variables 5-40) and evaluated on the SR(40) test set (40 variables). The curriculum procedure starts training with small formulas (size 5) and a low number of MP iterations, then incrementally increases both the problem size (by 2 variables) and the MP iterations once a validation accuracy threshold is met for the current size. The goal was to reach the 85% validation accuracy reported in the original NeuroSAT paper. The results, visualized in Figure 3, show a dramatic speedup: the curriculum model converged in about 30 minutes on an A100 GPU, while the two baselines (original NeuroSAT and the non-curriculum simplified model) required over 5 hours. This demonstrates that the curriculum, motivated by the connection to iterative approximation algorithms like SDP, is highly effective for accelerating GNN training for SAT.\n"
      },
      "exp_2": {
        "name": "Sampling and Decimation Performance Evaluation",
        "paper_section": "5.3.2, Table 1",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate the effectiveness of sampling initial embeddings and the decimation procedure in increasing the percentage of solved satisfiable problems.",
        "benchmarks": [
          {
            "name": "SR(40)",
            "variant": "Random SAT/UNSAT pairs",
            "problem_sizes": "Avg 40 variables, 5000 SAT problems"
          },
          {
            "name": "Latin Squares",
            "variant": "9x9",
            "problem_sizes": "Avg 196.9 variables, 200 SAT problems"
          },
          {
            "name": "Latin Squares",
            "variant": "8x8",
            "problem_sizes": "Avg 133.5 variables, 200 SAT problems"
          },
          {
            "name": "Logical Circuits",
            "variant": "Generated from modular inequalities",
            "problem_sizes": "Avg 131.1 variables, 344 SAT problems"
          },
          {
            "name": "Sudoku",
            "variant": "9x9",
            "problem_sizes": "Avg 245.6 variables, 200 SAT problems"
          }
        ],
        "baselines": [
          {
            "name": "Single-sample inference",
            "description": "Running the trained GNN once per formula with a single random initialization of literal embeddings.",
            "source": "Proposed method variant (no sampling)"
          },
          {
            "name": "32-sample inference (no decimation)",
            "description": "Running the GNN 32 times with different random initializations and taking the majority vote, but without applying decimation.",
            "source": "Proposed method variant"
          }
        ],
        "conclusion": {
          "main_finding": "Sampling multiple initial embeddings and applying a two-pass decimation procedure significantly increases the number of solved satisfiable problems across all benchmarks.",
          "supporting_findings": [
            "For SR(40), single-sample solved 80%, 32-sample solved 89.1%, and 16-sample+decimation solved 94%.",
            "Improvements are even more pronounced on larger, structured problems (e.g., Sudoku 9x9: 17.5% -> 51.5%).",
            "Decimation adds value beyond just more samples, as seen by comparing 32-sample (no decimation) vs. 16-sample+decimation results."
          ],
          "winner": "The proposed method combining 16-sample initialization and 2-pass decimation."
        },
        "detailed_description": "This is the core quantitative evaluation of the proposed inference enhancements. The authors test their simplified NeuroSAT model, augmented with sampling and decimation, on five different SAT benchmark sets. A \"solved\" problem requires the model to correctly predict SAT and produce a satisfying assignment. The core comparison is between three inference configurations: 1) Standard single forward pass (1 sample), 2) Majority vote over 32 independent runs with different random initial embeddings (32 samples, no decimation), and 3) The proposed method: run 16 samples, apply decimation based on distances to average true/false vectors to fix some variables, simplify the formula, and run the GNN again for a second pass (with 1 sample per decimated formula). The number of MP iterations was scaled with problem size (100 for SR(40), 1000 for structured problems). Results in Table 1 show consistent and substantial gains from both sampling and decimation. For example, on the challenging Sudoku 9x9 benchmark, performance jumped from 17.5% (single sample) to 47% (32 samples) to 51.5% (16 samples + decimation). This demonstrates that leveraging the stochastic nature of the GNN's initialization (inspired by SDP solver initialization) and a decimation procedure (inspired by Belief Propagation) greatly improves its practical utility as an incomplete SAT solver.\n"
      },
      "exp_3": {
        "name": "SDP Objective Evolution on NeuroSAT Embeddings",
        "paper_section": "5.2.1, Figure 4, Supplementary S.5, Figures 7, 8, 9",
        "experiment_type": "qualitative_analysis",
        "purpose": "To empirically investigate the connection between NeuroSAT's message-passing process and the optimization of a Semidefinite Programming (SDP) relaxation objective.",
        "benchmarks": [
          {
            "name": "Random 2-CNF formulas",
            "variant": "MAX-2-SAT SDP formulation",
            "problem_sizes": "Several hundred formulas (specific size not given)"
          }
        ],
        "baselines": [
          {
            "name": "SDP Solver",
            "description": "A standard SDP solver optimizing the MAX-2-SAT relaxation objective.",
            "source": "Standard optimization method"
          }
        ],
        "conclusion": {
          "main_finding": "The literal embeddings updated by NeuroSAT's MP iterations act as if they are optimizing an SDP-like objective function, increasing its value over time.",
          "supporting_findings": [
            "Plotting the SDP objective computed from NeuroSAT embeddings shows a monotonic increase over MP iterations (Figure 4, S.5 Figure 7).",
            "The final objective value from NeuroSAT is close to, but slightly below, the optimum found by an SDP solver (S.5 Figure 8 left).",
            "Further gradient-based optimization of the NeuroSAT-derived matrix Y closes most of this gap, with the largest changes occurring in the row corresponding to the 'true' vector y0 (S.5 Figures 8 right, 9)."
          ],
          "winner": "N/A (Qualitative analysis)"
        },
        "detailed_description": "This experiment provides qualitative evidence supporting the paper's core thesis: that GNNs for SAT learn an algorithm analogous to SDP relaxation. The authors take hundreds of random 2-CNF formulas (for which the SDP relaxation for MAX-2-SAT is well-defined). They run a trained NeuroSAT model on each formula and, after each MP iteration, construct a matrix Y from the literal embeddings (treating positive literal embeddings as vectors and estimating the 'true' vector y0 from the solution). They then compute the value of the SDP objective function Tr(WY) using the coefficient matrix W derived from the formula. The results show this objective value increases over the MP steps, mimicking an optimization process. As a reference, they compute the optimal value using an SDP solver. While NeuroSAT's final value is slightly lower, subsequent optimization shows the gap is largely due to suboptimal alignment with the y0 vector. This analysis visually and quantitatively demonstrates that NeuroSAT's internal dynamics are performing a continuous optimization akin to SDP.\n"
      },
      "exp_4": {
        "name": "Training with SDP Objective vs. Classification Loss",
        "paper_section": "5.2.2, Supplementary S.6",
        "experiment_type": "ablation",
        "purpose": "To test if directly training NeuroSAT to optimize a MAX-SAT SDP objective, rather than a binary classification loss, changes its learning behavior and dependence on curriculum.",
        "benchmarks": [
          {
            "name": "SR(40)",
            "variant": "Test set",
            "problem_sizes": "n=40 variables"
          }
        ],
        "baselines": [
          {
            "name": "NeuroSAT trained with classification loss",
            "description": "The standard NeuroSAT model trained with binary cross-entropy loss for SAT/UNSAT prediction.",
            "source": "Selsam et al. [29]"
          }
        ],
        "conclusion": {
          "main_finding": "Training with a direct MAX-SAT SDP objective leads to faster initial learning without a curriculum but achieves lower final accuracy compared to classification loss training.",
          "supporting_findings": [
            "The SDP-objective model classified only ~73% of problems correctly vs. ~85% for the classification-loss model.",
            "The SDP-objective model showed rapid improvement even when trained only on the largest (40-variable) problems, indicating less dependence on a progressive curriculum.",
            "Direct optimization of embeddings via Autograd with the SDP objective yielded even lower accuracy (~65%), showing the GNN provides a useful inductive bias."
          ],
          "winner": "Classification loss for final accuracy, SDP objective for curriculum-agnostic learning speed."
        },
        "detailed_description": "This ablation study explores an alternative training paradigm motivated by the SDP connection. Instead of training NeuroSAT with the standard binary cross-entropy loss for SAT/UNSAT classification, the authors defined a differentiable loss function based on the SDP relaxation for MAX-SAT. This loss encourages the literal embeddings (treated as unit vectors) to maximize the number of satisfied clauses. After training, a Boolean assignment is extracted by taking the sign of the inner product between each literal's embedding and a fixed 'true' vector (y0). The resulting model was evaluated on the SR(40) test set. It achieved significantly lower accuracy (~73%) than the classification-trained model (~85%). However, a key observation was that this model did not require the careful curriculum training schedule; it learned effectively even when trained from scratch on the largest problems. This suggests the MAX-SAT objective provides richer, more structured supervision than the single-bit SAT label, aligning with the intuition that the GNN is learning an optimization algorithm. The authors propose that a combined loss function might yield benefits from both approaches.\n"
      },
      "exp_5": {
        "name": "Clustering-based Classification (Silhouette Score)",
        "paper_section": "4 (paragraph 3)",
        "experiment_type": "ablation",
        "purpose": "To verify that the formation of two clusters in literal embeddings is a robust feature sufficient for classification, replacing the learned voting MLP.",
        "benchmarks": [
          {
            "name": "SR(40)",
            "variant": "Training and test sets",
            "problem_sizes": "n=40 variables"
          }
        ],
        "baselines": [
          {
            "name": "Original NeuroSAT with voting layer",
            "description": "The standard NeuroSAT architecture which uses a final MLP to produce a 'vote' from each literal embedding, averaged for classification.",
            "source": "Selsam et al. [29]"
          }
        ],
        "conclusion": {
          "main_finding": "The quality of the two-cluster structure in literal embeddings, measured by Silhouette score, is as effective for classification as the trained voting layer.",
          "supporting_findings": [
            "After training, the authors removed the final voting MLP.",
            "They ran K-means on the literal embeddings to assign them to two clusters.",
            "They computed the Silhouette score (measuring cluster separation quality) for each formula.",
            "A threshold on this score, estimated from the training set, was used to classify test formulas as SAT/UNSAT.",
            "This method achieved the same 85% accuracy as the original model with the voting layer."
          ],
          "winner": "Both methods performed equally, validating the cluster formation hypothesis."
        },
        "detailed_description": "This experiment serves as an ablation/analysis study to confirm a core observation from prior work (Selsam et al.) and this paper: that for correctly classified satisfiable formulas, the literal embeddings form two well-separated clusters. The authors test whether this structural property alone is sufficient for classification, rendering the final learned voting MLP redundant. They took their trained model, removed the final linear layer (simplified from an MLP), and for each formula, applied K-means (k=2) to the literal embeddings. They then computed the Silhouette score, a metric for cluster cohesion and separation, based on these cluster assignments. They calibrated a classification threshold on the training set's Silhouette scores. On the test set, they classified a formula as SAT if its Silhouette score was above this threshold. The result was 85% accuracy, matching the performance of the full model with the voting layer. This robustly confirms that the GNN's primary mechanism for SAT prediction is the formation of these two clusters, and the voting layer merely learns to detect this already-present geometric property.\n"
      },
      "exp_6": {
        "name": "Distance Histogram for Average True/False Vectors",
        "paper_section": "4, Figure 2",
        "experiment_type": "qualitative_analysis",
        "purpose": "To visualize and validate that literal embeddings converge to specific regions in vector space corresponding to their assigned truth value, enabling the proposed decimation procedure.",
        "benchmarks": [
          {
            "name": "Aggregated SR(40) problems",
            "variant": "Correctly classified satisfiable instances",
            "problem_sizes": "n=40 variables"
          }
        ],
        "baselines": [],
        "conclusion": {
          "main_finding": "Literal embeddings assigned 'true' are consistently close to a central 'average true vector' and far from an 'average false vector', and vice versa, providing a reliable distance-based metric for variable assignment.",
          "supporting_findings": [
            "The authors aggregated literal embeddings from many correctly solved satisfiable SR(40) problems.",
            "For literals ultimately assigned 'true', they computed distances to both the average of all 'true' embeddings and the average of all 'false' embeddings.",
            "A histogram (Figure 2) shows a clear separation: 'true' literals are close to the average true vector ('0 to 0' distance small) and far from the average false vector ('0 to 1' distance large).",
            "The symmetric pattern holds for 'false' literals."
          ],
          "winner": "N/A (Visual validation)"
        },
        "detailed_description": "This experiment provides the empirical justification for the decimation procedure introduced in the paper. The authors hypothesized that if NeuroSAT's embeddings behave like SDP vectors, there should exist a central direction representing 'truth'. They computed two aggregate vectors: the average of all literal embeddings that were assigned the value 'true' in recovered satisfying assignments, and the average of all embeddings assigned 'false'. Then, for each literal from the test problems, they plotted its Euclidean distance to these two average vectors, separated by its actual assigned truth value. Figure 2 shows a striking bimodal distribution: literals assigned 'true' have small distances to the average true vector and large distances to the average false vector (and symmetrically for 'false' literals). This clear separation demonstrates that the embedding space develops a meaningful geometry where truth value is encoded as proximity to a specific point. This finding directly motivates the decimation heuristic: variables whose embeddings are very close to one of these average vectors can be confidently fixed, simplifying the problem.\n"
      },
      "exp_7": {
        "name": "Multi-pass Decimation Study",
        "paper_section": "Supplementary S.7, Table 2",
        "experiment_type": "scalability",
        "purpose": "To investigate the marginal utility of applying the decimation procedure more than two times.",
        "benchmarks": [
          {
            "name": "SR(40)",
            "variant": "Random SAT/UNSAT pairs",
            "problem_sizes": "5000 SAT problems"
          },
          {
            "name": "Latin Squares 9x9",
            "variant": "",
            "problem_sizes": "200 SAT problems"
          },
          {
            "name": "Latin Squares 8x8",
            "variant": "",
            "problem_sizes": "200 SAT problems"
          },
          {
            "name": "Logical Circuits",
            "variant": "",
            "problem_sizes": "344 SAT problems"
          },
          {
            "name": "Sudoku 9x9",
            "variant": "",
            "problem_sizes": "200 SAT problems"
          }
        ],
        "baselines": [
          {
            "name": "Two-pass decimation",
            "description": "The primary decimation procedure reported in the main paper (first pass with 16 samples, second pass with 1 sample).",
            "source": "Proposed method (main configuration)"
          }
        ],
        "conclusion": {
          "main_finding": "Applying a third decimation pass yields minimal additional gains, indicating that most solvable problems are solved within two passes.",
          "supporting_findings": [
            "On SR(40), a third pass solved an additional 35 problems (0.7% of the set) beyond the two-pass result.",
            "Gains on structured problems were even smaller (2% max for Latin Squares 9x9, 0% for Logical Circuits).",
            "This suggests the decimation threshold or procedure is not optimized for deeper recursion, or that benefits plateau quickly."
          ],
          "winner": "Two-pass decimation (as it captures nearly all benefit)"
        },
        "detailed_description": "This experiment extends the main decimation evaluation to see if further recursive simplification could solve more problems. Using the same setup as the main experiment (16 samples in first pass, 1 sample in subsequent passes), the authors applied the decimation procedure a third time to formulas not solved after two passes. The results, shown in Supplementary Table 2, indicate sharply diminishing returns. The third pass provided only a tiny incremental improvement (e.g., 0.7% on SR(40)). This implies that either the problems remaining unsolved after two passes are inherently harder for this method, or the fixed decimation threshold (based on distance to average vectors) becomes less effective after the first round of variable fixing. The authors note they did not try to optimize the decimation threshold, which might improve multi-pass performance. However, the primary conclusion is that the two-pass procedure presented in the main paper captures the vast majority of the potential benefit from this iterative simplification approach.\n"
      },
      "exp_8": {
        "name": "Comparison with Belief Propagation",
        "paper_section": "Supplementary S.8, Table 3",
        "experiment_type": "main_comparison",
        "purpose": "To compare the base performance of the NeuroSAT GNN (without sampling/decimation enhancements) against a classical message-passing algorithm, Belief Propagation (BP), on the same benchmark sets.",
        "benchmarks": [
          {
            "name": "SR(40)",
            "variant": "Random SAT/UNSAT pairs",
            "problem_sizes": "5000 SAT problems"
          },
          {
            "name": "Latin Squares 9x9",
            "variant": "",
            "problem_sizes": "200 SAT problems"
          },
          {
            "name": "Latin Squares 8x8",
            "variant": "",
            "problem_sizes": "200 SAT problems"
          },
          {
            "name": "Logical Circuits",
            "variant": "",
            "problem_sizes": "344 SAT problems"
          },
          {
            "name": "Sudoku 9x9",
            "variant": "",
            "problem_sizes": "200 SAT problems"
          }
        ],
        "baselines": [
          {
            "name": "Belief Propagation (BP)",
            "description": "The standard Belief Propagation algorithm for SAT, run for a maximum of 1000 iterations.",
            "source": "Classical approximation algorithm"
          }
        ],
        "conclusion": {
          "main_finding": "The NeuroSAT GNN significantly outperforms standard Belief Propagation across all tested problem types, especially on structured problems.",
          "supporting_findings": [
            "On SR(40), NeuroSAT solved 80% vs. BP's 42.34%.",
            "On Logical Circuits, NeuroSAT solved 85.2% vs. BP's mere 0.5%.",
            "BP performed relatively best on Latin Squares 8x8 (32.5%) but was still far behind NeuroSAT (56.5%).",
            "This demonstrates the advantage of the learned message-passing functions in NeuroSAT over the hand-designed BP update rules."
          ],
          "winner": "NeuroSAT (without enhancements)"
        },
        "detailed_description": "This experiment provides a baseline comparison against a classical algorithm related to one of the paper's proposed inspirations (Belief Propagation). The authors run the standard Belief Propagation algorithm (with decimation) on the same set of SAT problems used to evaluate NeuroSAT. NeuroSAT is tested in its basic, single-sample inference mode (no multiple initializations, no decimation). The results, shown in Supplementary Table 3, show a decisive advantage for the learned GNN approach. NeuroSAT's performance is nearly double that of BP on random problems (80% vs. 42%) and vastly superior on structured problems like Logical Circuits (85.2% vs. 0.5%) and Sudoku (17.5% vs. 6%). This highlights that while BP provides conceptual inspiration, the data-driven, learned message-passing functions in NeuroSAT capture a more powerful algorithm for these problem distributions, particularly those with structure.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "Training Convergence with Curriculum vs. Baselines",
        "benchmarks": "SR(40) Validation Set",
        "baselines": "Original NeuroSAT implementation, Our model without curriculum",
        "key_conclusion": "Curriculum training reduces training time to reach 85% validation accuracy from >5 hours to ~30 minutes."
      },
      {
        "experiment": "Sampling and Decimation Performance Evaluation",
        "benchmarks": "SR(40), Latin Squares 9x9, Latin Squares 8x8, Logical Circuits, Sudoku 9x9",
        "baselines": "Single-sample inference (no sampling/decimation), 32-sample inference (no decimation)",
        "key_conclusion": "Sampling (16 init.) + decimation (2 passes) solves significantly more SAT problems than baselines across all benchmark types."
      },
      {
        "experiment": "SDP Objective Evolution on NeuroSAT Embeddings",
        "benchmarks": "Hundreds of random 2-CNF formulas",
        "baselines": "SDP Solver objective value",
        "key_conclusion": "NeuroSAT's MP process increases an SDP-like objective function, showing its internal optimization resembles SDP relaxation."
      },
      {
        "experiment": "Training with SDP Objective vs. Classification Loss",
        "benchmarks": "SR(40)",
        "baselines": "NeuroSAT trained with classification loss",
        "key_conclusion": "Model trained with MAX-SAT SDP objective reaches lower final accuracy (~73%) but trains faster without a curriculum."
      },
      {
        "experiment": "Clustering-based Classification (Silhouette Score)",
        "benchmarks": "SR(40) training and test sets",
        "baselines": "Original NeuroSAT with voting layer",
        "key_conclusion": "Using K-means clustering and Silhouette score on literal embeddings achieves the same accuracy (85%) as the original voting layer, confirming cluster formation is robust."
      },
      {
        "experiment": "Distance Histogram for Average True/False Vectors",
        "benchmarks": "Aggregated embeddings from correctly classified satisfiable SR(40) problems",
        "baselines": "N/A",
        "key_conclusion": "Literal embeddings assigned 'true' cluster near an average 'true' vector and far from an average 'false' vector, validating the SDP analogy for assignment."
      },
      {
        "experiment": "Multi-pass Decimation Study",
        "benchmarks": "SR(40), Latin Squares 9x9, Latin Squares 8x8, Logical Circuits, Sudoku 9x9",
        "baselines": "First-pass decimation results",
        "key_conclusion": "A third decimation pass provides negligible additional solved problems (~0.7% max gain), suggesting diminishing returns after two passes."
      },
      {
        "experiment": "Comparison with Belief Propagation",
        "benchmarks": "SR(40), Latin Squares 9x9, Latin Squares 8x8, Logical Circuits, Sudoku 9x9",
        "baselines": "Belief Propagation (BP) algorithm",
        "key_conclusion": "NeuroSAT (without enhancements) significantly outperforms BP across all structured and random problem types."
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "SR(40)",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_4",
            "exp_5",
            "exp_6",
            "exp_7",
            "exp_8"
          ]
        },
        {
          "benchmark": "Latin Squares 9x9",
          "used_in_experiments": [
            "exp_2",
            "exp_7",
            "exp_8"
          ]
        },
        {
          "benchmark": "Latin Squares 8x8",
          "used_in_experiments": [
            "exp_2",
            "exp_7",
            "exp_8"
          ]
        },
        {
          "benchmark": "Logical Circuits",
          "used_in_experiments": [
            "exp_2",
            "exp_7",
            "exp_8"
          ]
        },
        {
          "benchmark": "Sudoku 9x9",
          "used_in_experiments": [
            "exp_2",
            "exp_7",
            "exp_8"
          ]
        },
        {
          "benchmark": "Random 2-CNF formulas",
          "used_in_experiments": [
            "exp_3"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "Original NeuroSAT",
          "compared_in_experiments": [
            "exp_1"
          ]
        },
        {
          "baseline": "Our model without curriculum",
          "compared_in_experiments": [
            "exp_1"
          ]
        },
        {
          "baseline": "Single-sample inference",
          "compared_in_experiments": [
            "exp_2"
          ]
        },
        {
          "baseline": "32-sample inference (no decimation)",
          "compared_in_experiments": [
            "exp_2"
          ]
        },
        {
          "baseline": "SDP Solver",
          "compared_in_experiments": [
            "exp_3"
          ]
        },
        {
          "baseline": "NeuroSAT trained with classification loss",
          "compared_in_experiments": [
            "exp_4"
          ]
        },
        {
          "baseline": "Original NeuroSAT with voting layer",
          "compared_in_experiments": [
            "exp_5"
          ]
        },
        {
          "baseline": "Two-pass decimation",
          "compared_in_experiments": [
            "exp_7"
          ]
        },
        {
          "baseline": "Belief Propagation (BP)",
          "compared_in_experiments": [
            "exp_8"
          ]
        }
      ],
      "overall_narrative": "The experiments collectively tell a story of grounding and improving a GNN model through algorithmic insights. First, qualitative experiments (exp_3, exp_6) establish the core connection: NeuroSAT's internal dynamics resemble the optimization of an SDP relaxation, and its embeddings converge to geometrically meaningful clusters. This understanding directly inspires the improvements. The curriculum training (exp_1), analogous to gradually increasing iterations in iterative solvers, dramatically speeds up training. The stochastic nature of SDP solvers inspires sampling multiple initializations (exp_2), and the geometric cluster analysis inspires the decimation procedure (exp_2, exp_7), together greatly boosting solved problems. An ablation (exp_4) shows that an SDP-inspired loss changes learning dynamics, and another (exp_5) confirms the centrality of cluster formation. Finally, a comparison to BP (exp_8) shows the learned model surpasses its classical inspiration. All experiments consistently use the same core simplified NeuroSAT model and benchmarks, providing a coherent validation of the paper's thesis that interpreting GNNs through approximation algorithms leads to practical enhancements.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "The message-passing process of a GNN for SAT learns an algorithm analogous to optimizing an SDP relaxation.",
          "supported_by": [
            "exp_3",
            "exp_6"
          ],
          "strength": "strong"
        },
        {
          "claim": "A curriculum training procedure, motivated by iterative solvers, significantly accelerates GNN training.",
          "supported_by": [
            "exp_1"
          ],
          "strength": "strong"
        },
        {
          "claim": "Sampling initial embeddings and applying a decimation procedure, inspired by BP and SDP rounding, substantially improves the number of solved problems.",
          "supported_by": [
            "exp_2",
            "exp_7"
          ],
          "strength": "strong"
        },
        {
          "claim": "The formation of two clusters in literal embeddings is the primary mechanism for SAT prediction and assignment recovery.",
          "supported_by": [
            "exp_5",
            "exp_6"
          ],
          "strength": "strong"
        }
      ],
      "limitations_acknowledged": [
        "Experiments are conducted on relatively small problems (up to ~250 variables) and generated benchmarks, not 'real-world' SAT instances.",
        "The model cannot certify unsatisfiability.",
        "The precise mechanism of optimization within the GNN's MP steps is not fully theoretically characterized (empirical focus)."
      ],
      "key_takeaways": [
        "Interpreting GNNs through the lens of classical approximation algorithms (SDP, BP) provides actionable insights for improving their training and inference.",
        "Curriculum training based on problem size and MP iteration count can reduce training time by an order of magnitude.",
        "Leveraging the stochasticity of initialization (sampling) and iterative simplification (decimation) can dramatically boost the performance of a trained GNN as an incomplete solver.",
        "The internal representations of the GNN develop a clear geometric structure (clusters aligned with truth values) that can be directly exploited."
      ]
    }
  },
  {
    "filename": "2504.01173_Neural_Approaches_to_SAT_Solving_Design_Choices_an.md",
    "paper_title": "Neural Approaches to SAT Solving: Design Choices and Interpretability",
    "paper_focus": "The paper provides a comprehensive evaluation of graph neural networks for Boolean satisfiability problems, introducing training improvements and analyzing model interpretability.",
    "experiment_inventory": {
      "total_experiments": 5,
      "main_experiments": 3,
      "ablation_studies": 0,
      "other_experiments": 2
    },
    "experiments": {
      "exp_1": {
        "name": "Comparison of Graph Representations, Update Functions, and Training Methods",
        "paper_section": "5.2.1",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate the impact of different graph representations (LCG vs VCG), update functions (RNN vs LSTM), and training objectives on SAT solving performance.",
        "benchmarks": [
          {
            "name": "SR40",
            "variant": "Exactly 40 variables",
            "problem_sizes": "40 variables, average 228.40 clauses"
          }
        ],
        "baselines": [
          {
            "name": "LCG+RNN",
            "description": "Literal-Clause Graph with RNN update function",
            "source": "proposed method"
          },
          {
            "name": "LCG+LSTM",
            "description": "Literal-Clause Graph with LSTM update function",
            "source": "proposed method"
          },
          {
            "name": "VCG+RNN",
            "description": "Variable-Clause Graph with RNN update function",
            "source": "proposed method"
          },
          {
            "name": "VCG+LSTM",
            "description": "Variable-Clause Graph with LSTM update function",
            "source": "proposed method"
          },
          {
            "name": "SAT/UNSAT classification training",
            "description": "Training to predict satisfiability status using binary cross-entropy",
            "source": "NeuroSAT (Selsam et al., 2018)"
          },
          {
            "name": "Assignment prediction training",
            "description": "Training to predict variable assignments using cross-entropy or MSE loss",
            "source": "proposed method"
          },
          {
            "name": "Unsupervised training",
            "description": "Training using clause validity loss without assignment labels",
            "source": "Ozolins et al., 2022"
          }
        ],
        "conclusion": {
          "main_finding": "VCG representation with RNN updates and assignment prediction training achieves the best balance of SAT accuracy (68.8%) and decision accuracy (84.4%).",
          "supporting_findings": [
            "Unsupervised training achieves the lowest average gaps (as low as 0.81 for LCG+LSTM).",
            "RNN-based models are more interpretable and computationally efficient than LSTM.",
            "SAT/UNSAT classification training enables implicit separation of variable embeddings, allowing assignment recovery via clustering."
          ],
          "winner": "VCG+RNN with assignment prediction"
        },
        "detailed_description": "This experiment comprehensively compares architectural choices for GNN-based SAT solving on the SR40 benchmark. Models are trained with three different objectives: SAT/UNSAT classification, assignment prediction, and unsupervised learning. The Literal-Clause Graph (LCG) and Variable-Clause Graph (VCG) representations are tested with both RNN and LSTM update functions. Results show that VCG+RNN with assignment prediction achieves the highest SAT accuracy (68.8%) and decision accuracy (84.4%), while unsupervised training consistently yields the lowest average gaps (unsatisfied clauses). The VCG representation is more computationally efficient due to fewer nodes (n variables vs 2n literals in LCG). RNN updates offer better interpretability and performance for assignment prediction, though LSTM shows advantages for unsupervised training. Interestingly, models trained only for SAT/UNSAT classification develop separable embeddings that can be clustered to recover assignments, despite not being explicitly trained for this task.\n"
      },
      "exp_2": {
        "name": "Test-time Scaling (Iteration and Resampling Effects)",
        "paper_section": "5.3",
        "experiment_type": "scalability",
        "purpose": "To evaluate how increasing message-passing iterations and resampling at inference time improves performance on various benchmarks.",
        "benchmarks": [
          {
            "name": "SR40",
            "variant": "Exactly 40 variables",
            "problem_sizes": "40 variables"
          },
          {
            "name": "SR100",
            "variant": "Exactly 100 variables",
            "problem_sizes": "100 variables"
          },
          {
            "name": "SR200",
            "variant": "Exactly 200 variables",
            "problem_sizes": "200 variables"
          },
          {
            "name": "SR400",
            "variant": "Exactly 400 variables",
            "problem_sizes": "400 variables"
          },
          {
            "name": "3SAT100",
            "variant": "Exactly 100 variables at phase transition",
            "problem_sizes": "100 variables"
          },
          {
            "name": "3SAT200",
            "variant": "Exactly 200 variables at phase transition",
            "problem_sizes": "200 variables"
          }
        ],
        "baselines": [
          {
            "name": "Model trained on SR40 (VCG+RNN, closest assignment supervision)",
            "description": "Base model trained on SR40 distribution, evaluated with varying iterations and resampling",
            "source": "proposed method"
          },
          {
            "name": "Model trained on SR100 (VCG+RNN, closest assignment supervision)",
            "description": "Base model trained on SR100 distribution, evaluated with varying iterations",
            "source": "proposed method"
          }
        ],
        "conclusion": {
          "main_finding": "Increasing message-passing iterations and resampling attempts consistently improves performance across all benchmarks, demonstrating the recurrent architecture's inference-time scalability.",
          "supporting_findings": [
            "On SR40, decision accuracy improves from 84% (1 sample, 25 iterations) to 93% (5 samples, 125 iterations).",
            "The model trained on SR40 generalizes reasonably to larger instances (e.g., 74.2% decision accuracy on SR100).",
            "Training on larger instances (SR100) significantly improves performance on even larger benchmarks (e.g., 36.5% SAT accuracy on SR400 vs 3% for SR40-trained model)."
          ],
          "winner": "Model with maximum iterations (125) and resampling (5 samples)"
        },
        "detailed_description": "This experiment investigates the test-time scalability of the recurrent GNN architecture. Models trained on SR40 and SR100 (with VCG+RNN and closest assignment supervision) are evaluated on six benchmarks (SR40, SR100, SR200, SR400, 3SAT100, 3SAT200) with varying message-passing iterations (25-125) and resampling attempts (1-5). Results show consistent improvement with increased computation: on SR40, decision accuracy improves from 84% (1 sample, 25 iterations) to 93% (5 samples, 125 iterations). The model trained on SR40 demonstrates reasonable generalization to larger instances (74.2% decision accuracy on SR100), though performance degrades on much larger problems (58.5% on SR200). Training on larger instances (SR100) yields significantly better scaling, achieving 36.5% SAT accuracy on SR400 compared to 3% for the SR40-trained model. The experiment highlights the flexible computation-performance tradeoff enabled by recurrent weight sharing, allowing adaptive inference based on available resources.\n"
      },
      "exp_3": {
        "name": "Diffusion Model Extension",
        "paper_section": "5.4",
        "experiment_type": "main_comparison",
        "purpose": "To extend the GNN into a diffusion model for incremental assignment sampling and evaluate its performance with different step configurations and unit propagation integration.",
        "benchmarks": [
          {
            "name": "SR100",
            "variant": "Exactly 100 variables",
            "problem_sizes": "100 variables"
          }
        ],
        "baselines": [
          {
            "name": "Diffusion model with varying GNN steps and diffusion steps",
            "description": "GNN-based diffusion model predicting original assignment from noisy input, with configurable steps",
            "source": "proposed method (adapted from Sun and Yang, 2023)"
          },
          {
            "name": "Diffusion model interleaved with unit propagation",
            "description": "Diffusion model combined with classical unit propagation for problem simplification",
            "source": "proposed method"
          },
          {
            "name": "Single-step assignment prediction",
            "description": "Base GNN model for assignment prediction (equivalent to diffusion model with one step)",
            "source": "proposed method"
          }
        ],
        "conclusion": {
          "main_finding": "Increasing GNN steps is more important than diffusion steps for performance; interleaving diffusion with unit propagation improves decision accuracy by approximately 10%.",
          "supporting_findings": [
            "Timestep conditioning in the diffusion model is unnecessary; removing it simplifies the model.",
            "Performance plateaus beyond 50 GNN steps and 8 diffusion steps.",
            "Unit propagation reduces computational cost on harder problems by early conflict detection."
          ],
          "winner": "Diffusion model interleaved with unit propagation"
        },
        "detailed_description": "This experiment extends the GNN to a categorical diffusion model for incremental assignment generation. The model is trained to predict the original assignment from a noisy version, conditioned on the CNF graph. On SR100 benchmark, different configurations of GNN steps (message-passing iterations per diffusion step) and diffusion steps (denoising iterations) are evaluated, keeping total computation roughly constant (~300 iterations). Results show that increasing GNN steps improves performance more than increasing diffusion steps. For example, with 50 GNN steps and 6 diffusion steps, average gap is 0.94 and decision accuracy 76.2%. A key finding is that timestep conditioning (standard in diffusion models) is unnecessary, simplifying the architecture. The diffusion model is further enhanced by interleaving with unit propagation: at each step, variable beliefs above a threshold are fixed, and unit propagation simplifies the formula. This hybrid approach improves decision accuracy by ~10% across problem sizes (e.g., from 88.4% to 94.2% on SR40). Interestingly, computational cost with unit propagation is lower on harder problems due to early conflict pruning.\n"
      },
      "exp_4": {
        "name": "Impact of Hidden Dimension on GNN Performance",
        "paper_section": "A.1.4",
        "experiment_type": "ablation",
        "purpose": "To evaluate how the hidden dimension size (d_model) affects model performance and training efficiency.",
        "benchmarks": [
          {
            "name": "Implied SR40",
            "variant": "Not explicitly stated, but context suggests SR40",
            "problem_sizes": "40 variables"
          }
        ],
        "baselines": [
          {
            "name": "VCG+RNN model with hidden dimension 16",
            "description": "Base model with 16-dimensional embeddings",
            "source": "proposed method"
          },
          {
            "name": "VCG+RNN model with hidden dimension 32",
            "description": "Base model with 32-dimensional embeddings",
            "source": "proposed method"
          },
          {
            "name": "VCG+RNN model with hidden dimension 48",
            "description": "Base model with 48-dimensional embeddings",
            "source": "proposed method"
          },
          {
            "name": "VCG+RNN model with hidden dimension 64",
            "description": "Base model with 64-dimensional embeddings",
            "source": "proposed method"
          },
          {
            "name": "VCG+RNN model with hidden dimension 96",
            "description": "Base model with 96-dimensional embeddings",
            "source": "proposed method"
          },
          {
            "name": "VCG+RNN model with hidden dimension 128",
            "description": "Base model with 128-dimensional embeddings",
            "source": "proposed method"
          },
          {
            "name": "VCG+RNN model with hidden dimension 256",
            "description": "Base model with 256-dimensional embeddings",
            "source": "proposed method"
          }
        ],
        "conclusion": {
          "main_finding": "Hidden dimension 64 provides the best trade-off between accuracy (86.9%) and training time (1.87 hours), with diminishing returns beyond this size.",
          "supporting_findings": [
            "Accuracy improves from 78.2% (d=16) to 86.9% (d=64), then plateaus (87.7% at d=256).",
            "Training time increases significantly with dimension (1.62 hours for d=16 to 7.55 hours for d=256)."
          ],
          "winner": "Hidden dimension 64"
        },
        "detailed_description": "This ablation study evaluates the effect of hidden dimension (embedding size) on model performance and training efficiency. The VCG+RNN model is trained with dimensions ranging from 16 to 256 on the SR40 benchmark (implied from context). Accuracy improves from 78.2% (d=16) to 86.9% (d=64), with diminishing returns beyond this point (87.7% at d=256). However, training time increases substantially: 1.62 hours for d=16 vs 7.55 hours for d=256. The results indicate that a hidden dimension of 64 offers the best balance, achieving near-peak accuracy without excessive computational cost. This dimension was consequently used in most experiments throughout the paper.\n"
      },
      "exp_5": {
        "name": "Influence of Number of Message-passing and Diffusion Steps (Fixed Variable Analysis)",
        "paper_section": "A.4",
        "experiment_type": "ablation",
        "purpose": "To separately analyze the impact of GNN steps and diffusion steps on performance when the other variable is fixed.",
        "benchmarks": [
          {
            "name": "SR100",
            "variant": "Exactly 100 variables",
            "problem_sizes": "100 variables"
          }
        ],
        "baselines": [
          {
            "name": "Fixed diffusion steps (10) with varying GNN steps (10-100)",
            "description": "Diffusion model with 10 diffusion steps and GNN steps ranging from 10 to 100",
            "source": "proposed method"
          },
          {
            "name": "Fixed GNN steps (25) with varying diffusion steps (4-50)",
            "description": "Diffusion model with 25 GNN steps and diffusion steps ranging from 4 to 50",
            "source": "proposed method"
          }
        ],
        "conclusion": {
          "main_finding": "Performance plateaus beyond 50 GNN steps (when diffusion steps fixed at 10) and beyond 8 diffusion steps (when GNN steps fixed at 25), indicating diminishing returns.",
          "supporting_findings": [
            "With fixed diffusion steps=10, accuracy improves from 55.0% (10 GNN steps) to 78.6% (70 GNN steps), then stabilizes.",
            "With fixed GNN steps=25, accuracy improves from 69.9% (4 diffusion steps) to 73.3% (20 diffusion steps), then fluctuates slightly."
          ],
          "winner": "Configuration with 70 GNN steps and 10 diffusion steps (or similar)"
        },
        "detailed_description": "This experiment provides a finer-grained analysis of the diffusion model's step configuration. Two settings are tested on SR100: (1) fixing diffusion steps at 10 and varying GNN steps (10-100), and (2) fixing GNN steps at 25 and varying diffusion steps (4-50). In the first setting, accuracy improves from 55.0% (10 GNN steps) to 78.6% (70 GNN steps), then plateaus. In the second setting, accuracy improves from 69.9% (4 diffusion steps) to 73.3% (20 diffusion steps), with minimal gains beyond 8 diffusion steps. The results confirm that GNN steps (message-passing iterations) are more critical for performance than diffusion steps, and that increasing either beyond a threshold yields diminishing returns. This informs practical configuration choices for balancing performance and computational cost.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "Comparison of Graph Representations, Update Functions, and Training Methods",
        "benchmarks": "SR40",
        "baselines": "LCG+RNN, LCG+LSTM, VCG+RNN, VCG+LSTM with SAT/UNSAT classification, assignment prediction, unsupervised training",
        "key_conclusion": "VCG+RNN with assignment prediction achieves the best SAT accuracy (68.8%) and decision accuracy (84.4%)."
      },
      {
        "experiment": "Test-time Scaling (Iteration and Resampling Effects)",
        "benchmarks": "SR40, SR100, SR200, SR400, 3SAT100, 3SAT200",
        "baselines": "Models trained on SR40 and SR100 with varying message-passing iterations (25-125) and resampling attempts (1-5)",
        "key_conclusion": "Increasing iterations and resampling consistently improves performance, with decision accuracy improving from 84% (1 sample, 25 iterations) to 93% (5 samples, 125 iterations) on SR40."
      },
      {
        "experiment": "Diffusion Model Extension",
        "benchmarks": "SR100",
        "baselines": "Diffusion model with varying GNN steps (20-50) and diffusion steps (6-15), and diffusion model interleaved with unit propagation",
        "key_conclusion": "Increasing GNN steps is more important than diffusion steps for performance; interleaving with unit propagation improves accuracy by ~10%."
      },
      {
        "experiment": "Impact of Hidden Dimension on GNN Performance",
        "benchmarks": "Implied SR40 (from context)",
        "baselines": "VCG+RNN model with hidden dimensions 16, 32, 48, 64, 96, 128, 256",
        "key_conclusion": "Hidden dimension 64 provides the best balance between performance (86.9% accuracy) and training efficiency (1.87 hours)."
      },
      {
        "experiment": "Influence of Number of Message-passing and Diffusion Steps (Fixed Variable Analysis)",
        "benchmarks": "SR100",
        "baselines": "Fixed diffusion steps (10) with varying GNN steps (10-100); fixed GNN steps (25) with varying diffusion steps (4-50)",
        "key_conclusion": "Performance plateaus beyond 50 GNN steps and 8 diffusion steps, indicating diminishing returns."
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "SR40",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3 (unit propagation)",
            "closest_assignment_ablation"
          ]
        },
        {
          "benchmark": "SR100",
          "used_in_experiments": [
            "exp_2",
            "exp_3",
            "exp_5",
            "closest_assignment_ablation"
          ]
        },
        {
          "benchmark": "SR200",
          "used_in_experiments": [
            "exp_2"
          ]
        },
        {
          "benchmark": "SR400",
          "used_in_experiments": [
            "exp_2"
          ]
        },
        {
          "benchmark": "3SAT100",
          "used_in_experiments": [
            "exp_2",
            "closest_assignment_ablation"
          ]
        },
        {
          "benchmark": "3SAT200",
          "used_in_experiments": [
            "exp_2"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "VCG+RNN with assignment prediction",
          "compared_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3",
            "exp_4",
            "exp_5"
          ]
        },
        {
          "baseline": "Diffusion model variants",
          "compared_in_experiments": [
            "exp_3",
            "exp_5"
          ]
        },
        {
          "baseline": "Unit propagation integration",
          "compared_in_experiments": [
            "exp_3"
          ]
        }
      ],
      "overall_narrative": "The experiments collectively demonstrate that recurrent GNNs with variable-clause graph representation and RNN updates form an effective base architecture for neural SAT solving. The novel closest assignment training method significantly improves performance, especially on problems with large solution spaces. The recurrent design enables flexible test-time scaling through increased iterations and resampling, allowing performance-computation tradeoffs. Extending the model to a diffusion framework provides another avenue for incremental sampling, and combining it with classical unit propagation yields further gains. The ablation studies confirm that a hidden dimension of 64 is optimal and that performance plateaus beyond certain step counts. Overall, the experiments show that these models implicitly perform continuous optimization similar to MaxSAT relaxations, explaining their generalization ability.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "Variable-clause graph representation with RNN updates is effective for SAT assignment prediction.",
          "supported_by": [
            "exp_1"
          ],
          "strength": "strong"
        },
        {
          "claim": "Closest assignment supervision improves performance on problems with large solution spaces.",
          "supported_by": [
            "closest_assignment_ablation"
          ],
          "strength": "strong"
        },
        {
          "claim": "Recurrent architectures scale well at test time through increased iterations and resampling.",
          "supported_by": [
            "exp_2"
          ],
          "strength": "strong"
        },
        {
          "claim": "Diffusion model extension enables incremental sampling and benefits from unit propagation.",
          "supported_by": [
            "exp_3"
          ],
          "strength": "strong"
        },
        {
          "claim": "GNNs implicitly perform continuous optimization similar to MaxSAT relaxations.",
          "supported_by": [
            "exp_1",
            "exp_2",
            "exp_3"
          ],
          "strength": "moderate"
        }
      ],
      "limitations_acknowledged": [
        "Models are not competitive with state-of-the-art SAT solvers on real-world benchmarks.",
        "Computational cost of closest assignment training is high for large benchmarks.",
        "Performance degrades on very large problem sizes beyond training distribution."
      ],
      "key_takeaways": [
        "VCG+RNN with assignment prediction and closest assignment training is the most effective configuration.",
        "Test-time scaling via iterations and resampling is a key advantage of recurrent GNNs.",
        "Diffusion models can be simplified (no timestep conditioning) and combined with classical techniques for improved performance.",
        "Hidden dimension 64 offers the best accuracy-efficiency tradeoff."
      ]
    }
  },
  {
    "filename": "2504.11885_HyperSAT_Unsupervised_Hypergraph_Neural_Networks_f.md",
    "paper_title": "HyperSAT: Unsupervised Hypergraph Neural Networks for Weighted MaxSAT Problems",
    "paper_focus": "Proposes HyperSAT, an unsupervised hypergraph neural network model with cross-attention and shared representation constraint loss to solve Weighted MaxSAT problems.",
    "experiment_inventory": {
      "total_experiments": 3,
      "main_experiments": 1,
      "ablation_studies": 1,
      "other_experiments": 1
    },
    "experiments": {
      "exp_1": {
        "name": "Main Performance Comparison",
        "paper_section": "4.3. Result",
        "experiment_type": "main_comparison",
        "purpose": "To compare the overall solving performance of HyperSAT against state-of-the-art baseline methods on standard Weighted MaxSAT benchmarks.",
        "benchmarks": [
          {
            "name": "SATLIB Dataset",
            "variant": "UF100-430 (SAT)",
            "problem_sizes": "100 variables, 430 clauses"
          },
          {
            "name": "SATLIB Dataset",
            "variant": "UUF100-430 (UNSAT)",
            "problem_sizes": "100 variables, 430 clauses"
          },
          {
            "name": "SATLIB Dataset",
            "variant": "UF200-860 (SAT)",
            "problem_sizes": "200 variables, 860 clauses"
          },
          {
            "name": "SATLIB Dataset",
            "variant": "UUF200-860 (UNSAT)",
            "problem_sizes": "200 variables, 860 clauses"
          },
          {
            "name": "SATLIB Dataset",
            "variant": "UF250-1065 (SAT)",
            "problem_sizes": "250 variables, 1065 clauses"
          },
          {
            "name": "SATLIB Dataset",
            "variant": "UUF250-1065 (UNSAT)",
            "problem_sizes": "250 variables, 1065 clauses"
          }
        ],
        "baselines": [
          {
            "name": "HypOp",
            "description": "An advanced unsupervised learning framework that solves constrained combinatorial optimization problems using hypergraphs.",
            "source": "Heydaribeni et al., 2024"
          },
          {
            "name": "Liu et al. (2023)",
            "description": "A supervised GNN-based approach that predicts solutions in an end-to-end manner by transforming CNF formulas into factor graphs.",
            "source": "Liu et al., 2023"
          }
        ],
        "conclusion": {
          "main_finding": "HyperSAT consistently achieves the lowest average weighted sum of unsatisfied clauses across all tested datasets, significantly outperforming both baselines.",
          "supporting_findings": [
            "On UF100-430, HyperSAT's result (15.64) is much better than Liu et al. (32.48) and HypOp (99.15).",
            "The performance advantage holds across both SAT and UNSAT instance types and scales with problem size.",
            "Average weight of unsatisfied clauses is reduced by approximately 50% compared to Liu et al. and over 80% compared to HypOp."
          ],
          "winner": "HyperSAT"
        },
        "detailed_description": "This is the main comparative experiment evaluating HyperSAT's core performance. It uses six standard 3-SAT benchmarks from SATLIB (UF/UUF series) converted to Weighted MaxSAT instances by assigning random integer weights (1-10) to each clause. The primary metric is the average weighted sum of unsatisfied clauses (lower is better). The experiment compares HyperSAT against two GNN-based baselines: HypOp (unsupervised hypergraph method) and a supervised GNN method from Liu et al. (2023). Results are presented in Table 2. HyperSAT achieves the best scores on every dataset. For example, on UF100-430, it scores 15.64 vs. 32.48 (Liu et al.) and 99.15 (HypOp). The performance gains are consistent and substantial, with relative reductions of unsatisfied clause weight ranging from ~50% to over 80%. The experiment demonstrates that HyperSAT's novel architecture (hypergraph modeling of literals, cross-attention, shared loss) is highly effective for Weighted MaxSAT.\n"
      },
      "exp_2": {
        "name": "Analytical Experiment (Convergence Analysis)",
        "paper_section": "4.2. Analytical Experiment",
        "experiment_type": "other",
        "purpose": "To analyze and compare the training convergence behavior of the unsupervised methods HyperSAT and HypOp.",
        "benchmarks": [
          {
            "name": "SATLIB Dataset",
            "variant": "UUF250-1065",
            "problem_sizes": "250 variables, 1065 clauses"
          }
        ],
        "baselines": [
          {
            "name": "HypOp",
            "description": "Unsupervised hypergraph optimization baseline.",
            "source": "Heydaribeni et al., 2024"
          }
        ],
        "conclusion": {
          "main_finding": "HyperSAT converges more quickly and to a significantly lower final loss value than HypOp.",
          "supporting_findings": [
            "Both models converge within 300 epochs.",
            "HyperSAT's final primary task loss (L_task) is around 52, while HypOp's is around 139."
          ],
          "winner": "HyperSAT"
        },
        "detailed_description": "This experiment focuses on the training dynamics of the unsupervised solvers. It tracks the evolution of the primary task loss (L_task from Eq. 8, representing the sum of weights of unsatisfied clauses) over 300 epochs during inference on the UUF250-1065 dataset. A figure (Figure 3) illustrates the loss curves. The results show HyperSAT not only decreases the loss more rapidly but also plateaus at a much lower value (~52) compared to HypOp (~139). This demonstrates that HyperSAT's optimization process is more efficient and effective at minimizing the core objective. It provides evidence that the proposed architectural innovations lead to better gradient-based optimization for the Weighted MaxSAT problem.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "Main Performance Comparison",
        "benchmarks": "UF100-430 (SAT), UUF100-430 (UNSAT), UF200-860 (SAT), UUF200-860 (UNSAT), UF250-1065 (SAT), UUF250-1065 (UNSAT)",
        "baselines": "HypOp, Liu et al. (2023)",
        "key_conclusion": "HyperSAT consistently outperforms both baselines across all datasets, reducing unsatisfied clause weight by ~50% vs. Liu et al. and ~80% vs. HypOp."
      },
      {
        "experiment": "Analytical Experiment (Convergence)",
        "benchmarks": "UUF250-1065",
        "baselines": "HypOp",
        "key_conclusion": "HyperSAT converges faster and to a lower loss value (~52 vs. ~139) than HypOp."
      },
      {
        "experiment": "Ablation Study",
        "benchmarks": "UUF250-1065",
        "baselines": "Ablated variants of HyperSAT",
        "key_conclusion": "All three components (literal node hypergraph modeling, transformer/cross-attention, shared representation loss) contribute significantly, with the full model performing best."
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "SATLIB UF/UUF Series",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "ablation_1"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "HypOp",
          "compared_in_experiments": [
            "exp_1",
            "exp_2"
          ]
        },
        {
          "baseline": "Liu et al. (2023)",
          "compared_in_experiments": [
            "exp_1"
          ]
        }
      ],
      "overall_narrative": "The experiments collectively build a case for HyperSAT's superiority. The analytical experiment establishes its efficient convergence. The main comparison proves it outperforms existing state-of-the-art methods across a range of standard benchmarks. Finally, the ablation study deconstructs the proposed architecture, validating that each novel component (literal-node hypergraph, cross-attention, shared loss) is necessary and contributes synergistically to the final performance. Together, they demonstrate that the proposed approach effectively addresses the challenges of Weighted MaxSAT (non-linear dependencies, sensitive objective) and represents a significant advance over prior GNN-based methods.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "HyperSAT achieves better performance than state-of-the-art competitors on Weighted MaxSAT.",
          "supported_by": [
            "exp_1"
          ],
          "strength": "strong"
        },
        {
          "claim": "Modeling Weighted MaxSAT as a hypergraph with distinct literal nodes is superior to variable-node modeling.",
          "supported_by": [
            "ablation_1"
          ],
          "strength": "strong"
        },
        {
          "claim": "The cross-attention mechanism and shared representation constraint loss are effective components.",
          "supported_by": [
            "ablation_1"
          ],
          "strength": "strong"
        },
        {
          "claim": "The proposed unsupervised framework converges effectively.",
          "supported_by": [
            "exp_2"
          ],
          "strength": "strong"
        }
      ],
      "limitations_acknowledged": [
        "Experiments are limited to random 3-SAT benchmarks from SATLIB with uniformly random weights (1-10).",
        "Problem sizes tested are moderate (100-250 variables)."
      ],
      "key_takeaways": [
        "HyperSAT significantly outperforms existing GNN-based methods (HypOp, Liu et al. 2023) on Weighted MaxSAT.",
        "The hypergraph representation using literal nodes (not variable nodes) is a critical design choice for performance.",
        "The unsupervised multi-objective loss (primary task + shared constraint) is effective for this problem.",
        "The model scales well and maintains performance advantage on larger problem instances."
      ]
    }
  },
  {
    "filename": "2505.16053_Learning_from_Algorithm_Feedback_One-Shot_SAT_Solv.md",
    "paper_title": "Learning from Algorithm Feedback: One-Shot SAT Solver Guidance with GNNs",
    "paper_focus": "Proposes Reinforcement Learning from Algorithm Feedback (RLAF), a paradigm to train Graph Neural Network (GNN) policies that guide SAT solver branching heuristics by injecting variable weights and polarities in a single forward pass, using the solver's computational cost as the sole reward signal.",
    "experiment_inventory": {
      "total_experiments": 6,
      "main_experiments": 3,
      "ablation_studies": 1,
      "other_experiments": 2
    },
    "experiments": {
      "exp_4": {
        "name": "GNN Overhead Analysis",
        "paper_section": "Appendix B.5, Table 5",
        "experiment_type": "scalability",
        "purpose": "To quantify the computational overhead of the GNN forward pass in the RLAF guidance pipeline and confirm the efficiency of the one-shot approach.",
        "benchmarks": [
          {
            "name": "All test sets from Main Results",
            "variant": "3SAT, 3COL, CRYPTO of various sizes",
            "problem_sizes": "As listed in Table 1/5"
          }
        ],
        "baselines": [],
        "conclusion": {
          "main_finding": "The wall-clock time for the single GNN forward pass is minimal (between 0.02 and 0.1 seconds) and is negligible compared to the total solver runtime, especially on harder instances.",
          "supporting_findings": [
            "Overhead scales modestly with instance size (e.g., ~0.02s for 3SAT(300) to ~0.1s for 3COL(600)).",
            "This overhead is orders of magnitude smaller than the cost of performing a GNN pass for *every* branching decision, which would be infeasible."
          ],
          "winner": "N/A"
        },
        "detailed_description": "This experiment addresses a practical concern about the proposed method: the extra time required to run the GNN. Table 5 extends the main results table by adding a column for the mean GNN inference time. The data shows that this overhead is very small, typically a fraction of a second. For example, on a very hard 3SAT(400) UNSAT instance solved by Glucose+RLAF in ~1112 seconds, the GNN took only 0.0265 seconds. This validates a key advantage of the one-shot guidance paradigm: the guidance signal is computed once upfront with minimal penalty, unlike prior per-decision guidance methods which would incur crippling overhead. The analysis confirms the practical viability of the approach.\n"
      },
      "exp_5": {
        "name": "Supervised Comparison with March",
        "paper_section": "Appendix B.4, Figure 7",
        "experiment_type": "main_comparison",
        "purpose": "To extend the comparison between RLAF and supervised heuristic prediction methods to the March base solver.",
        "benchmarks": [
          {
            "name": "Random 3SAT",
            "variant": "For Backbone comparison",
            "problem_sizes": "n=300, 350, 400"
          },
          {
            "name": "Graph 3-Coloring (3COL)",
            "variant": "For UNSAT Core comparison",
            "problem_sizes": "n=400, 500, 600"
          },
          {
            "name": "Cryptographic (CRYPTO)",
            "variant": "For UNSAT Core comparison",
            "problem_sizes": "n=20, 15, 10"
          }
        ],
        "baselines": [
          {
            "name": "Supervised Backbone GNN",
            "description": "Same as in exp_2, but integrated into the March solver.",
            "source": "Adapted from Wang et al. (2024)"
          },
          {
            "name": "Supervised UNSAT Core GNN",
            "description": "Same as in exp_2, but integrated into the March solver.",
            "source": "Adapted from Selsam and BjÃ¸rner (2019)"
          }
        ],
        "conclusion": {
          "main_finding": "For the March solver, RLAF guidance is competitive with or superior to supervised guidance, and it avoids the performance degradation seen when applying backbone guidance to unsatisfiable instances.",
          "supporting_findings": [
            "On satisfiable 3SAT, RLAF and backbone guidance perform similarly.",
            "On unsatisfiable 3SAT, backbone guidance increases March's runtime by ~10%, while RLAF does not.",
            "On 3COL and CRYPTO, RLAF consistently outperforms UNSAT core-based guidance for March."
          ],
          "winner": "RLAF"
        },
        "detailed_description": "This experiment replicates the supervised comparison (exp_2) but for the March solver, presented in Figure 7 of the appendix. The results reinforce the findings from the Glucose comparison. RLAF's performance is robust across solvers. A notable observation is that using backbone predictions to guide March on unsatisfiable 3SAT instances is actively harmful, increasing runtime. In contrast, RLAF, which is trained with a cost signal agnostic to such heuristics, does not suffer from this issue. This further underscores the advantage of RLAF's direct optimization for runtime over methods reliant on predefined, potentially misapplied, concepts.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "Main Results Evaluation",
        "benchmarks": "3SAT(300,350,400), 3COL(400,500,600), CRYPTO(20,15,10)",
        "baselines": "Base Glucose, Base March",
        "key_conclusion": "RLAF guidance consistently accelerates both solvers across most problem distributions, with up to 69% runtime reduction, demonstrating effective generalization to larger, harder instances."
      },
      {
        "experiment": "Comparison to Supervised Approaches",
        "benchmarks": "3SAT (Backbone), 3COL & CRYPTO (UNSAT Core)",
        "baselines": "Supervised GNN predicting Backbone, Supervised GNN predicting UNSAT Core",
        "key_conclusion": "RLAF outperforms supervised methods on most benchmarks, showing that learning directly from solver feedback is more effective than imitating handcrafted heuristics."
      },
      {
        "experiment": "Learned Variable Weights Analysis",
        "benchmarks": "Validation sets from 3SAT, 3COL, CRYPTO",
        "baselines": "N/A (Analysis of learned weights)",
        "key_conclusion": "Weights learned by different solvers are highly correlated (r=0.73-0.85) and for structured problems (3COL, CRYPTO) correlate with UNSAT core membership, indicating capture of solver-agnostic structural properties."
      },
      {
        "experiment": "Training Curves Ablation",
        "benchmarks": "Validation sets for each distribution",
        "baselines": "RLAF models at different training iterations",
        "key_conclusion": "Training consistently reduces solver cost; March models show noisier training on 3SAT, plateauing early, but all models learn effectively."
      },
      {
        "experiment": "GNN Overhead Analysis",
        "benchmarks": "All test sets from Main Results",
        "baselines": "N/A",
        "key_conclusion": "GNN forward pass overhead is minimal (0.02-0.1s), negligible compared to solver runtimes, validating the efficiency of the one-shot guidance approach."
      },
      {
        "experiment": "Supervised Comparison with March",
        "benchmarks": "3SAT (Backbone), 3COL & CRYPTO (UNSAT Core)",
        "baselines": "Supervised Backbone GNN, Supervised UNSAT Core GNN",
        "key_conclusion": "RLAF matches or outperforms supervised guidance for March, particularly on unsatisfiable instances where backbone guidance harms performance."
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "Random 3SAT",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3",
            "exp_5"
          ]
        },
        {
          "benchmark": "Graph 3-Coloring (3COL)",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3",
            "exp_5"
          ]
        },
        {
          "benchmark": "Cryptographic (CRYPTO)",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3",
            "exp_5"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "Base Glucose",
          "compared_in_experiments": [
            "exp_1",
            "exp_2"
          ]
        },
        {
          "baseline": "Base March",
          "compared_in_experiments": [
            "exp_1",
            "exp_5"
          ]
        },
        {
          "baseline": "Supervised Backbone GNN",
          "compared_in_experiments": [
            "exp_2",
            "exp_5"
          ]
        },
        {
          "baseline": "Supervised UNSAT Core GNN",
          "compared_in_experiments": [
            "exp_2",
            "exp_5"
          ]
        }
      ],
      "overall_narrative": "The experiments collectively build a compelling case for the RLAF paradigm. The Main Results (exp_1) establish that RLAF can train effective guidance policies that generalize to harder problems. The comparison against supervised methods (exp_2, exp_5) demonstrates that RLAF's RL-from-feedback approach outperforms learning to mimic existing expert heuristics, showing it can discover superior guidance strategies. The analysis of learned weights (exp_3) provides explanatory power, showing that RLAF captures useful, solver-agnostic structural properties (sometimes aligning with, sometimes diverging from known heuristics). The ablation (ablation_1) confirms the training process works, and the overhead analysis (exp_4) validates the practical efficiency of the one-shot design. Together, they validate RLAF as a powerful, general, and efficient method for data-driven heuristic design.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "RLAF can train GNN-based policies that significantly reduce SAT solver runtimes.",
          "supported_by": [
            "exp_1"
          ],
          "strength": "strong"
        },
        {
          "claim": "RLAF-trained policies generalize effectively to larger and harder instances than those used in training.",
          "supported_by": [
            "exp_1"
          ],
          "strength": "strong"
        },
        {
          "claim": "RLAF outperforms expert-supervised approaches based on learning handcrafted heuristics (backbone/UNSAT core).",
          "supported_by": [
            "exp_2",
            "exp_5"
          ],
          "strength": "strong"
        },
        {
          "claim": "The one-shot guidance mechanism is efficient, with negligible GNN runtime overhead.",
          "supported_by": [
            "exp_4"
          ],
          "strength": "strong"
        },
        {
          "claim": "Policies learned with different solvers capture related, solver-agnostic structural insights.",
          "supported_by": [
            "exp_3"
          ],
          "strength": "moderate"
        }
      ],
      "limitations_acknowledged": [
        "Training is computationally intensive, requiring thousands of solver calls per iteration, limiting the hardness of training instances on moderate hardware.",
        "The expressive power of the GNN is bounded by color refinement, potentially limiting handling of high symmetry.",
        "For March on unsatisfiable random 3SAT, the strong baseline heuristic leaves little room for improvement, and GNN overhead negates small gains."
      ],
      "key_takeaways": [
        "Reinforcement Learning from Algorithm Feedback (RLAF) is a viable and powerful paradigm for learning to guide combinatorial search heuristics without expert supervision.",
        "The proposed one-shot weight injection method is generic, efficient, and effective across different solver architectures (CDCL vs. DPLL).",
        "Optimizing directly for the algorithm's cost signal can discover guidance strategies superior to those designed by human experts."
      ]
    }
  },
  {
    "filename": "2506.11057_STRCMP_Integrating_Graph_Structural_Priors_with_La.md",
    "paper_title": "STRCMP: Integrating Graph Structural Priors with Language Models for Combinatorial Optimization",
    "paper_focus": "Proposes a structure-aware LLM-based algorithm discovery framework (STRCMP) that integrates Graph Neural Network embeddings with LLMs for generating solver-specific code to improve solution quality and computational efficiency in combinatorial optimization.",
    "experiment_inventory": {
      "total_experiments": 3,
      "main_experiments": 2,
      "ablation_studies": 1,
      "other_experiments": 0
    },
    "experiments": {
      "exp_1": {
        "name": "Optimization Performance Comparison",
        "paper_section": "5.3 Results",
        "experiment_type": "main_comparison",
        "purpose": "To determine if STRCMP identifies superior algorithmic implementations compared to existing algorithm discovery approaches (RQ1).",
        "benchmarks": [
          {
            "name": "SAT Benchmarks",
            "variant": "CNP, CoinsGrid, PRP, Zamkeller",
            "problem_sizes": "Varied (mean constraints: 261k to 2.1M, mean variables: 8.8k to 317k)"
          },
          {
            "name": "MILP Benchmarks",
            "variant": "Easy (Set Covering, MIS, Knapsack), Medium (MIK, CORLAT), Hard (Load Balancing)",
            "problem_sizes": "Easy: 500-1000 vars; Medium: ~400 vars; Hard: 61k vars"
          }
        ],
        "baselines": [
          {
            "name": "L2B",
            "description": "Graph convolutional neural network for variable selection in branch-and-bound.",
            "source": "Gasse et al. [14]"
          },
          {
            "name": "HEM",
            "description": "Hierarchical sequence model for cut selection in MILP solvers.",
            "source": "Wang et al. [15,16]"
          },
          {
            "name": "NeuroSAT",
            "description": "Message-passing neural network trained on binary SAT labels.",
            "source": "Selsam et al. [40]"
          },
          {
            "name": "AutoSAT",
            "description": "LLM-based evolutionary framework to optimize SAT solver heuristics.",
            "source": "Sun et al. [29]"
          },
          {
            "name": "LLM4Solver",
            "description": "LLM integrated with evolutionary algorithms to design diving heuristics for MILP.",
            "source": "Zhou et al. [30]"
          }
        ],
        "conclusion": {
          "main_finding": "STRCMP consistently matches or exceeds all baseline performance across SAT and MILP domains.",
          "supporting_findings": [
            "For SAT: STRCMP shows universal superiority over AutoSAT, reducing timeouts by 77.8% on Zamkeller (18â†’4) and 66.7% on PRP (9â†’3).",
            "For SAT: Solving time reduced from 22967s to 21146s on PRP, and from 20772s to 6929s on Zamkeller.",
            "For MILP: STRCMP maintains strong performance parity with NCO methods (L2B/HEM) and LLM4Solver."
          ],
          "winner": "STRCMP"
        },
        "detailed_description": "This experiment evaluates STRCMP's optimization performance against two categories of baselines: Neural Combinatorial Optimization (L2B, HEM, NeuroSAT) and Evolutionary Code Optimization (AutoSAT, LLM4Solver). The evaluation spans nine benchmark datasets across two CO domains: Boolean Satisfiability (SAT) and Mixed Integer Linear Programming (MILP). For SAT, four datasets (CNP, CoinsGrid, PRP, Zamkeller) are used with metrics including PAR-2 score, solving time, and number of timeouts (5000s limit). For MILP, three difficulty tiers (Easy, Medium, Hard) covering six problem types are evaluated using primal-dual (PD) integral and solving time. STRCMP and its variants (DPO Only, SFT Only, w/o GNN) are compared. Results show STRCMP achieves the best overall performance. Specifically, in SAT, it significantly reduces timeouts and solving time compared to AutoSAT. In MILP, it performs comparably or better than specialized neural methods (L2B/HEM) and the evolutionary baseline LLM4Solver. The experiment validates RQ1, demonstrating that STRCMP successfully identifies superior algorithmic configurations.\n"
      },
      "exp_2": {
        "name": "Efficiency Comparison (Convergence Rate)",
        "paper_section": "5.3 Results, Appendix G.2",
        "experiment_type": "main_comparison",
        "purpose": "To assess if STRCMP reduces computational overhead in evolutionary-based algorithm discovery frameworks (RQ2).",
        "benchmarks": [
          {
            "name": "Zamkeller (SAT)",
            "variant": "Standard",
            "problem_sizes": "Mean constraints: 310,804, mean variables: 24,592"
          },
          {
            "name": "PRP (SAT)",
            "variant": "Standard",
            "problem_sizes": "Mean constraints: 2,120,983, mean variables: 317,635"
          },
          {
            "name": "MILP Benchmarks",
            "variant": "Set Covering, MIS, Knapsack, MIK, CORLAT, Load Balancing",
            "problem_sizes": "Varied (see Appendix D.2)"
          }
        ],
        "baselines": [
          {
            "name": "AutoSAT",
            "description": "LLM-based evolutionary framework for SAT.",
            "source": "Sun et al. [29]"
          },
          {
            "name": "LLM4Solver",
            "description": "LLM with multi-objective EA for MILP.",
            "source": "Zhou et al. [30]"
          }
        ],
        "conclusion": {
          "main_finding": "STRCMP achieves convergence significantly faster than existing evolutionary-based frameworks and attains higher-quality convergence points.",
          "supporting_findings": [
            "STRCMP converges faster than AutoSAT and LLM4Solver across metrics (PAR-2, solving time).",
            "STRCMP achieves stable convergence, while AutoSAT exhibits persistent oscillations even after convergence.",
            "The convergence advantage is consistent across SAT and MILP domains (Figures 4, 8, 9, 10)."
          ],
          "winner": "STRCMP"
        },
        "detailed_description": "This experiment measures the efficiency of the evolutionary search process within STRCMP compared to other LLM-based evolutionary frameworks (AutoSAT for SAT, LLM4Solver for MILP). The evaluation tracks convergence curves over iterations/epochs for key performance metrics: PAR-2 for SAT, and PD integral and solving time for MILP. The results, visualized in Figures 4, 8, 9, and 10 (Appendix), show that STRCMP reaches lower (better) metric values in fewer iterations. Notably, on the Zamkeller SAT dataset (Figure 4), STRCMP's PAR-2 score drops rapidly and stabilizes at a lower value than AutoSAT, which oscillates. Similar trends are observed for solving time and number of timeouts. For MILP (Figure 10), STRCMP converges faster than LLM4Solver on PD integral and solving time across all benchmark types (Easy, Medium, Hard). This demonstrates that integrating structural priors via the composite model reduces the number of expensive solver-invocation cycles needed during evolutionary search, directly addressing RQ2 by showing reduced computational overhead.\n"
      },
      "ablation_1": {
        "name": "Ablation Studies on Composite Model",
        "paper_section": "5.3 Results, Appendix G.3",
        "experiment_type": "ablation",
        "purpose": "",
        "benchmarks": [
          {
            "name": "SAT Benchmarks",
            "variant": "CNP, CoinsGrid, PRP, Zamkeller",
            "problem_sizes": "Varied"
          },
          {
            "name": "MILP Benchmarks",
            "variant": "Set Covering, MIS, Knapsack, MIK, CORLAT, Load Balancing",
            "problem_sizes": "Varied"
          }
        ],
        "baselines": [],
        "conclusion": {
          "main_finding": "The structural prior (GNN) provides measurable benefits, but the full post-training protocol (SFT+DPO) does not uniformly outperform its individual components.",
          "importance_of_component": "The GNN is important for performance and stability; the SFT and DPO stages show conflicting effects on different benchmarks."
        },
        "detailed_description": "This ablation study systematically deactivates components of the STRCMP composite model to evaluate their contribution. Four variants are compared: the full STRCMP, SFT-only, DPO-only, and a version without the GNN (w/o GNN). The evaluation uses the same SAT and MILP benchmarks as the main experiments. Key findings: 1) The 'STRCMP w/o GNN' variant consistently shows inferior optimization performance and increased solution variability during search compared to variants with the GNN, confirming that the structural prior is beneficial (addressing RQ3). 2) Counterintuitively, the full STRCMP model does not always outperform the SFT-only or DPO-only variants across all benchmarks. For example, on the PRP SAT dataset (Table 1), STRCMP (DPO Only) achieves only 3 timeouts, while full STRCMP has 44. The authors hypothesize this is due to \"underlying conflicts within the post-training data distribution.\" The ablation confirms the value of the GNN for incorporating structural priors but reveals a complexity in the interaction between SFT and DPO optimization objectives.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "Optimization Performance Comparison",
        "benchmarks": "CNP, CoinsGrid, PRP, Zamkeller (SAT); Set Covering, MIS, Knapsack, MIK, CORLAT, Load Balancing (MILP)",
        "baselines": "L2B, HEM, NeuroSAT (Neural); AutoSAT, LLM4Solver (Evolutionary)",
        "key_conclusion": "STRCMP outperforms all baselines in SAT and matches/beats them in MILP domains."
      },
      {
        "experiment": "Efficiency Comparison (Convergence Rate)",
        "benchmarks": "Zamkeller, PRP (SAT); MILP benchmarks",
        "baselines": "AutoSAT, LLM4Solver",
        "key_conclusion": "STRCMP converges faster and to higher-quality solutions than evolutionary baselines."
      },
      {
        "experiment": "Ablation Studies on Composite Model",
        "benchmarks": "SAT and MILP benchmarks",
        "baselines": "STRCMP variants (SFT Only, DPO Only, w/o GNN)",
        "key_conclusion": "Structural prior (GNN) improves performance; full post-training can underperform due to data conflicts."
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "SAT Benchmarks (CNP, CoinsGrid, PRP, Zamkeller)",
          "used_in_experiments": [
            "exp_1",
            "ablation_1"
          ]
        },
        {
          "benchmark": "MILP Benchmarks (Easy, Medium, Hard tiers)",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "ablation_1"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "AutoSAT",
          "compared_in_experiments": [
            "exp_1",
            "exp_2"
          ]
        },
        {
          "baseline": "LLM4Solver",
          "compared_in_experiments": [
            "exp_1",
            "exp_2"
          ]
        },
        {
          "baseline": "L2B, HEM, NeuroSAT",
          "compared_in_experiments": [
            "exp_1"
          ]
        },
        {
          "baseline": "STRCMP variants (SFT Only, DPO Only, w/o GNN)",
          "compared_in_experiments": [
            "exp_1",
            "ablation_1"
          ]
        }
      ],
      "overall_narrative": "The experiments together systematically validate the paper's core thesis: that integrating graph structural priors into LLM-based algorithm discovery improves both solution quality (RQ1) and search efficiency (RQ2). The main comparison experiments (exp_1, exp_2) demonstrate STRCMP's superiority over a range of strong neural and evolutionary baselines across diverse CO problems. The ablation study (ablation_1) directly isolates and confirms the contribution of the structural prior (GNN), providing empirical evidence for the theoretical claims made in Section 4.2. The pattern across experiments shows that the structural prior is crucial for performance gains, but the optimal fine-tuning strategy (SFT vs. DPO) is problem-dependent and requires further investigation.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "STRCMP identifies superior algorithmic implementations compared to existing approaches.",
          "supported_by": [
            "exp_1"
          ],
          "strength": "strong"
        },
        {
          "claim": "STRCMP reduces computational overhead in evolutionary algorithm discovery frameworks.",
          "supported_by": [
            "exp_2"
          ],
          "strength": "strong"
        },
        {
          "claim": "Integrating structural priors benefits generative models for solving CO problems.",
          "supported_by": [
            "ablation_1",
            "exp_1",
            "exp_2"
          ],
          "strength": "strong"
        }
      ],
      "limitations_acknowledged": [
        "End-to-end joint training of the GNN and LLM is challenging, requiring future work on architecture/alignment.",
        "Full post-training (SFT+DPO) can underperform individual stages, suggesting conflicts in the optimization objectives.",
        "Experiments are limited to SAT and MILP domains; generalization to other CO problems is not tested."
      ],
      "key_takeaways": [
        "STRCMP outperforms state-of-the-art neural and LLM-based methods on standard CO benchmarks.",
        "The integration of graph structural priors via a GNN significantly accelerates convergence in evolutionary search.",
        "The structural prior is a performance-enhancing component, but the best fine-tuning protocol may be problem-specific."
      ]
    }
  },
  {
    "filename": "2508.04235_Circuit-Aware_SAT_Solving_Guiding_CDCL_via_Conditi.md",
    "paper_title": "Circuit-Aware SAT Solving: Guiding CDCL via Conditional Probabilities",
    "paper_focus": "Proposes CASCAD, a circuit-aware SAT solving framework that uses GNN-predicted conditional probabilities to guide CDCL heuristics (phase selection and clause management) for improved efficiency on Circuit SAT problems.",
    "experiment_inventory": {
      "total_experiments": 8,
      "main_experiments": 5,
      "ablation_studies": 2,
      "other_experiments": 1
    },
    "experiments": {
      "exp_1": {
        "name": "Conditional Probability Prediction Performance",
        "paper_section": "4.3",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate the accuracy and scalability of the GNN model in predicting conditional probabilities under varying numbers of condition nodes and circuit sizes.",
        "benchmarks": [
          {
            "name": "ITC99, EPFL, OpenCore AIGs",
            "variant": "Representative circuits of three size categories",
            "problem_sizes": "928 nodes, 6428 nodes, 17796 nodes"
          }
        ],
        "baselines": [],
        "conclusion": {
          "main_finding": "The model maintains low prediction error (MAE) across different circuit scales and as the number of condition nodes increases from 1 to 5.",
          "supporting_findings": [
            "For small circuits (~928 nodes), MAE is as low as 0.0243 with one condition.",
            "For large circuits (>17K nodes), MAE remains below 0.05 even with 5 conditions.",
            "Inference overhead for multi-condition settings is negligible due to lightweight aggregators."
          ],
          "winner": "CASCAD GNN model demonstrates strong generalization and robustness."
        },
        "detailed_description": "This experiment evaluates the core predictive capability of the proposed GNN model. The model was trained on small circuits and then tested on a validation set comprising circuits of varying sizes (small: ~928 nodes, medium: ~6428 nodes, large: ~17796 nodes) from ITC99, EPFL, and OpenCore benchmarks. The primary metric is Mean Absolute Error (MAE) in predicting conditional probabilities. The test varies the number of condition nodes (1, 2, 5) to assess the model's ability to handle multiple conditions. Results show consistently low MAE across all scales (e.g., 0.0243 to 0.0457 for 1 condition) and a modest increase in error as condition count grows (up to 0.0532 for 5 conditions on large circuits). This demonstrates the model's accuracy and scalability, which is critical for its integration into the SAT solver. The lightweight design of the aggregator functions ensures that multi-condition inference does not incur significant computational overhead.\n"
      },
      "exp_2": {
        "name": "Ablation: Effectiveness of div Gate",
        "paper_section": "4.4 (and Appendix A2)",
        "experiment_type": "ablation",
        "purpose": "To compare the proposed div gate method against direct division for computing conditional probabilities, especially under challenging numerical conditions.",
        "benchmarks": [
          {
            "name": "ac97_ctrl.aig",
            "variant": "Case study circuit",
            "problem_sizes": "N/A"
          }
        ],
        "baselines": [
          {
            "name": "Direct division",
            "description": "Computing P(A|C) by dividing the predicted joint probability P(Aâˆ§C) by the predicted marginal P(C).",
            "source": "Standard arithmetic operation"
          }
        ],
        "conclusion": {
          "main_finding": "The div gate method yields consistently and significantly lower MAE than direct division, particularly when the condition probability P(C) is small ('polar condition').",
          "supporting_findings": [
            "For one moderate condition, div gate MAE is 0.0312 vs. 0.1173 for direct division.",
            "For one polar condition, div gate MAE is 0.0395 vs. a very high 0.7129 for direct division.",
            "Error amplification in direct division is severe when denominators are small."
          ],
          "winner": "div gate method"
        },
        "detailed_description": "This ablation study isolates the contribution of the novel 'div gate' introduced in the graph construction. The goal is to show that explicitly modeling conditional probability as a gate type is superior to naively computing it via division of predicted joint and marginal probabilities. The experiment uses the ac97_ctrl.aig circuit as a case study. Two scenarios are tested: 'Moderate Condition' (where P(C) is not extremely small) and 'Polar Condition' (where P(C) is very small, e.g., 0.01). For each, the MAE of the predicted P(A|C) is compared between the div gate method and the direct division method. Results overwhelmingly favor the div gate. For example, with one polar condition, direct division yields an MAE of 0.7129, which is ~18 times higher than the div gate's 0.0395. This is because direct division suffers from unstable gradients and error amplification when dividing by a small predicted probability. The div gate learns the division operation within the GNN, making it numerically stable and more accurate. This finding validates a key architectural innovation of CASCAD.\n"
      },
      "exp_3": {
        "name": "Ablation: Effectiveness of Two-Stage Training",
        "paper_section": "4.4",
        "experiment_type": "ablation",
        "purpose": "To evaluate the contribution of the two-stage training strategy (pattern-based pre-training followed by workload-aware fine-tuning) compared to using either stage alone.",
        "benchmarks": [
          {
            "name": "Validation set from ITC99, EPFL, OpenCore AIGs",
            "variant": "N/A",
            "problem_sizes": "N/A"
          }
        ],
        "baselines": [
          {
            "name": "Only Stage-1",
            "description": "Model trained only with pattern-based pre-training (100 patterns per minibatch).",
            "source": "Ablation variant"
          },
          {
            "name": "Only Stage-2",
            "description": "Model trained only with workload-aware fine-tuning (20,000-pattern distributions).",
            "source": "Ablation variant"
          }
        ],
        "conclusion": {
          "main_finding": "The combined two-stage training strategy achieves the lowest prediction error (L1 loss) across different numbers of condition nodes.",
          "supporting_findings": [
            "For one condition node, two-stage loss is 0.0331, vs. 0.0422 (Stage-1) and 0.0435 (Stage-2).",
            "The performance gap widens with more condition nodes (e.g., for 5 nodes, two-stage loss is 0.0387 vs. 0.0738 for Stage-1).",
            "Stage-2 model also accelerates inference by 200x compared to Stage-1, which requires multiple invocations."
          ],
          "winner": "Two-stage training strategy"
        },
        "detailed_description": "This ablation study examines the efficacy of the proposed two-stage training pipeline. Stage-1 (pattern-based pre-training) uses 100 random patterns per minibatch to capture fine-grained circuit behavior. Stage-2 (workload-aware fine-tuning) uses aggregated probabilities from 20,000 patterns under diverse Primary Input (PI) workloads to capture broader statistical properties. The experiment compares the full two-stage model against models trained with only Stage-1 or only Stage-2. The evaluation metric is L1 loss on conditional probability prediction for 1, 2, and 5 condition nodes. Results show that the two-stage model consistently outperforms both individual stages. For instance, with 5 condition nodes, the two-stage loss is nearly half that of Stage-1 alone (0.0387 vs. 0.0738). Additionally, the Stage-2 model is much faster at inference because it directly uses workload-based PI embeddings, whereas Stage-1 would need to be invoked 200 times to simulate the 20,000 patterns. This demonstrates that the two-stage strategy is crucial for achieving both accuracy and efficiency.\n"
      },
      "exp_4": {
        "name": "Node Probability for Phase Selection (SAT instances)",
        "paper_section": "5.2",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate the performance improvement in SAT solving when using CASCAD's predicted conditional probabilities to guide variable phase selection, compared to default and prior learning-based heuristics.",
        "benchmarks": [
          {
            "name": "LECSAT",
            "variant": "150 hard satisfiable instances generated via miter construction on ForgeEDA circuits",
            "problem_sizes": "Average solving time 17.39s with CaDiCaL"
          }
        ],
        "baselines": [
          {
            "name": "CaDiCaL",
            "description": "State-of-the-art CDCL SAT solver with default phase selection heuristic.",
            "source": "Fleury and Heisinger 2020"
          },
          {
            "name": "DeepGate4",
            "description": "Prior circuit representation model that guides phase selection by assigning opposite values to functionally similar nodes to trigger conflicts.",
            "source": "Zheng et al. 2025"
          }
        ],
        "conclusion": {
          "main_finding": "CASCAD's probability-guided phase selection significantly reduces solving time, achieving up to 10x speedup over CaDiCaL and outperforming DeepGate4.",
          "supporting_findings": [
            "At threshold Ï„=0.005, speedup of up to 10x is observed; at Ï„=0.01, nearly 5x speedup.",
            "The method consistently places points below the diagonal (y=x) in the log-scale comparison plot, indicating better performance than CaDiCaL.",
            "It also consistently achieves lower solving times than DeepGate4 across instances."
          ],
          "winner": "CASCAD with Ï„=0.005"
        },
        "detailed_description": "This experiment applies CASCAD's predicted conditional probabilities to the CDCL heuristic of phase selection (deciding whether to assign TRUE or FALSE to a chosen variable). The benchmark is LECSAT, a set of 150 challenging satisfiable instances from Logic Equivalence Checking. CASCAD's phase selection uses Equation (1) with a threshold Ï„. Different Ï„ values (0.005, 0.01, etc.) are tested. The baseline solvers are CaDiCaL (with its default phase selection) and a version of CaDiCaL augmented with DeepGate4's similarity-based guidance. Solving times are compared on a log-scale scatter plot (CASCAD vs. CaDiCaL). The results show that most points lie above the diagonal, meaning CASCAD is faster. The speedup is most pronounced at Ï„=0.005 (up to 10x). The method also consistently beats DeepGate4, demonstrating that conditional probability is a more effective guide than node similarity for this task. This experiment validates the first major application of CASCAD's probabilities.\n"
      },
      "exp_5": {
        "name": "Clause Probability for Clause Management (UNSAT instances)",
        "paper_section": "5.3",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate the effectiveness of using clause probability (derived from circuit-level signal estimates) as a metric for filtering learned clauses during CDCL solving, compared to the standard LBD metric.",
        "benchmarks": [
          {
            "name": "LECUNSAT",
            "variant": "150 unsatisfiable instances (130 small, 20 large) from datapath circuit miters",
            "problem_sizes": "Cases unsolvable within 20s by CaDiCaL"
          }
        ],
        "baselines": [
          {
            "name": "CaDiCaL (LBD-based)",
            "description": "Default clause filtering heuristic based on Literal Block Distance (LBD) and clause length.",
            "source": "Integrated in CaDiCaL"
          }
        ],
        "conclusion": {
          "main_finding": "Probability-based clause filtering outperforms LBD-based filtering, solving more instances faster and achieving a lower Par2 score, with an optimal probability threshold around 0.9.",
          "supporting_findings": [
            "On 130 small cases, the method solves more instances within the 400s timeout (shown in cactus plot).",
            "On 20 large cases (Table 5), the best Par2 score is 1408.03 (overall) at Ï„=0.9, compared to baseline 1828.83.",
            "Thresholds too low (0.8) retain too few clauses; too high (0.95) retain noisy clauses."
          ],
          "winner": "CASCAD clause filtering with Ï„=0.9"
        },
        "detailed_description": "This experiment applies CASCAD's probability estimation to the second CDCL heuristic: clause management. The solver runs until 50,000 conflicts, extracts all learned clauses, and filters them based on their computed probability P(C) (Eq. 2). Clauses with probability below a threshold Ï„ are retained. This is compared against CaDiCaL's default LBD-based filtering. The benchmark is LECUNSAT, a set of 150 unsatisfiable instances split into 130 small and 20 large/hard cases. Performance is measured by the number of instances solved within a time limit (for small cases) and by the Par2 score (for large cases), which penalizes timeouts. Results show that probability-based filtering consistently outperforms the baseline. The cactus plot for small cases shows more instances solved at any given time. For large cases, the Par2 score is reduced from 1828.83 to 1408.03 (solving time only) at Ï„=0.9. Even when including model inference time, the overall Par2 (224.95) is significantly better. This demonstrates that circuit-derived clause probability is a more informative quality metric than CNF-level features like LBD.\n"
      },
      "exp_6": {
        "name": "Inprocessing Effectiveness Beyond Preprocessing",
        "paper_section": "5.4",
        "experiment_type": "main_comparison",
        "purpose": "To demonstrate that CASCAD's inprocessing guidance provides additional performance gains on top of state-of-the-art static preprocessing techniques for circuit optimization.",
        "benchmarks": [
          {
            "name": "LECSAT",
            "variant": "150 SAT instances",
            "problem_sizes": "N/A"
          },
          {
            "name": "LECUNSAT",
            "variant": "150 UNSAT instances",
            "problem_sizes": "N/A"
          }
        ],
        "baselines": [
          {
            "name": "Preprocessing-only",
            "description": "Applying circuit optimization techniques from prior work (Shi et al. 2025c) to transform the circuit into a solver-friendly format before solving.",
            "source": "Shi et al. 2025c"
          }
        ],
        "conclusion": {
          "main_finding": "Integrating CASCAD's inprocessing guidance with preprocessing yields substantial additional speedups, proving that dynamic guidance during solving complements static optimizations.",
          "supporting_findings": [
            "For SAT cases (Table 6), adding CASCAD (Ï„=0.005) reduces average Par2 from 13.39 (preprocessing-only) to 2.89 (4.6x improvement).",
            "For UNSAT cases (Table 7), adding clause filtering (Ï„=0.9) reduces Par2 from 304.42 to 224.95.",
            "The improvements are consistent across both SAT and UNSAT instances."
          ],
          "winner": "Preprocessing + CASCAD"
        },
        "detailed_description": "This experiment addresses the question of whether CASCAD's dynamic inprocessing provides value beyond powerful static preprocessing. Two strategies are compared: (1) Preprocessing-only: using the circuit optimization techniques from prior work (Shi et al. 2025c) to simplify the circuit before feeding it to the SAT solver. (2) Preprocessing + CASCAD: applying the same preprocessing, but then also using CASCAD's probability-guided phase selection (for SAT) and clause filtering (for UNSAT) during the solving process. The experiments are run on the LECSAT and LECUNSAT benchmarks. Results in Tables 6 and 7 show that the combined approach significantly outperforms preprocessing alone. For SAT, the best Par2 score drops from 13.39 to 2.89 (a 4.6x improvement). For UNSAT, the Par2 drops from 304.42 to 224.95. This clearly demonstrates that CASCAD's dynamic guidance is not redundant with preprocessing; it provides complementary benefits by influencing heuristics during the solver's search, which static transformations cannot do.\n"
      },
      "exp_7": {
        "name": "Analysis of Probability-Based Clause Metric",
        "paper_section": "Appendix A5",
        "experiment_type": "other",
        "purpose": "To deeply analyze why probability-based clause filtering works by examining the relationship between clause probability, LBD, and their impact on solving efficiency.",
        "benchmarks": [
          {
            "name": "Clauses extracted during solving",
            "variant": "Clauses learned after 50,000 conflicts in the solving process",
            "problem_sizes": "N/A"
          }
        ],
        "baselines": [
          {
            "name": "Clause groups by LBD and probability",
            "description": "Clauses are segmented by their LBD value (1,2,3) and further by whether their predicted probability is below or above 0.8.",
            "source": "Post-hoc analysis"
          }
        ],
        "conclusion": {
          "main_finding": "Clauses with lower predicted probability are more beneficial for subsequent solving, and probability is a more informative metric than LBD alone.",
          "supporting_findings": [
            "For clauses with the same LBD, those with probability <0.8 lead to time and decision reductions, while those with probability â‰¥0.8 can hurt performance (e.g., LBD=2, Pâ‰¥0.8 group increased time by 71%).",
            "The distribution of probability correlates with LBD: lower LBD clauses tend to have lower probability.",
            "Some LBD=3 clauses with low probability still help solving, whereas CaDiCaL's default discards all LBD>2 clauses."
          ],
          "winner": "Probability-based filtering retains more useful clauses."
        },
        "detailed_description": "This experiment provides a detailed analysis to justify the probability-based clause metric. The solver is paused at 50,000 conflicts, and all learned clauses are extracted. These clauses are grouped by their LBD (1, 2, 3) and by a probability threshold (P<0.8 vs. Pâ‰¥0.8). Each group is then re-inserted into the solver independently, and its impact on the remaining solving process is measured in terms of solving time reduction and decision count reduction. Results in Table A5 show that within each LBD group, the low-probability clauses are beneficial (positive time reduction), while high-probability clauses are often detrimental (negative time reduction). For example, for LBD=2 clauses, the low-probability group reduced time by 21.2%, but the high-probability group increased time by 71.0%. Furthermore, the analysis shows that probability provides finer granularity than LBD: some clauses with LBD=3 (normally discarded) but with low probability actually help solving. This validates the design choice of using circuit-derived probability over CNF-based LBD.\n"
      },
      "exp_8": {
        "name": "Phase Selection as UNSAT Classifier",
        "paper_section": "Appendix A6",
        "experiment_type": "other",
        "purpose": "To leverage the SAT-accelerating property of probability-guided phase selection as a lightweight classifier to detect likely UNSAT instances and adapt solver strategy accordingly.",
        "benchmarks": [
          {
            "name": "LECUNSAT",
            "variant": "150 UNSAT instances",
            "problem_sizes": "N/A"
          }
        ],
        "baselines": [
          {
            "name": "Raw CaDiCaL",
            "description": "CaDiCaL solver without adaptive strategy.",
            "source": "Default"
          }
        ],
        "conclusion": {
          "main_finding": "Using the phase selection method as a classifier (with a 5-second timeout) to switch to a UNSAT-tuned solver configuration reduces average solving time by about 2x.",
          "supporting_findings": [
            "Average solving time reduced from 166.26 seconds (raw CaDiCaL) to 80.36 seconds with the adaptive strategy.",
            "The cactus plot (Figure A2) shows the adaptive strategy solves more instances faster across the timeline."
          ],
          "winner": "Adaptive strategy using CASCAD phase selection as classifier"
        },
        "detailed_description": "This experiment presents an application of the phase selection heuristic beyond direct guidance. Since the probability-guided phase selection greatly accelerates SAT solving, the authors hypothesize that if an instance remains unsolved under this heuristic after a short time (5 seconds), it is likely UNSAT. They implement an adaptive strategy: run CaDiCaL with CASCAD's phase selection for 5 seconds; if not solved, switch the solver to a configuration specifically tuned for UNSAT problems (e.g., different heuristic parameters). This strategy is evaluated on the LECUNSAT dataset. Results show that the adaptive strategy reduces the average solving time from 166.26 seconds (raw CaDiCaL) to 80.36 secondsâ€”a speedup of about 2x. The cactus plot confirms that more instances are solved earlier with the adaptive strategy. This demonstrates a practical way to use CASCAD's phase selection not just as a guide, but also as a diagnostic tool to improve solver performance on mixed SAT/UNSAT benchmarks.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "Conditional Probability Prediction Performance",
        "benchmarks": "ITC99, EPFL, OpenCore AIGs (3 size categories: ~928, ~6428, ~17796 nodes)",
        "baselines": "N/A (evaluates model prediction error)",
        "key_conclusion": "Model maintains low MAE (<0.05) across circuit scales and multiple conditions, proving generalization."
      },
      {
        "experiment": "Ablation: Effectiveness of div Gate",
        "benchmarks": "ac97_ctrl.aig (case study circuit)",
        "baselines": "Direct division of joint/marginal probabilities",
        "key_conclusion": "div gate consistently yields lower MAE than direct division, especially for polar conditions (P(C) small), due to better numerical stability."
      },
      {
        "experiment": "Ablation: Effectiveness of Two-Stage Training",
        "benchmarks": "Validation set from ITC99, EPFL, OpenCore AIGs",
        "baselines": "Training with only Stage-1 or only Stage-2",
        "key_conclusion": "Two-stage training (pattern-based pre-training + workload-aware fine-tuning) achieves the lowest L1 loss and accelerates inference."
      },
      {
        "experiment": "Node Probability for Phase Selection (SAT instances)",
        "benchmarks": "LECSAT (150 hard satisfiable instances from ForgeEDA)",
        "baselines": "CaDiCaL (default phase selection), DeepGate4 (similarity-based guidance)",
        "key_conclusion": "CASCAD's probability-guided phase selection (Ï„=0.005) achieves up to 10x speedup over CaDiCaL and outperforms DeepGate4."
      },
      {
        "experiment": "Clause Probability for Clause Management (UNSAT instances)",
        "benchmarks": "LECUNSAT (150 unsatisfiable instances: 130 small, 20 large)",
        "baselines": "CaDiCaL (default LBD-based clause filtering)",
        "key_conclusion": "Probability-based clause filtering (Ï„=0.9) outperforms LBD-based filtering, solving more instances faster and reducing Par2 score."
      },
      {
        "experiment": "Inprocessing Effectiveness Beyond Preprocessing",
        "benchmarks": "LECSAT (SAT), LECUNSAT (UNSAT)",
        "baselines": "Preprocessing-only (circuit optimization techniques from prior work)",
        "key_conclusion": "CASCAD's inprocessing provides substantial additional speedup on top of preprocessing (4.6x on SAT, ~26% Par2 reduction on UNSAT)."
      },
      {
        "experiment": "Analysis of Probability-Based Clause Metric",
        "benchmarks": "Clauses extracted from solving process (after 50k conflicts)",
        "baselines": "Clause groups segmented by LBD and probability threshold",
        "key_conclusion": "Low-probability clauses are more beneficial for solving; probability is more informative than LBD alone (e.g., some LBD=3 clauses with low probability help)."
      },
      {
        "experiment": "Phase Selection as UNSAT Classifier",
        "benchmarks": "LECUNSAT (150 UNSAT instances)",
        "baselines": "Raw CaDiCaL",
        "key_conclusion": "Using phase selection as a lightweight classifier (5s timeout) to switch to UNSAT-tuned configuration reduces average solving time from 166.26s to 80.36s."
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "ITC99, EPFL, OpenCore AIGs",
          "used_in_experiments": [
            "exp_1",
            "exp_3"
          ]
        },
        {
          "benchmark": "LECSAT (SAT instances)",
          "used_in_experiments": [
            "exp_4",
            "exp_6"
          ]
        },
        {
          "benchmark": "LECUNSAT (UNSAT instances)",
          "used_in_experiments": [
            "exp_5",
            "exp_6",
            "exp_8"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "CaDiCaL",
          "compared_in_experiments": [
            "exp_4",
            "exp_5",
            "exp_6",
            "exp_8"
          ]
        },
        {
          "baseline": "DeepGate4",
          "compared_in_experiments": [
            "exp_4"
          ]
        },
        {
          "baseline": "Preprocessing-only (Shi et al. 2025c)",
          "compared_in_experiments": [
            "exp_6"
          ]
        }
      ],
      "overall_narrative": "The experiments collectively tell a coherent story of innovation and validation. First, the core predictive model is shown to be accurate and scalable (exp_1), and its key architectural components (div gate, two-stage training) are validated through ablations (exp_2, exp_3). Then, the probabilities are applied to two critical CDCL heuristics: phase selection for SAT instances (exp_4) and clause filtering for UNSAT instances (exp_5), both showing substantial improvements over state-of-the-art baselines. The value of this dynamic inprocessing is further underscored by its complementary gains on top of strong static preprocessing (exp_6). Additional analyses provide deeper insights into why the probability-based clause metric works (exp_7) and how the phase selection can be repurposed for adaptive solving (exp_8). Together, they robustly support the paper's thesis that leveraging circuit-level conditional probabilities dynamically during CDCL solving significantly enhances performance on Circuit SAT problems.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "Circuit-level conditional probabilities can effectively guide CDCL heuristics.",
          "supported_by": [
            "exp_4",
            "exp_5"
          ],
          "strength": "strong"
        },
        {
          "claim": "A GNN can accurately predict these conditional probabilities from circuit structure.",
          "supported_by": [
            "exp_1",
            "exp_2",
            "exp_3"
          ],
          "strength": "strong"
        },
        {
          "claim": "Dynamic inprocessing guidance provides benefits beyond static preprocessing.",
          "supported_by": [
            "exp_6"
          ],
          "strength": "strong"
        }
      ],
      "limitations_acknowledged": [
        "The method requires training a GNN model, which adds an initial overhead.",
        "Performance depends on the accuracy of probability prediction, though errors are shown to be low.",
        "The phase selection threshold (Ï„) and clause probability threshold need tuning."
      ],
      "key_takeaways": [
        "CASCAD reduces solving time by up to 10x on hard SAT instances via probability-guided phase selection.",
        "Probability-based clause filtering achieves an additional 23.5% runtime reduction on UNSAT benchmarks.",
        "The proposed div gate and two-stage training are critical for accurate and efficient probability prediction.",
        "The framework successfully bridges static circuit information with the dynamic CDCL solving process."
      ]
    }
  },
  {
    "filename": "2508.21513_On_the_Hardness_of_Learning_GNN-based_SAT_Solvers_.md",
    "paper_title": "On the Hardness of Learning GNN-based SAT Solvers: The Role of Graph Ricci Curvature",
    "paper_focus": "This paper proposes that the performance degradation of GNN-based SAT solvers on harder instances is fundamentally linked to the negative Ricci curvature of the bipartite graph representations of logical formulas, which causes oversquashing and limits the GNNs' ability to learn long-range dependencies.",
    "experiment_inventory": {
      "total_experiments": 4,
      "main_experiments": 2,
      "ablation_studies": 1,
      "other_experiments": 1
    },
    "experiments": {
      "exp_1": {
        "name": "Relationship between curvature and satisfiability",
        "paper_section": "Section 4.1, Figure 2",
        "experiment_type": "main_comparison",
        "purpose": "To empirically validate the theoretical claim that graph Ricci curvature (BFC) correlates with SAT problem difficulty and GNN solver performance.",
        "benchmarks": [
          {
            "name": "Random 3-SAT",
            "variant": "N=256 variables, Î± âˆˆ [3,5] in steps of Î”Î±=0.1",
            "problem_sizes": "N=256"
          },
          {
            "name": "Random 4-SAT",
            "variant": "Not detailed in main text, referenced in Appendix (Figure 7,8)",
            "problem_sizes": "N=256"
          }
        ],
        "baselines": [
          {
            "name": "Analytical SAT-UNSAT threshold",
            "description": "Theoretical critical threshold Î±c â‰ˆ 4.267 for random 3-SAT [32]",
            "source": "Mertens et al. (2006)"
          },
          {
            "name": "NeuroSAT",
            "description": "GNN-based SAT solver trained to produce satisfying assignments",
            "source": "Selsam et al. (2019)"
          }
        ],
        "conclusion": {
          "main_finding": "The probability of finding a solution with NeuroSAT as a function of the mean and variance of the Balanced Forman Curvature (BFC) replicates a SAT/UNSAT phase transition, establishing a direct link between curvature and solver performance.",
          "supporting_findings": [
            "Average BFC drops monotonically with clause density Î±.",
            "For random 3-SAT, curvature becomes highly negative and concentrated near the dynamical threshold Î±d â‰ˆ 3.927.",
            "For random 4-SAT, problems have more negatively curved edges even at low Î±, severely impacting GNN solver performance."
          ],
          "winner": "N/A (correlational study)"
        },
        "detailed_description": "This experiment aims to empirically verify the paper's core theoretical claim: that the geometric property of graph Ricci curvature (specifically Balanced Forman Curvature or BFC) is a strong indicator of SAT problem hardness for GNN-based solvers. The authors generate random 3-SAT instances with N=256 variables across a range of clause densities Î± from 3 to 5, spanning both satisfiable and unsatisfiable regimes around the known critical threshold Î±c â‰ˆ 4.267. They train the NeuroSAT model to produce satisfying assignments, scaling message-passing iterations by 2N during evaluation. They then analyze solver performance in relation to computed curvature metrics. The key results show that: 1) The average BFC of the problem graphs decreases monotonically as Î± increases (Figure 2a). 2) When plotting the solution probability against the first two moments (mean and variance) of the BFC distribution, a clear phase-transition-like phenomenon emerges, mirroring the classic SAT/UNSAT transition defined by Î± (Figure 2b). This demonstrates that curvature statistics can serve as an alternative, geometry-based signature of problem hardness. The experiment is extended to random 4-SAT (Figures 7, 8 in Appendix), where the negative curvature is more pronounced even at lower Î± values, explaining the steeper performance drop-off observed for GNN solvers on these problems.\n"
      },
      "exp_2": {
        "name": "Test-time rewiring evaluation",
        "paper_section": "Section 4.1 (Test-time Rewiring), Table 1",
        "experiment_type": "main_comparison",
        "purpose": "To test the causal relationship between graph curvature and solver difficulty by modifying test-set graphs to be 'flatter' (less negatively curved) and observing performance changes.",
        "benchmarks": [
          {
            "name": "SAT benchmark datasets from Li et al. (2024)",
            "variant": "Random 3-SAT (generated near Î±c)",
            "problem_sizes": "Not specified"
          },
          {
            "name": "SAT benchmark datasets from Li et al. (2024)",
            "variant": "Random 4-SAT (generated near Î±c)",
            "problem_sizes": "Not specified"
          },
          {
            "name": "SAT benchmark datasets from Li et al. (2024)",
            "variant": "SR (mixed k-SAT)",
            "problem_sizes": "Not specified"
          },
          {
            "name": "SAT benchmark datasets from Li et al. (2024)",
            "variant": "CA (mimics industrial problem community structure)",
            "problem_sizes": "Not specified"
          }
        ],
        "baselines": [
          {
            "name": "GCN-solver",
            "description": "Graph Convolutional Network based solver [25]",
            "source": "Kipf (2016)"
          },
          {
            "name": "NeuroSAT",
            "description": "Recurrent GNN-based SAT solver",
            "source": "Selsam et al. (2019)"
          },
          {
            "name": "No Rewiring (Control)",
            "description": "Standard evaluation on the original test graphs",
            "source": "Proposed method (control condition)"
          }
        ],
        "conclusion": {
          "main_finding": "Applying a stochastic discrete Ricci flow rewiring procedure to increase the average BFC of test graphs (making them less negatively curved) significantly improves the accuracy of both GCN and NeuroSAT solvers at test-time without retraining.",
          "supporting_findings": [
            "The improvement is most substantial on the more difficult 4-SAT problems (+0.194 for GCN, +0.250 for NeuroSAT).",
            "Improvement is smaller on the CA dataset, which already has a community structure associated with less severe bottlenecks.",
            "NeuroSAT consistently outperforms GCN, suggesting recurrence helps mitigate oversquashing."
          ],
          "winner": "NeuroSAT with test-time rewiring"
        },
        "detailed_description": "This experiment investigates a causal intervention: if curvature is a fundamental bottleneck, then artificially reducing the negative curvature of a problem graph should make it easier for a pre-trained GNN solver. The authors use four benchmark SAT datasets (3-SAT, 4-SAT, SR, CA). They train both a GCN-solver and NeuroSAT on the standard training partitions. For evaluation, they apply a test-time graph rewiring procedure (Algorithm 1 in Appendix A.3) based on stochastic discrete Ricci flow to the test set. This procedure iteratively identifies the most negatively curved edge, removes it, and adds a new edge between the neighborhoods of its endpoints to increase the local average curvature. The key result is that this rewiring, which modifies the constraint structure but makes the graph geometrically \"flatter\", leads to substantial accuracy improvements for both solvers on the rewired test sets compared to the original test sets. The gains are largest for the most challenging dataset (4-SAT) and smallest for the CA dataset, which the theory predicts has a naturally less bottlenecked community structure. This provides strong evidence that negative curvature is not just correlated with but actively contributes to the difficulty GNNs face in solving SAT problems.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "Relationship between curvature and satisfiability (NeuroSAT)",
        "benchmarks": "Random 3-SAT (N=256, Î±âˆˆ[3,5]), Random 4-SAT",
        "baselines": "Analytical SAT-UNSAT critical threshold Î±c",
        "key_conclusion": "Average BFC drops monotonically with Î±; solution probability as a function of BFC's first two moments replicates a SAT/UNSAT phase transition."
      },
      {
        "experiment": "Test-time rewiring evaluation",
        "benchmarks": "Random 3-SAT (near Î±c), Random 4-SAT (near Î±c), SR (mixed k-SAT), CA (community structure)",
        "baselines": "Same models (GCN, NeuroSAT) without rewiring",
        "key_conclusion": "Rewiring (increasing average BFC) at test-time improves solver accuracy across all datasets, with largest gains on more difficult (4-SAT) problems."
      },
      {
        "experiment": "Curvature-aware solver variants (ablation)",
        "benchmarks": "Random 3-SAT (near Î±c), Random 4-SAT (near Î±c), SR, CA",
        "baselines": "Vanilla GCN, Vanilla NeuroSAT",
        "key_conclusion": "Naive integration of curvature information (Curvature Gate, Online LCP) does not yield consistent performance improvements over vanilla message passing."
      },
      {
        "experiment": "Hardness heuristic validation",
        "benchmarks": "Random 3-SAT (near Î±c), Random 4-SAT (near Î±c), SR, CA",
        "baselines": "Average clause density (á¾±)",
        "key_conclusion": "Proposed curvature-based heuristics (Ï‰Ì„, Ï‰Ì„*) correlate strongly with generalization error (Ï=0.86, 0.98), outperforming correlation with average clause density (Ï=0.32)."
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "Random 3-SAT (varying Î±)",
          "used_in_experiments": [
            "exp_1"
          ]
        },
        {
          "benchmark": "Random 4-SAT",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "ablation_1",
            "exp_3"
          ]
        },
        {
          "benchmark": "SAT benchmark suite (Li et al., 2024): 3-SAT, 4-SAT, SR, CA",
          "used_in_experiments": [
            "exp_2",
            "ablation_1",
            "exp_3"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "NeuroSAT",
          "compared_in_experiments": [
            "exp_1",
            "exp_2",
            "ablation_1",
            "exp_3"
          ]
        },
        {
          "baseline": "GCN-solver",
          "compared_in_experiments": [
            "exp_2",
            "ablation_1"
          ]
        },
        {
          "baseline": "Analytical thresholds (Î±c, Î±d)",
          "compared_in_experiments": [
            "exp_1"
          ]
        },
        {
          "baseline": "Average clause density (Î±Ì„)",
          "compared_in_experiments": [
            "exp_3"
          ]
        }
      ],
      "overall_narrative": "The experiments collectively build a compelling case for the paper's thesis. First, Exp 1 establishes a strong correlation between graph curvature (BFC) and solver performance, replicating phase transitions. Second, Exp 2 provides causal evidence through test-time rewiring: directly reducing negative curvature improves solver accuracy, especially on hard problems. Third, Exp 3 shows that curvature-based heuristics are superior predictors of solver difficulty compared to traditional metrics, offering a practical tool. Finally, Ablation 1 demonstrates that simply making solvers \"aware\" of curvature is insufficient, highlighting the complexity of the problem and pointing to the need for more fundamental architectural changes (like recurrence, which NeuroSAT uses) to mitigate oversquashing. The narrative moves from observation (correlation) to intervention (causality) to application (prediction) and finally to architectural exploration, thoroughly validating the link between graph Ricci curvature and the limitations of GNN-based SAT solvers.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "The performance of GNN-based SAT solvers is fundamentally limited by the negative Ricci curvature of their input graph representations, which causes oversquashing.",
          "supported_by": [
            "exp_1",
            "exp_2"
          ],
          "strength": "strong"
        },
        {
          "claim": "Curvature-based metrics are better predictors of GNN solver generalization error than traditional clause density.",
          "supported_by": [
            "exp_3"
          ],
          "strength": "strong"
        },
        {
          "claim": "Recurrence in GNN architectures (like NeuroSAT) partially mitigates the oversquashing problem compared to simpler architectures like GCN.",
          "supported_by": [
            "exp_2"
          ],
          "strength": "moderate"
        }
      ],
      "limitations_acknowledged": [
        "Naively integrating curvature information into solver architectures does not guarantee performance improvements (Ablation 1).",
        "The test-time rewiring procedure modifies the SAT problem constraints, so it demonstrates a causal link but is not a practical solver improvement.",
        "The analysis focuses on average curvature; distributional characterization is noted as future work."
      ],
      "key_takeaways": [
        "Graph Ricci Curvature (BFC) strongly correlates with and influences the difficulty SAT problems pose to GNN-based solvers.",
        "Making problem graphs 'flatter' (less negatively curved) via rewiring makes them significantly easier for pre-trained GNN solvers.",
        "A curvature-based heuristic (Ï‰Ì„*) is a near-perfect predictor (Ï=0.98) of GNN solver generalization error on a dataset.",
        "GNN-based SAT solvers face two types of hardness: the algorithmic hardness of SAT and the representational hardness due to oversquashing in negatively curved graphs."
      ]
    }
  },
  {
    "filename": "2510.15583_Attn-JGNN_Attention_Enhanced_Join-Graph_Neural_Net.md",
    "paper_title": "Attn-JGNN: Attention Enhanced Join-Graph Neural Networks",
    "paper_focus": "Proposes a neural framework (Attn-JGNN) combining tree decomposition, Iterative Join-Graph Propagation (IJGP), and hierarchical attention mechanisms to approximate the solution to #SAT (model counting) problems.",
    "experiment_inventory": {
      "total_experiments": 3,
      "main_experiments": 1,
      "ablation_studies": 2,
      "other_experiments": 0
    },
    "experiments": {
      "exp_1": {
        "name": "Main Performance Comparison on BIRD and SATLIB Benchmarks",
        "paper_section": "4.3 Main Results",
        "experiment_type": "main_comparison",
        "purpose": "To evaluate the overall accuracy and effectiveness of Attn-JGNN compared to state-of-the-art neural and approximate #SAT solvers on standard benchmarks.",
        "benchmarks": [
          {
            "name": "BIRD",
            "variant": "Subset containing 8 categories from DQMR networks, grid networks, SMTLIB bit-blasted, ISCAS89 circuits",
            "problem_sizes": "Formulas with >10,000 variables and clauses (large-scale)"
          },
          {
            "name": "SATLIB",
            "variant": "5 selected distributions with â‰¥100 satisfiable instances",
            "problem_sizes": "Variables ranging from 100 to 600"
          }
        ],
        "baselines": [
          {
            "name": "NSNet",
            "description": "A general neural probabilistic framework for satisfiability problems, based on simple Belief Propagation (BP).",
            "source": "[23]"
          },
          {
            "name": "BPNN",
            "description": "Neural baseline combining Belief Propagation with a neural network architecture.",
            "source": "[18]"
          },
          {
            "name": "ApproxMC3",
            "description": "State-of-the-art approximate model counting solver, a hash-based counter with provable guarantees.",
            "source": "[6,7,31,30,36]"
          },
          {
            "name": "F2",
            "description": "State-of-the-art fast and flexible probabilistic model counter.",
            "source": "[1]"
          }
        ],
        "conclusion": {
          "main_finding": "Attn-JGNN significantly outperforms neural baselines NSNet and BPNN, achieving 31% and 45% better RMSE respectively, and is competitive with the strong solver ApproxMC3, particularly excelling on very large, difficult instances.",
          "supporting_findings": [
            "On the BIRD benchmark (Fig 3a), Attn-JGNN estimates tighter counts than NSNet, BPNN, and F2 in all categories.",
            "For ground truth counts less than e^100, Attn-JGNN and ApproxMC3 provide more accurate estimates than others (Fig 3b).",
            "For ground truth counts exceeding e^100 (where ApproxMC3 times out), Attn-JGNN can still provide close approximations, even beyond e^1000.",
            "On the more randomly generated SATLIB benchmark (Table 2), Attn-JGNN still outperforms NSNet and F2 in most categories (RND3SAT, BMS, CBS, SW-GCP)."
          ],
          "winner": "Attn-JGNN among neural methods; ApproxMC3 is strong but times out on largest instances."
        },
        "detailed_description": "This is the primary experiment evaluating the proposed Attn-JGNN model against key baselines. It uses two benchmarks: BIRD (challenging, large real-world formulas) and SATLIB (broader range of synthetic formulas). The ground truth model counts are generated using the exact solver DSharp with a 5000-second timeout. The key metric is Root Mean Square Error (RMSE) between the estimated log count and the ground truth. Results show Attn-JGNN's superiority over other neural methods (NSNet, BPNN) on both benchmarks. Notably, while the strong solver ApproxMC3 is more accurate on smaller instances, it fails to complete within the time limit on the largest instances (count > e^100), whereas Attn-JGNN continues to provide reasonable approximations, demonstrating its scalability and effectiveness on hard, large-scale #SAT problems. The performance gap on SATLIB, while still favorable, is smaller, attributed to the dataset's random nature making it harder to exploit learned common features.\n"
      },
      "exp_2": {
        "name": "Ablation: IJGP vs. BP Algorithm (Attn-JGNN without Attention)",
        "paper_section": "4.3 Main Results (referenced), Table 1",
        "experiment_type": "ablation",
        "purpose": "To isolate and evaluate the contribution of the underlying IJGP algorithm (vs. BP) to the model's performance, by comparing a version of Attn-JGNN with the attention mechanism removed against NSNet (which uses BP).",
        "benchmarks": [
          {
            "name": "SATLIB",
            "variant": "5 categories (RND3SAT, BMS, CBS, GCP, SW-GCP)",
            "problem_sizes": "Variables ranging from 100 to 600"
          }
        ],
        "baselines": [
          {
            "name": "NSNet",
            "description": "Neural framework based on the Belief Propagation (BP) algorithm.",
            "source": "[23]"
          }
        ],
        "conclusion": {
          "main_finding": "The IJGP algorithm, even without the attention mechanism, leads to better solving accuracy than the BP algorithm used in NSNet, confirming the superior reasoning capability of IJGP.",
          "supporting_findings": [
            "Attn-JGNN-Att (without attention) achieves lower RMSE than NSNet across all five SATLIB categories (Table 1).",
            "The solving speed is reported to be of the same order of magnitude as NSNet."
          ],
          "winner": "Attn-JGNN-Att (IJGP-based) over NSNet (BP-based)."
        },
        "detailed_description": "This ablation study decouples the algorithm contribution from the attention mechanism. The authors compare \"Attn-JGNN-Att\" (their model with the attention mechanism removed, thus essentially a neural parameterization of IJGP) against NSNet (a neural parameterization of BP). Both models are evaluated on the SATLIB benchmark using RMSE. The results in Table 1 consistently show lower RMSE for Attn-JGNN-Att across all problem categories, demonstrating that the core IJGP message-passing framework is inherently more accurate for #SAT approximation than the BP framework used in prior work. This experiment validates the foundational choice of IJGP over BP.\n"
      },
      "exp_3": {
        "name": "Ablation: Effectiveness of Proposed Attention Mechanisms",
        "paper_section": "4.3 Main Results, Table 3",
        "experiment_type": "ablation",
        "purpose": "To evaluate the individual and cumulative contribution of the three proposed attention mechanism refinements: Hierarchical (H), Constraint-Aware (C), and Dynamic (D).",
        "benchmarks": [
          {
            "name": "Implied to be SATLIB",
            "variant": "Not explicitly stated, but context suggests the main benchmark.",
            "problem_sizes": "Implied standard range."
          }
        ],
        "baselines": [
          {
            "name": "GAT",
            "description": "Base variant with standard Graph Attention, without the three proposed enhancements.",
            "source": "proposed method (ablated)"
          },
          {
            "name": "GAT-H",
            "description": "Variant with only the Hierarchical attention mechanism added.",
            "source": "proposed method (ablated)"
          },
          {
            "name": "GAT-HC",
            "description": "Variant with Hierarchical and Constraint-Aware mechanisms added.",
            "source": "proposed method (ablated)"
          }
        ],
        "conclusion": {
          "main_finding": "All three proposed attention mechanisms progressively improve model accuracy (reduce RMSE) and efficiency, with the dynamic mechanism specifically reducing training time and computational redundancy.",
          "supporting_findings": [
            "Adding Hierarchical attention (GAT-H) reduces RMSE from 1.33 to 1.26 and training time.",
            "Adding Constraint-Aware mechanism (GAT-HC) further reduces RMSE to 1.19.",
            "Adding Dynamic attention (GAT-HCD, the full Attn-JGNN) achieves the best RMSE (1.16), reduces training time significantly (113s vs 185s), and cuts attention head utilization to 62.5%, indicating efficient pruning of redundant heads."
          ],
          "winner": "GAT-HCD (Full Attn-JGNN)"
        },
        "detailed_description": "This internal ablation study, detailed in Table 3, systematically adds the three novel components to a base GAT model to measure their impact. The evaluation metrics are RMSE, attention head utilization percentage, and training time/convergence. Starting from a base GAT model, adding the Hierarchical mechanism (separate intra- and inter-cluster attention) improves accuracy and speed. Adding the Constraint-Aware mechanism (which biases learning towards satisfying clauses) further boosts accuracy. Finally, adding the Dynamic mechanism (which adjusts the number of active attention heads over time) yields the best accuracy, the fastest training, and significantly reduces the utilization of attention heads from 100% to 62.5%, proving it effectively trims unnecessary computation. This experiment conclusively demonstrates that each proposed architectural refinement contributes positively to the final model's performance and efficiency.\n"
      }
    },
    "summary_table": [
      {
        "experiment": "Main Performance Comparison on BIRD and SATLIB Benchmarks",
        "benchmarks": "BIRD (8 categories), SATLIB (5 categories: RND3SAT, BMS, CBS, GCP, SW-GCP)",
        "baselines": "NSNet, BPNN, ApproxMC3, F2",
        "key_conclusion": "Attn-JGNN outperforms neural baselines (NSNet, BPGAT) significantly (31% & 45% better RMSE) and is competitive with ApproxMC3, especially on large, hard instances."
      },
      {
        "experiment": "Ablation: IJGP vs. BP Algorithm (Attn-JGNN without Attention)",
        "benchmarks": "SATLIB (5 categories)",
        "baselines": "NSNet (BP-based)",
        "key_conclusion": "Attn-JGNN without attention (IJGP algorithm) outperforms NSNet (BP algorithm), proving the superior reasoning capability of IJGP over BP."
      },
      {
        "experiment": "Ablation: Effectiveness of Proposed Attention Mechanisms",
        "benchmarks": "Implied from main experiments (SATLIB likely)",
        "baselines": "Ablated variants of Attn-JGNN (GAT, GAT-H, GAT-HC)",
        "key_conclusion": "All three proposed mechanisms (Hierarchical, Constraint-Aware, Dynamic) improve performance, with Dynamic attention reducing training time and head utilization."
      }
    ],
    "cross_experiment_analysis": {
      "benchmark_coverage": [
        {
          "benchmark": "BIRD",
          "used_in_experiments": [
            "exp_1"
          ]
        },
        {
          "benchmark": "SATLIB",
          "used_in_experiments": [
            "exp_1",
            "exp_2",
            "exp_3 (implied)"
          ]
        }
      ],
      "baseline_coverage": [
        {
          "baseline": "NSNet",
          "compared_in_experiments": [
            "exp_1",
            "exp_2"
          ]
        },
        {
          "baseline": "BPNN",
          "compared_in_experiments": [
            "exp_1"
          ]
        },
        {
          "baseline": "ApproxMC3",
          "compared_in_experiments": [
            "exp_1"
          ]
        },
        {
          "baseline": "F2",
          "compared_in_experiments": [
            "exp_1"
          ]
        }
      ],
      "overall_narrative": "The experiments together tell a cohesive story: 1) The core innovation of using IJGP is validated (Exp 2), showing it outperforms the previous BP-based approach. 2) On top of this superior foundation, the novel attention mechanisms (Exp 3) provide significant further gains in accuracy and efficiency. 3) The complete Attn-JGNN system (Exp 1) then demonstrates state-of-the-art performance among neural solvers and competitive results with the best approximate counters, particularly showcasing robustness and scalability on large, hard instances where traditional solvers fail.\n"
    },
    "overall_conclusions": {
      "main_claims_validated": [
        {
          "claim": "Attn-JGNN provides a more accurate neural framework for #SAT than prior neural methods.",
          "supported_by": [
            "exp_1"
          ],
          "strength": "strong"
        },
        {
          "claim": "The IJGP algorithm is superior to BP for this task.",
          "supported_by": [
            "exp_2"
          ],
          "strength": "strong"
        },
        {
          "claim": "The proposed hierarchical, constraint-aware, and dynamic attention mechanisms are effective improvements.",
          "supported_by": [
            "exp_3"
          ],
          "strength": "strong"
        }
      ],
      "limitations_acknowledged": [
        "On the randomly generated SATLIB benchmark, performance gains are less pronounced than on BIRD, as it's harder to exploit common features.",
        "Attn-JGNN cannot fully compete with ApproxMC3's accuracy on smaller instances where ApproxMC3 completes."
      ],
      "key_takeaways": [
        "Combining IJGP with hierarchical attention provides a powerful and scalable neural architecture for approximate #SAT.",
        "The model is particularly effective for large, complex instances that challenge traditional solvers.",
        "The dynamic attention mechanism is key for balancing model expressiveness with training efficiency."
      ]
    }
  }
];

        // Initialize
        function init() {
            if (embeddedData && embeddedData.length > 0) {
                papersData.push(...embeddedData);
                renderPapers(papersData);
                updateStats();
            } else {
                papersGrid.innerHTML = '<div class="no-results">No data loaded. Please run the data generator script.</div>';
            }
        }

        init();
    </script>
</body>
</html>
